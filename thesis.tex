% Created 2019-03-27 Wed 16:46
% Intended LaTeX compiler: pdflatex
\documentclass[a4paper, 12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\usepackage[left=2.5cm, right=2.5cm, top=2.5cm, bottom=2.5cm, bindingoffset=1.5cm, head=15pt]{geometry}
\usepackage{setspace}
\usepackage{caption}
\onehalfspacing
\usepackage[official]{eurosym}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{notation/rl}
\usepackage{notation/model}
\usepackage{template/list}
\let\itemize\hitemize
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhead{}
\fancyfoot{}
\fancyhead[LE,RO]{\textsl{\leftmark}}
\fancyhead[RE,LO]{Tobias Richter}
\fancyfoot[C]{\thepage}
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0pt}
\usepackage{apacite}
\let\cite\shortcite
\let\textcite\shortciteA
\usepackage[nohyperlinks]{acronym}
\interfootnotelinepenalty=10000
\usepackage[notlof,notlot,nottoc]{tocbibind}
\newcommand{\studentID}{558305}
\newcommand{\thesistype}{Master Thesis}
\newcommand{\supervisor}{Univ.-Prof. Dr. Wolfgang Ketter}
\newcommand{\cosupervisor}{Karsten Schroer}
\pagenumbering{Roman}
\author{Tobias Richter}
\date{\today}
\title{Reinforcement Learning Portfolio Optimization of Electric Vehicle Virtual Power Plants}
\hypersetup{
 pdfauthor={Tobias Richter},
 pdftitle={Reinforcement Learning Portfolio Optimization of Electric Vehicle Virtual Power Plants},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 26.1 (Org mode 9.2.1)},
 pdflang={English}}
\begin{document}

\makeatletter
\begin{titlepage}
    \begin{center}
        \vspace*{1cm}

        \Large
        \textbf{\@title{}}

        \vspace{1.5cm}

        \thesistype{}

        \vspace{1cm}

        \begin{figure}[htbp]
             \centering
             \includegraphics[width=.5\linewidth]{./fig/UoC_Logo.png}
        \end{figure}

        \vspace{1cm}

        \large
        \textbf{Author}: \@author{} (Student ID: \studentID{})\\
        \large
        \textbf{Supervisor}: \supervisor{}\\
        \large
        \textbf{Co-Supervisor}: \cosupervisor{}

        \vspace{1cm}
        \large
        Department of Information Systems for Sustainable Society\\
        Faculty of Management, Economics and Social Sciences\\
        University of Cologne\\

        \vspace{1cm}
        \@date{}

    \end{center}
\end{titlepage}
\makeatother
\clearpage
\thispagestyle{empty}
\section*{Eidesstattliche Versicherung}
\label{sec:SOOA}

\vspace{2.5cm}

% Statement of original authorship - Needs to be in German
% see also here: https://www.wiso.uni-koeln.de/sites/fakultaet/dokumente/PA/formulare/eidesstattliche_erklaerung.pdf

Hiermit versichere ich an Eides statt, dass ich die vorliegende Arbeit selbstständig und ohne die Benutzung anderer als der angegebenen Hilfsmittel angefertigt habe. Alle Stellen, die wörtlich oder sinngemäß aus veröffentlichten und nicht veröffentlichten Schriften entnommen wurden, sind als solche kenntlich gemacht. Die Arbeit ist in gleicher oder ähnlicher Form oder auszugsweise im Rahmen einer anderen Prüfung noch nicht vorgelegt worden. Ich versichere, dass die eingereichte elektronische Fassung der eingereichten Druckfassung vollständig entspricht.

\vspace{1cm}

\noindent
Die Strafbarkeit einer falschen eidesstattlichen Versicherung ist mir bekannt, namentlich die Strafandrohung gemäß § 156 StGB bis zu drei Jahren Freiheitsstrafe oder Geldstrafe bei vorsätzlicher Begehung der Tat bzw. gemäß § 161 Abs. 1 StGB bis zu einem Jahr Freiheitsstrafe oder Geldstrafe bei fahrlässiger Begehung.

\vspace{3cm}
\noindent
\textbf{Tobias Richter}

\vspace{0.5cm}
\noindent
Köln, den 01.05.2019
\clearpage

\setcounter{page}{1}
\tableofcontents
\clearpage
\listoffigures
\clearpage
\listoftables
\clearpage

\section*{List of Abbreviations} \markboth{LIST OF ABBREVIATIONS}{}
\begin{acronym}[GCRM]
	\acro{ANN}{Artificial Neural Network}
	\acro{DP}{Dynamic Programming}
	\acro{DSO}{Distribution System Operator}
	\acro{DQN}{Deep Q-Networks}
	\acro{DDQN}{Double Deep Q-Networks}
	\acro{EPEX}{European Power Exchange}
	\acro{EV}{Electric Vehicle}
	\acro{GCRM}{German Control Reserve Market}
	\acro{GP}{Genetic Programming}
	\acro{MAW}{Mean Asymmetric Weighted Objective Function}
	\acro{MC}{Monte Carlo}
	\acro{MDP}{Markov Decision Process}
	\acro{PDF}{Probability Density Function}
	\acro{RES}{Renewable Energy Sources}
	\acro{RL}{Reinforcement Learning}
	\acro{TD}{Temporal-Difference}
	\acro{TSO}{Transmission System Operator}
	\acro{V2G}{Vehicle-to-Grid}
	\acro{VPP}{Virtual Power Plant}
\end{acronym}
\clearpage
\section*{Summary of Notation} \markboth{SUMMARY OF NOTATION}{}
Capital letters are used for random variables, whereas lower case letters are used for
the values of random variables and for scalar functions. Quantities that are required to
be real-valued vectors are written in bold and in lower case (even if random variables).
\begin{tabbing}
    \=~~~~~~~~~~~~~~~~~~  \= \kill
    \>$\defeq$            \> equality relationship that is true by definition\\
    \>$\approx$           \> approximately equal\\
    \>$\E{X}$             \> expectation of a random variable $X$, i.e., $\E{X}\defeq\sum_x p(x)x$\\
    \>$\Re$               \> set of real numbers\\
    \>$\leftarrow$        \> assignment\\
    \\
    \>$\e$                \> probability of taking a random action in an \e-greedy policy\\
    \>$\alpha$            \> step-size parameter\\
    \>$\gamma$            \> discount-rate parameter\\
    \>$\lambda$           \> decay-rate parameter for eligibility traces\\
    \\
    \>$s, s'$             \> states\\
    \>$a$                 \> an action\\
    \>$r$                 \> a reward\\
    \>$\S$                \> set of all nonterminal states\\
    \>$\A$                \> set of all available actions\\
    \>$\R$                \> set of all possible rewards, a finite subset of $\Re$\\
    \>$\subset$           \> subset of; e.g., $\R\subset\Re$\\
    \>$\in$               \> is an element of; e.g., $s\in\S$, $r\in\R$\\
    \\
    \>$t$                 \> discrete time step\\
    \>$T, T(t)$           \> final time step of an episode, or of the episode including time step $t$\\
    \>$A_t$               \> action at time $t$\\
    \>$S_t$               \> state at time $t$, typically due, stochastically, to $S_{t-1}$ and $A_{t-1}$\\
    \>$R_t$               \> reward at time $t$, typically due, stochastically, to $S_{t-1}$ and $A_{t-1}$\\
    \>$\pi$               \> policy (decision-making rule)\\
    \>$\pi(s)$            \> action taken in state $s$ under {\it deterministic\/} policy $\pi$\\
    \>$\pi(a|s)$          \> probability of taking action $a$ in state $s$ under {\it stochastic\/} policy $\pi$\\
    \>$G_t$               \> return following time $t$\\
    \\
    \>$\p(s',r|s,a)$      \> probability of transition to state $s'$ with reward $r$, from state $s$ and action $a$\\
    \>$\p(s'|s,a)$        \> probability of transition to state $s'$, from state $s$ taking action $a$\\
    \>$\vpi(s)$           \> value of state $s$ under policy $\pi$ (expected return)\\
    \>$\vstar(s)$         \> value of state $s$ under the optimal policy\\
    \>$\qpi(s,a)$         \> value of taking action $a$ in state $s$ under policy $\pi$\\
    \>$\qstar(s,a)$       \> value of taking action $a$ in state $s$ under the optimal policy\\
    \>$V, V_t$            \> array estimates of state-value function $\vpi$ or $\vstar$\\
    \>$Q, Q_t$            \> array estimates of action-value function $\qpi$ or $\qstar$\\
    \\
    \>$d$                 \> dimensionality---the number of components of $\w$\\
    \>$\w$                \> $d$-vector of weights underlying an approximate value function\\
    \>$\hat v(s,\w)$      \> approximate value of state $s$ given weight vector $\w$\\
    \>$\mu(s)$            \> on-policy distribution over states\\
    \>$\MSVEm$            \> mean square value error\\
\end{tabbing}
\clearpage

\pagenumbering{arabic}
\section{Introduction}
\label{sec:orgb5e5fb2}
\subsection{Research Motivation}
\label{sec:orgdd0461c}
\begin{itemize}
\item \cite{lopes11_integ_elect_vehic_elect_power_system}
\end{itemize}

The global climate change is one biggest challenges of our time. Carbon
emissions need to be reduced and the shift to sustainable energy sources is
inevitable. But the integration of renewables into the electricity grid proves
to be difficult: Solar and wind energy is intermittent and hard to integrate
into the power grid. Sustainable electricity production is dependent on the
weather; under- and oversupplies occur and can destabilize the grid. Virtual
Power Plants (VPP) play an important role in stabilizing the grid. VPPs
aggregate distributed power sources to consume and produce electricity when it
is needed. Carsharing companies operate large, centrally managed fleets of
Electric Vehicles (EV) in major cities around the world. These EV fleets can be
turned into VPPs, using their batteries as combined electricity storage. In this
way, EV fleets can offer balancing services to the power grid or trade
electricity on the open markets for arbitrage purposes. Carsharing companies can
charge the fleet (buy electricity) and discharge the fleet (sell electricity)
when market conditions are favorable. By making EVs available to be used as a
VPP, carsharing companies compromise customer mobility and the profitability of
the fleet. Renting out an EV to a customer is considerably more lucrative than
using it for trading electricity. Knowing how many EVs will most likely be
rented out in a future point of time is essential for a successful trading
strategy. By obtaining an accurate forecasts of available battery capacity,
carsharing companies can determine the capacity to trade on the market. It is
also possible for fleet owners to trade on multiple electricity markets
simultaneously. EV fleets can make use of differences in the market properties,
different lead times to delivery and price elasticity can be favorable for the
trading strategy.


We state, that participating in operating reserve markets and spot markets at
the same time can mitigate risks and increase profits. In this research, we
propose a portfolio optimizing strategy, in which the best composition of the
VPP portfolio, consisting of operating reserve VPPs and spot market VPPs, will
be dynamically learned using a \emph{Reinforcement Learning} (RL) approach. An
intelligent agent using RL can adapt to changing electricity price levels and
customer demands. The agent learns from historical data, its current environment
and realized profits to adjust its trading strategy dynamically. The following
tasks have to be performed by the agent in real-time: 1) \emph{Allocation of plugged
in EVs to an idle or a VPP state}, 2) \emph{Learn the optimal VPP portfolio
composition} and 3) \emph{Place bids and asks on corresponding electricity markets
with an integrated trading strategy}.

\subsection{Research Question}
\label{sec:org28ba08b}

Drawing upon the research motivation, the following research questions are
derived. They build upon another and will be sequentially addressed during this
research:

\begin{enumerate}
\item \emph{What are spatio-temporal customer demand patterns of carsharing EVs?}

Knowing customer demand patterns results in an accurate forecast of how much
available battery capacity an EV fleet will have at any point in the future.

\item \emph{What is the optimal allocation ratio of the available capacity between operating
reserve market VPPs and spot market VPPs?}

Dynamically learning the optimal share of capacity to trade on the respective
markets will maximize profits while reducing the risk of foregone customer
profits.

\item /How does an integrated bidding strategy look like, which considers trading
electricity on the secondary operating reserve market and the continuous
intraday market simultaneously?/

Combine the previous results by designing a bidding strategy and determine
optimal auction prices, which take the different market designs into account.
\end{enumerate}
\subsection{Relevance}
\label{sec:orgfa00031}
From a scientific perspective, this research is relevant to the stream of
agent-based decision making in smart markets
\textcite{bichler10_desig_smart_market,peters13_reinf_learn_approac_to_auton}. We
will contribute to the body of Design Science in Information Systems
\textcite{hevner04_desig_scien_infor_system_resear} and draw upon work, which has
been done in a multitude of research areas: Virtual Power Plants in smart
electricity markets \textcite{pudjianto07_virtual_power_plant_system_integ},
carsharing as a new way of sustainable mobility and advanced machine learning
(ML) and AI methods for forecasting and prediction. We build on research that
has been carried out by \textcite{kahlen18_elect_vehic_virtual_power_plant_dilem}
and \textcite{kahlen17_fleet}. In their papers, the authors concentrate on
participating in one type of electricity market at a time. As proposed by
\citeauthor{kahlen18_elect_vehic_virtual_power_plant_dilem}, we will take this
research further and use EV VPPs to act on multiple types of electricity markets
simultaneously. Moreover, we aim to use more sophisticated machine learning
methods (i.e. Reinforcement Learning, Neural Networks, Ensemble Learning) to
carry out accurate forecasts of rental demand and dynamically learn optimal VPP
portfolio compositions. \textcite{he16_optim_biddin_strat_batter_storag} and
\textcite{mashhour11_biddin_strat_virtual_power_plant_2,mashhour11_biddin_strat_virtual_power_plant_1}
conducted research on optimal bidding strategies for using VPPs to jointly bid
on multiple markets. The authors use stationary storage to participate in
day-ahead and spinning-reserve markets. Contrarily, we aim to use non-stationary
storage (i.e. EV batteries) to participate in the continuous intraday market and
the secondary reserve market.

From a business perspective, this research is relevant to carsharing companies
operating an EV fleet, such as Car2Go or DriveNow. We will show how these
companies can increase their profits, using idle EVs as VPPs to trade
electricity on multiple markets simultaneously. We propose the use of a decision
support system (DSS), which allocates idle EVs to different types of VPPs or to
be available for rent. Further, the DSS will determine optimal capacity-price
pairs to place ask and bid on the individual electricity markets. Using an
event-based simulation we will estimate the profitability increase, when
implementing the proposed methods. This will be done using real-world data from
German electricity markets and trip data from a German carsharing provider.

This research also contributes to the overall welfare of society. First, VPPs of
EVs provide extra balancing services to the power grid. The VPPs can consume
excess electricity almost instantly and stabilize the power grid. When
integrating more intermittent renewable electricity sources into the grid in the
future, such balancing services will become indispensable. Second, a reduction
of electricity prices for the end-consumer is expected. Integrating VPPs into
the power grid increases the efficiency of the whole system and hence will
lower prices. \textcite{kahlen18_elect_vehic_virtual_power_plant_dilem} show
results, where electricity prices decrease up to 3.4\% on the wholesale market.
We anticipate similar or even better results in our research. Third, VPPs can
lead to a decrease in CO\textsubscript{2} emissions. With an increasing share of renewable
energy production, the supply of sustainable electricity can excess the total
electricity demand at times of good weather conditions. The VPPs can consume
this electricity by charging the EV fleet and the sustainable energy production
does not need to be curtailed. The EV fleet can feed the electricity back into
the grid when there is more demand than sustainable electricity production. This
mechanism increases the utilization of renewable electricity generation and
reduces the total CO\textsubscript{2} emissions.
\section{Background}
\label{sec:org91ee8c9}
\subsection{Smart Electricity Markets}
\label{sec:org31130dd}
On electricity markets, actors participate in auctions to match the supply of
electricity generation and the demand for electricity consumption. Participants
place asks (sale offers) and bids (purchase orders). The electricity price is
determined by an auction mechanism, which can take different forms depending on
the type of market. Germany, like many other western countries, has a
liberalized energy system in which the generation and distribution of
electricity are decoupled. Multiple electricity markets exist in a liberalized
energy system. They differ in the auction design and in their reaction time
between the order contract and the delivery of electricity. Day-ahead markets
and spot markets have a reaction time between a day and several hours, whereas
in operating reserve markets the reaction time ranges from minutes to seconds.
The auction mechanism design is essential for electricity markets
\cite{kambil98_reeng_dutch_flower_auction}. Electricity markets work according to
the merit order principle in which resources are considered in ascending order
of the energy price until the capacity demand is met. The clearing price is
determined by the energy price, at the point where supply meets demand. Payment
models differ in the markets: In contrast to day-ahead markets, where a uniform
pricing schema is applied, in secondary reserve markets and intraday markets
bidders, get compensated by the price they bid (pay-as-bid principle).

EV fleet operators can offer the capacity of their EV batteries on multiple
markets at the same time to make use of the different market properties. On
operating reserve markets,prices are usually more volatile and consequently more
attractive for VPPs \cite{tomic07_using_fleet_elect_drive_vehic_grid_suppor}.
Operating reserve markets also bear a higher risk for the fleet: Commitments
have to be made one week in advance when customer demands are still uncertain.
In order to not face penalties for unfulfilled commitments only a conservative
amount of capacity can be offered to the market. On the other hand, spot markets
allow participants to continuously trade electricity products up to five minutes
prior to delivery. At this point in time, it is possible to predict available
battery capacity of the fleet with high accuracy. This certainty creates the
possibility to trade the remaining available capacity with low risk at the spot
market. In the following, we will explain the market design of balancing markets
and spot markets in more detail, since they are the markets we included in our
research.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\linewidth]{fig/electricity_markets.png}
\caption[Electricity Market Design]{Interaction between electricity markets in relation to capacity allocation \label{fig-electricity-markets}}
\end{figure}
\subsection{Electricity Market Theory:}
\label{sec:org4bb4012}
\subsubsection{Balancing Market \label{sec-balancing-market}}
\label{sec:org8f66ad6}
The balancing market is a tool to balance frequency deviations in the power
grid. It offers auctions for primary control reserve, secondary control reserve
as well as tertiary control reserve (minute reserve), which primarily differ in
the required ramp-up times of the participants. As depicted in Figure
\ref{fig-electricity-markets}, the balancing market can be seen as the last link
in a chain of electricity markets \cite{veen16_elect_balan_market}. In this study,
we will look at the German Control Reserve Market (GCRM), one of the largest
frequency regulation markets in the world. However, the presented concepts can
be easily transferred to other balancing markets in unbundled energy systems,
since the market design is similar
\cite{brandt17_evaluat_busin_model_vehic_grid_integ}. Transmission Systems
Operators (TSO) procure their required control reserve via tender auctions at
the GCRM. The market conducts daily auctions for the three types of control
reserve. This thesis focuses on the secondary operating reserve auction, in
which participants must be able to supply or absorb a minimum of 1MW of power
over a 4-hour interval with a reaction time of 30 seconds.\footnote{See \url{https://regelleistung.net}, accessed on 15\textsuperscript{th} February
2019, for further information on the market design and historical data.} Since EV
batteries can absorb energy almost instantly, when they are connected to a
charging station, they are suitable to provide such balancing services.
Operating reserve providers have to be qualified by the TSO to participate in
the market and are able to reliably provide the committed capacity. Although EV
fleets are currently not qualified by the GCRM to be used as operating reserve,
they could theoretically handle the minimum capacity requirements. Around 220
EVs would need to simultaneously charge at standard 4.6kW charging stations to
provide 1MW of downward regulating capacity.

Up until 28\textsuperscript{th} July 2018, auctions were held weekly, with two different
segments each week (peak hours/non-peak hours). Afterwards, the auction
mechanism changed to \emph{daily} auctions of six four-hour segments of positive and
negative control reserve.\footnote{\url{https://www.bundesnetzagentur.de/SharedDocs/Pressemitteilungen/DE/2017/28062017\_Regelenergie.html},
accessed 18\textsuperscript{th} February, 2019} Shorter auction cycles facilitate the
integration of renewable energy generators into the secondary control reserve
market, as they are dependent on accurate (short-term) capacity forecasts.

Positive control reserve is energy that is supplied to the grid, when the grid
frequency falls below 50Hz. It can be provided by increasing the electricity
generation or by reducing the grid load (i.e., electricity consumption). On the
contrary, negative control reserve is required when the grid frequency rises
above 50Hz and can be provided by adding grid load or reducing electricity
generation. Since we do not consider V2G in this thesis, the EV fleets in our
model are only able provide \emph{negative control reserve}, which we will refer to
as \emph{control reserve} until the end of the thesis. Market participants submit
bids in the following form to the market: \((\Pb{}, \cp{}, \ep{})\), where \(\Pb{}\)
is the amount of electrical power that can be supplied on demand in kW, \(\cp{}\) is
the capacity price for keeping the power available in \(\emw\) and
\(\ep{}\) is the energy price for delivered energy in \(\emwh\). The TSO
determines the target quantity of energy to acquire per timeslot, it usually
acquires much higher regulation capacity to minimize risks and activates the
capacity on demand. The TSO accepts the bids based on the capacity price in a
merit order. Providers, whose bids were accepted, instantly get compensated for
the provided capacity: \(R^c = \cp{} \times \Pb{}\). At the time regulation
capacity is needed, usually a day to a week later, the TSO activates the
capacity according to a merit order of the ascending \emph{energy prices} \(\ep{}\).
Hence, providers are also compensated according to the actual energy \(\Eb{}\)
they supplied or consumed: \(R^e = \ep{} \times \Eb{}\). Since provider get paid
according to their submitted price \(\ep{}\), instead of a market clearing price,
this type of auction is called \emph{pay-as-bid} auction.
\subsubsection{Spot Market \label{sec-spot-market}}
\label{sec:orgaea6b58}
As mentioned in the previous chapter, the equilibrium of electricity supply and
demand is ensured through a sequence of interdependent wholesale markets (cited)
\cite{pape16_are_fundam_enoug}. Next to the balancing market at the end of the
sequence, mainly two different types of spot markets exist, the day-ahead
market and the intraday market. In this research, we consider the European Power
Exchange (EPEX Spot) as it is the largest electricity market in Europe, with a
total trading volume of approximately 567TWh in 2018\footnote{\url{https://www.epexspot.com/en/press-media/press/details/press/Traded\_volumes\_soar\_to\_an\_all-time\_high\_in\_2018},
accessed 19\textsuperscript{th} February, 2019\label{orge3bcfdf}}, but most electronic
spot markets in western economies work with similar market mechanisms.

In Germany, the most important spot market is the day-ahead market with a
trading volume of over 234TWh in 2018\textsuperscript{\ref{orge3bcfdf}}. Participants place asks and bids
for hourly contracts of the following day on the \emph{EPEX Spot Day-ahead Auction}
market until the market closes at 12pm on the day before delivery (see Figure
\ref{fig-electricity-markets}). The day-ahead market plays an essential role in
integrating volatile renewable energy sources (RES) into the power system
\cite{pape16_are_fundam_enoug}. Generators forecast the expected generation
capacity for the next day and sell those quantities on the market
\cite{karanfil17_role_contin_intrad_elect_market}. After the market closes, the
participants have the opportunity to trade the difference between the day-ahead
forecast and the more accurate intraday forecast on the intraday market
\cite{kiesel17_econom_analy_intrad_elect_prices}. In this way, RES generators can
cost effectively self-balance their portfolios, instead of relying on balancing
services provided by the TSO, which imposes high imbalance costs on participants
\cite{pape16_are_fundam_enoug}.

On the \emph{EPEX Spot Intraday Continuous} market, electricity products are traded
up until 5 minutes before physical delivery. Hourly contracts, as well as
15-minute and block contracts, can be traded. In contrast to the day-ahead
auction, the intraday market is a continuous order-driven market. Participants
can submit limit orders at any time during the trading window and equally change
or withdraw the order at any time before the order is accepted. Limit orders are
specified as price-quantity pairs: \((\Pi{}, \up{})\), where \(\Pi{}\) is the
traded amount of electrical power in kW and \(\up{}\) is the price for the delivered
energy unit (hour/quarter/block) in \(\emwh\). When an order to buy
(bid) matches an order to sell (ask), the trade immediately gets executed. The
order book is visible to all participants, hence it is known which unmatched
orders exist at the time of interest. The intraday market has a trading volume
of 82TWh, which is considerably smaller than day-ahead market's volume. Despite
that, the intraday market plays a vital role to the stability of the grid. All
executed trades on the intraday market potentially reduce the activation of
control reserve through the TSO.

Purchasing electricity on the continuous intraday market is attractive for EV
fleets with uncertain mobility demand. Due to the intradays market's short time
before delivery, EV fleet operators can rely on highly accurate forecasts of
available battery capacity to charge, before submitting an order to buy. In this
way, they can reliably charge at a potentially lower price at the intraday
market than the regular industry tariff. In an integrated bidding strategy, EV
fleet operators can, similarly to RES generators, balance out forecast errors of
available battery capacity on the intraday market. Trades on the intraday market
can complement bids that have been committed to other markets earlier (e.g., to
the secondary operating reserve market).
\subsection{EV Fleet Control in the Smart Grid}
\label{sec:org2947633}
The increasing penetration of EVs has a substantial effect on electricity
consumption patterns. During charging periods, power flows and grid losses
increase considerably and challenge the grid. Operators have to reinforce the
grid to ensure that transformers and substations do not overload
\cite{sioshansi12_impac_elect_tarif_plug_in,lopes11_integ_elect_vehic_elect_power_system}.
Loading multiple EVs in the same neighborhood, or worse, whole EV fleets at
once, stress the grid. In these cases, even brown- or blackouts can occur.
\cite{kim12_carbit}. Despite these challenges, it is possible to support the
physical reinforcement by adopting smart charging strategies. In smart charging,
EVs get charged when the grid is less congested to ensure grid stability. Smart
charging reduces peaks in electricity demand, called \emph{Peak Cutting}, and
complement the grid in times of low demand, called \emph{Valley Filling}. Smart
charging has been researched thoroughly in the IS literature, in the following
we will outline some of the most important contributions.


\textcite{valogianni14_effec_manag_elect_vehic_storag} found that using
intelligent agents to schedule EV charging substantially reshapes the energy
demand and reduces peak demand without violating individual household
preferences. Moreover, they showed that the proposed smart charging behavior
reduces average energy prices and thus benefit households economically. In
another study, \textcite{kara15_estim_benef_elect_vehic_smart} investigated the
effect of smart charging on public charging stations in California. Controlling
for arrival and departure times, the authors presented beneficial results for
the distribution system operator (DSO) and the owners of EVs. Their approach
resulted in a price reduction in energy bills and a peak load reduction. An
extension of the smart charging concept is Vehicle-to-Grid (V2G). When equipped
with V2G devices, EVs can discharge their batteries back into the grid. Existing
research has focused on this technology in respect to grid stabilization effects
and arbitrage possibilities. For instance,
\textcite{schill11_elect_vehic_imper_elect_market} showed that the usage of EVs
can decrease average consumer electricity prices. Excess EV battery capacity can
be used to charge in off-peak hours and discharge in peak hours, when the prices
are higher. These arbitrage possibilities reverse welfare effects of generators
and increase the overall welfare and consumer surplus.
\textcite{tomic07_using_fleet_elect_drive_vehic_grid_suppor} found that the
arbitrage opportunities are especially prominent when a high variability in
electricity prices on the target electricity market exists. The authors stated
that short intervals between the contract of sale and the physical delivery of
electricity increase arbitrage benefits. Consequently, ancillary service
markets, like frequency control and operating reserve markets, are attractive
for smart charging.

\textcite{peterson10_econom_using_plug_in_hybrid} investigated energy arbitrage
profitability with V2G in the light of battery depreciation effects in the US.
The results of their study indicate that large-scale use of EV batteries for
grid storage does not yield enough monetary benefits to incentivize EV owners to
participate in V2G activities. Considering battery depreciation cost, the
authors arrived at an annual profit of only 6\$ - 72\$ per EV.
\textcite{brandt17_evaluat_busin_model_vehic_grid_integ} evaluated a business
model for parking garage operators operating on the German frequency regulation
market. When taking infrastructure costs and battery depreciation costs into
account, they conclude that the proposed vehicle-grid integration is not
profitable. Even with idealized assumptions about EV adoption rates in Germany
and altered auction mechanisms, the authors arrived at negative profits.
\textcite{kahlen17_fleet} used EV fleets to offer balancing services to the grid.
Evaluating the impact of V2G in their model, the authors conclude that V2G would
only be profitable if reserve power prices were twice as high. Considering the
results from the studies mentioned above, our research does not include V2G,
since only marginal profits are expected.

In order to maximize profits, it is essential for market participants to develop
successful bidding strategies. Several authors have investigated bidding
strategies to jointly participate in multiple markets
\cite{mashhour11_biddin_strat_virtual_power_plant_2,he16_optim_biddin_strat_batter_storag}.
\textcite{mashhour11_biddin_strat_virtual_power_plant_2} used stationary battery
storage to participate in the spinning reserve market and the day-ahead market
at the same time. The authors developed a non-equilibrium model, which solves
the presented mixed-integer program with Genetic Programming (GP). Contrarily,
we use a model-free RL agent that learns an optimal policy (i.e., a trading
strategy) from actions it takes in the environment (i.e., bidding on electricity
markets). Using a model-free approach is especially beneficial for us, since
additional unknown variables and constraints (i.e., customer mobility demand)
complicate the formulation of a mathematical model.

\textcite{he16_optim_biddin_strat_batter_storag} conducted similar research to
\textcite{mashhour11_biddin_strat_virtual_power_plant_2}. The authors additionally
incorporated battery life cycle in their profit maximization model, which proved
to be a decisive factor. In contrast to the authors, we jointly participated in
the secondary operating reserve and spot market with the \emph{non-stationary}
storage of EV batteries. Because shared EVs have to satisfy mobility demand,
they have to be charged in any case, which allows us to safely exclude battery
depreciation from our model. Further, we chose the intraday market over the
day-ahead market, as it has the lowest reaction time among the spot markets, and
thus potentially offers higher profits
\cite{tomic07_using_fleet_elect_drive_vehic_grid_suppor}.

Previous studies often assume that car owners or households can directly trade
on electricity markets. In reality, this is not possible due to the minimum
capacity requirements of the markets, requirements that single EVs do not meet.
For example, the German Control Reserve Market (GCRM) has a minimum trading
capacity of 1MW to 5MW, depending on the specific market. In order to reach the
minimum capacity, over 200 EVs would need to be connected to the grid via a
standard 4.6kW charging station at the same time. \textcite{ketter13_power_tac}
introduced the notion of electricity brokers, aggregators that act on behalf of
a group of individuals or households to participate in electricity markets.
\textcite{brandt17_evaluat_busin_model_vehic_grid_integ} and
\textcite{kahlen14_balan_with_elect_vehic} successfully showed that electricity
brokers can overcome the capacity issues by aggregating EV batteries. In
addition to electricity brokers, we apply the concept of Virtual Power Plants
(VPPs). VPPs are flexible portfolios of distributed energy resources, which are
presented with a single load profile to the system operator, making them
eligible for market participation and ancillary service provisioning
\cite{pudjianto07_virtual_power_plant_system_integ}. Hence, VPPs allow providing
regulation capacity to the market without knowing which exact sources provide
the promised capacity until the delivery time \cite{kahlen17_fleet}. This concept
is specially useful when dealing with EV fleets: VPPs enable carsharing
providers to issue bids and asks based on an estimate of available fleet
capacity, without knowing beforehand which exact EVs will provide the capacity
at the time of delivery. Based on the battery charge and the availability of
EVs, an intelligent agent decides in real-time which vehicles provide the
capacity.

Centrally managed EV fleets make it possible for carsharing providers to use the
presented concepts as a viable business extension. Free float carsharing is a
popular concept that allows cars to be picked up and parked everywhere, and the
customers are billed is by the minute. Free float carsharing offers flexibility
to its users, saves resources, and reduces carbon emissions
\cite{firnkorn15_free_float_elect_carsh_fleet_smart_cities}. Most previous studies
concerned with the usage of EVs for electricity trading, assumed that trips are
fixed and known in advance, e.g., in
\textcite{tomic07_using_fleet_elect_drive_vehic_grid_suppor}. The free float
concept adds uncertainty and nondeterministic behavior, which make predictions
about future rentals a complex issue.

\textcite{kahlen17_fleet} showed that it is possible to use free float carsharing
fleets as VPPs to profitably offer balancing services to the grid. In their
study, the authors compared cases from three different cities across Europe and
the US. They used an event-based simulation, bootstrapped with real-world
carsharing and secondary operating reserve market data from the respective
cities. A central dilemma within their research was to decide whether an EV
should be committed to a VPP or free for rent. Since rental profits are
considerably higher than profits from electricity trading, it is crucial not to
allocate an EV to a VPP when it could have been rented out otherwise. To deal
with the asymmetric payoff, \citeauthor{kahlen17_fleet} used stratified sampling
in their classifier. This method gives rental misclassifications higher weights,
reducing the likelihood of EVs to participate in VPP activities. The authors
used a Random Forest regression model to predict the available balancing
capacity on an aggregated fleet level. Only at the delivery time, the agent
decides which individual EVs provide the regulation capacity. This heuristic is
based on the likelihood that the vehicle is rented out and on its expected
rental benefits.

In a similar study, the authors showed that carsharing companies can participate
in day-ahead markets for arbitrage purposes
\cite{kahlen18_elect_vehic_virtual_power_plant_dilem}. In the paper, the authors
used a sinusoidal time-series model to predict the available trading capacity.
Another central problem for carsharing providers is that committed trades, which
can not be fulfilled, result in substantial penalties from the system operator
or electricity exchange. In other words, fleet operators have to avoid buying
any amount of electricity, which they can't be sure to charge with available EVs
at the delivery time. To address this issue, the authors developed a mean
asymmetric weighted (MAW) objective function. They used it for their time-series
based prediction model, to penalize committing an EV to VPP when it would have
been rented out otherwise. Because of the two issues mentioned above,
\textcite{kahlen18_elect_vehic_virtual_power_plant_dilem} could only make very
conservative estimations and commitments of overall available trading capacity,
resulting in a high amount of missed profits. This effect is especially
prominent when participating in the secondary operating reserve market, since
commitments have to be made one week in advance when mobility demands are still
uncertain. \textcite{kahlen17_fleet} stated that in 42\% to 80\% of the cases, EVs
are \emph{not} committed to a VPP when it would have been profitable to do so.

This thesis proposes a solution in which the EV fleet participates in the
balancing market and intraday market simultaneously. With this approach, we
align the potentially higher profits on the balancing markets, with more
accurate capacity predictions for intraday markets
\cite{tomic07_using_fleet_elect_drive_vehic_grid_suppor}. This research followed
\textcite{kahlen17_fleet}, who proposed to work on a combination of multiple
markets in the future.

\subsection{Reinforcement Learning Controlled EV Charging}
\label{sec:orgc00e906}

Previous research shows that intelligent agents equipped with Reinforcement
Learning (RL) methods can successfully take action in the smart grid. The
following chapter outlines different research approaches of RL in the domain of
smart grids. For a more thorough description, mathematical formulations and
common issues, of RL refer to Chapter \ref{sec-reinforcement-learning}.

\textcite{reddy11_learn_behav_multip_auton_agent,reddy11_strat} used autonomous
broker agents to buy and sell electricity from DER on a proposed \emph{Tariff
Market}. The agents use Markov Decision Processes (MDPs) and RL to learn pricing
strategies to profitably participate in the Tariff Market. To control for a
large number of possible states in the domain, the authors used \emph{Q-Learning}
with derived state space features. Based on descriptive statistics, they defined
derived price and market participant features. By engaging with its environment,
the agent learns an optional sequence of actions (policy) based on the state of
the agent. \textcite{peters13_reinf_learn_approac_to_auton} built on that work and
further enhanced the method by using function approximation. Function
approximation allows to efficiently learn strategies over large state spaces, by
deriving a function that describes the states instead of defining discrete
states. By using this technique, the agent can adapt to arbitrary economic
signals from its environment, resulting in better performance than previous
approaches. Moreover, the authors applied feature selection and regularization
methods to explore the agent's adaption to the environment. These methods are
particularly beneficial in smart markets because market design, structures, and
conditions might change in the future. Hence, intelligent agents should be able
to adapt to it \cite{peters13_reinf_learn_approac_to_auton}.

\textcite{vandael15_reinf_learn_heuris_ev_fleet} facilitated learned EV fleet
charging behavior to optimally purchase electricity on the day-ahead market.
Similarly to \textcite{kahlen18_elect_vehic_virtual_power_plant_dilem}, the
problem is framed from the viewpoint of an aggregator that tries to define a
cost-effective day-ahead charging plan in the absence of knowing EV charging
parameters, such as departure time. A crucial point of the study is weighting
low charging prices against costs that have to be paid when an excessive or
insufficient amount of electricity is bought from the market (imbalance costs).
Contrarily, \textcite{kahlen18_elect_vehic_virtual_power_plant_dilem} did not
consider imbalance cost in their model and avoid them by sacrificing customer
mobility in order to balance the market (i.e., not showing the EV available for
rent, when it is providing balancing capacity).
\textcite{vandael15_reinf_learn_heuris_ev_fleet} used a \emph{fitted Q Iteration} to
control for continuous variables in their state and action space. In order to
achieve fast convergence, they additionally optimized the \emph{temperature step}
parameter of the Boltzmann exploration probability.

\textcite{dusparic13_multi} proposed a multi-agent approach for residential demand
response. The authors investigated a setting in which 9  EVs were connected to
the same transformer. The RL agents learned to charge at minimal costs, without
overloading the transformer. \textcite{dusparic13_multi} utilized \emph{W-Learning} to
simultaneously learn multiple policies (i.e., objectives such as ensuring
minimum battery charged or ensuring charging at low costs).
\textcite{taylor14_accel_learn_trans_learn} extended this research by employing
Transfer Learning and \emph{Distributed W-Learning} to achieve communication between
the learning processes of the agents in a multi-objective, multi-agent setting.
\textcite{dauer13_market_based_ev_charg_coord} proposed a market-based EV fleet
charging solution. The authors introduced a double-auction call market where
agents trade the available transformer capacity, complying with the minimum
required State of Charge (SoC). The participating EV agents autonomously learn
their bidding strategy with standard \emph{Q-Learning} and discrete state and action
spaces.

\textcite{di13_elect_vehic} presented a multi-agent solution to minimize charging
costs of EVs, a solution that requires neither prior knowledge of electricity
prices nor future price predictions. Similar to
\textcite{dauer13_market_based_ev_charg_coord}, the authors employed standard
\emph{Q-Learning} and the \(\epsilon\)-greedy approach for action selection.
\textcite{vaya14_optim} also proposed a multi-agent approach, in which the
individual EVs are agents that actively place bids in the spot market. Again,
the agents use \emph{Q-Learning}, with an \(\epsilon\)-greedy policy to learn their
optimal bidding strategy. The latter relies on the agents willingness-to-pay
which depends on the urgency to charge. State variables, such as SoC, time of
departure and price development on the market, determine the urgency to charge.
The authors compared this approach with a centralized aggregator-based approach
that they developed in another paper \cite{vaya15_optim_biddin_strat_plug_in}.
Compared to the centralized approach, in which the aggregator manages charging
and places bids for the whole fleet, the multi-agent approach causes slightly
higher costs but solves scalability and privacy problems.

\textcite{shi11_real} consider a V2G control problem, while assuming real-time
pricing. The authors proposed an online learning algorithm which they modeled as
a discrete-time MDP and solved through \emph{Q-Learning}. The algorithm controls the
V2G actions of the EV and can react to real-time price signals of the market. In
this single-agent approach, the action space compromises only charging,
discharging and regulation actions. The limited action spaces makes it
relatively easy to learn an optimal policy.
\textcite{chis16_reinf_learn_based_plug_in} looked at reducing the costs of
charging for a single EV using known day-ahead prices and predicted next-day
prices. A Bayesian ANN was employed for prediction and \emph{fitted Q-Learning} was
used to learn daily charging levels. In their research, the authors used
function approximation and batch reinforcement learning, an offline, model-free
learning method. \textcite{ko18_mobil_aware_vehic_to_grid} proposed a centralized
controller for managing V2G activities in multiple microgrids. The proposed
method considers mobility and electricity demands of microgrids, as well as SoC
of the EVs. The authors formulated a MDP with discrete state and action spaces
and use standard \emph{Q-Learning} with \(\epsilon\)-greedy policy to derive an optimal
charging policy. The approach takes microgrid autonomy and electricity prices
into special consideration.

It should be noted that advanced RL methods and techniques are not the only
solutions for problems in the smart grid, often basic algorithms and heuristics
provide satisfactory results \cite{vazquez-canteli19_reinf_learn_deman_respon}.
Despite that, our paper considers RL as an optimal fit for the design of our
proposed intelligent agent. Given the ability to learn user behavior (e.g.,
mobility demand) and the flexibility to adapt to the environment (e.g.,
electricity prices), RL methods are a promising way of solving complex
challenges in smart grids.

\subsection{Reinforcement Learning Theory \label{sec-reinforcement-learning}}
\label{sec:org0ff1111}
The following chapter will give an overview of the most important Reinforcement
Learning (RL) concepts and will introduce the corresponding mathematical
formulations. If not noted otherwise, the notation, equations, and insights are
adopted from \cite{sutton18_reinf}, the de-facto reference book of RL research.

RL is an agent-based machine learning algorithm in which the agent learns to
perform an optimal set of actions through interaction with its environment. The
agents objective is to maximize the rewards it receives based on the actions it
takes. Immediate rewards have to be weighted against long-term cumulative
returns that also depend on the agent's future actions. The RL problem is
formalized as Markov Decision Processes (MDPs) which will be introduced in
Chapter \ref{sec-mdp}. A critical task of RL agents is to continuously estimate
the value of the environment's state. Values indicate the long-term desirability
of a state, that is the total amount of reward the agent can expect to
accumulate over the future, following a learned set of actions, called the
policy. Policies and values are covered in Chapter \ref{sec-policies}, whereas the
core mathematical foundations for evaluating policies and updating value
functions are introduced in Chapter \ref{sec-bellman}. When the model of the
environment is fully known, the learning problem is reduced to a planning
problem (Chapter \ref{sec-dp}) in which optimal policies can be computed with
iterative approaches. Model-free RL approaches can be applied when rewards and
state transitions are unknown, and the agent's behavior has to be learned from
experience (Chapter \ref{sec-td-learning}). The last two chapter cover methods
that solve the RL problem more efficiently, tackle new challenges and are widely
used in practice and research.

\subsubsection{Markov Decision Processes \label{sec-mdp}}
\label{sec:orgf12dad2}
Markov Decision Processes (MDPs) are a classical formulation of sequential
decision making and an idealized mathematical formulation of the RL problem. MDPs
allow to derive exact theoretical statements about the learning problem and
possible solutions. Figure
\ref{agent-environment-interaction} depicts the \emph{agent-environment interaction}.
\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\linewidth]{fig/mdp_interaction.png}
\caption[Markov Decision Process]{The agent-environment interaction in a Markov decision process \cite{sutton18_reinf} \protect\footnotemark \label{agent-environment-interaction}}
\end{figure}
\footnotetext{\textbf{Figure 3.1} from "Reinforcement Learning: An Introduction" by Richard S. Sutton and Andew G. Barto is licencsed under CC BY-NC-ND 2.0 (https://creativecommons.org/licenses/by-nc-nd/2.0/)}

In RL the agent and the environment continuously interact with each other. The
agent takes actions that influence the environment, which in return presents
rewards to the agent. The agent's goal is to maximize rewards over time, trough
an optimal choice of actions. In each discrete timestep \(t\!=\!0,1,2,...,T\) the
RL agent interacts with the environment, which is perceived by the agent as a
representation, called \emph{state}, \(S_t \in \S\). Based on the state, the agents
selects an \emph{action}, \(A_t\in\A\), and receives a numerical \emph{reward} signal,
\(R_{t+1}\in\R\subset\Re\), in the next timestep. Actions influence immediate
rewards and successive states, and consequently also influence future rewards.
The agent has to continuously make a trade-off between immediate rewards and
delayed rewards to achieve its long-term goal.

The \emph{dynamics} of a MDP are defined by the probability that a state \(s'\in \S\)
and a reward \(r\in\R\) occurs, given the preceding state \(s\in\S\) and action
\(a\in\A\). In \emph{finite} MDPs, the random variables \(R_t\) and \(S_t\) have
well-defined probability density functions (PDF), which are solely dependent on
the previous state and action. Consequently, it is possible to define (\(\defeq\))
the \emph{dynamics} of the MDP as follows:
\begin{equation}
    p(s',r|s,a) \defeq \Pr{S_t=s',R_t=r|S_{t-1}=s,A_t=a},
\end{equation}
for all \(s',s\!\in\!\S\), \(r\!\in\!\R\) and \(a\!\in\!\A\). Note that each possible
value of the state \(\S_t\) depends only on the immediately preceding state
\(\S_{t-1}\). When a state includes all information of \emph{all} previous states, the
state possesses the so-called \emph{Markov property}. If not noted otherwise, the
Markov property is assumed throughout the whole chapter. The dynamics function
allows computing the \emph{state-transition probabilities}, another important
characteristic of an MDP, as follows:
\begin{equation}
    p(s'|s,a) \defeq \Pr{S_t\!=\!s'|S_{t-1}\!=\!s,A_t\!=\!a} = \sum_{r\in\R}{p(s', r|s, a)},
\end{equation}
for \(s',s\!\in\!\S\), \(r\!\in\!\R\) and \(a\!\in\!\A\).

The use of a \emph{reward signal} \(R_t\) to formalize the agent's goal is a unique
characteristic of RL. Each timestep the agent receives the rewards as a scalar
value \(\R_t\in\Re\). The sole purpose of the RL agent is to maximize the
long-term cumulative reward (as opposed to the immediate reward). The long-term
cumulative reward can also be expressed as the \emph{expected return} \(G_t\):
\begin{equation} \label{eq-expected-return}
\begin{split}
    G_t &\defeq R_{t+1} + \gamma R_{t+2} + \gamma R_{t+3} + \cdots \\
    &= \sum_{k=0}^{\infty}{\gamma^k R_{t+k+1}} \\
    &= R_{t+1} + \gamma G_{t+1},
\end{split}
\end{equation}
where \(\gamma\), \(0\leq\gamma\leq 1\), is the \emph{discount rate} parameter. The
discount rate determines how "myopic" the agent is. If \(\gamma\) approaches 0,
the agent is more concerned with maximizing immediate rewards. On the contrary,
when \(\gamma\!=\! 1\), the agent takes future rewards strongly into account, the
agent is "farsighted".

\subsubsection{Policies and Value Functions \label{sec-policies}}
\label{sec:orgb45cbac}
An essential task of almost every RL agent is estimating \emph{value functions}.
These functions describe how "good" it is to be in a given state, or how "good"
it is to perform an action in a given state. More formally, they take a state
\(s\) or a state-action pair \(s,a\) as input and give the expected return \(G_t\) as
output. The expected return is dependent on the actions the agent will take in
the future. Consequently, value functions are formulated with respect to a
\emph{policy} \(\pi\). A policy is a mapping of states to actions; it describes the
probability that an agent performs a certain action, based on the current state.
More formally, the policy is defined as
\(\pi(a|s)\defeq\Pr{A_t\!=\!a|S_t\!=\!s}\), a PDF of all \(a\!\in\!\A\) for each
\(s\!\in\!\S\). RL approaches mainly differ in how the policy is updated, based on
the agent's interaction with the environment.

In RL, value functions of states and value functions of state-action pairs are
used. The \emph{state-value function of policy} \(\pi\) is denoted as \(\vpi(s)\) and is
defined as the expected return when starting in \(s\) and following policy \(\pi\):
\begin{equation}
    \vpi(s) \defeq \EE{\pi}{G_t|S_t\!=\!s}, \text{ for all } s\in\S
\end{equation}
The \emph{action-value function of policy} \(\pi\) is denoted as \(\qpi(s,a)\) and is
defined as the expected return when starting in \(s\), taking action \(a\) and
following policy \(\pi\) afterwards:
\begin{equation}
    \qpi(s,a) \defeq \EE{\pi}{G_t|S_t\!=\!s, A_t\!=\!a}, \text{ for all } a\in\A, s\in\S
\end{equation}
The \emph{optimal policy} \(\pi_*\) has a greater (or equal) expected return than all
other policies. The \emph{optimal} state-value function and \emph{optimal} action-value
function are defined as follows:
\begin{equation}
    \vstar(s) \defeq \max_{\pi} \vpi(s), \text{ for all } s\in\S
\end{equation}
\begin{equation}
    \qstar(s,a) \defeq \max_{\pi} \qpi(s,a), \text{ for all } s\in\S, a\in\A
\end{equation}
The \emph{optimal} action-value function describes the expected return when taking
action \(a\) in state \(s\) following the optimal policy \(\pi_*\) afterwards.
Estimating \(\qstar\) to obtain an optimal policy is a substantial part of RL and
has been known as \emph{Q-learning} \cite{watkins92_q_learn}, which is described in
Chapter \ref{sec-td-learning}.

\subsubsection{Bellman Equations \label{sec-bellman}}
\label{sec:orgeff66b0}
A central characteristic of value functions is the recursive relationship
between the values. Similar to Equation (\ref{eq-expected-return}), current values
are related to expected values of successive states. This relationship is
heavily used in RL and has been formulated as \emph{Bellman equations}
\cite{bellman57_dynam_progr}. The Bellman equation for \(\vpi(s)\) is defined as
follows:
\begin{equation} \label{eq-bellman}
\begin{split}
    \vpi(s) &\defeq \EE{\pi}{G_t|S_t=s} \\
    &= \EE{\pi}{R_{t+1}+\gamma G_{t+1}|S_t\!=\!s} \\
    &= \sum_{a}{\pi(a|s)}\sum_{s',r}{p(s',r|s,a)}\bigg[r+\gamma\vpi(s')\bigg],
\end{split}
\end{equation}
where \(a\!\in\!\A\), \(s,s'\!\in\!\S\), \(r\!\in\!\R\).
In other words, the value of a state equals the immediate reward plus the
expected value of all possible successor states, weighted by their probability
of occurring. \(\vpi(s)\) is the only solution to its Bellman equation. The
Bellman equation of the optimal value function \(v_*\) is called the \emph{Bellman
optimality equation}:
\begin{equation} \label{eq-bellman-optimality}
\begin{split}
    \vstar(s) &\defeq \max_{a\in\A(s)}q_{\pi_*}(s,a) \\
    &= \max_{a}\EE{\pi_*}{R_{t+1}+\gamma G_{t+1}|S_t\!=\!s, A_t\!=a} \\
    &= \max_{a}\EE{\pi_*}{R_{t+1}+\gamma \vstar(S_{t+1})|S_t\!=\!s, A_t\!=a} \\
    &= \max_{a}\sum_{s',r}{p(s',r|s,a)}\bigg[r+\gamma\vstar(s')\bigg]
\end{split}
\end{equation}
where \(a\!\in\!\A\), \(s,s'\!\in\!\S\), \(r\!\in\!\R\). In other words, the value of
a state under an optimal policy equals the expected return for the best action
from that state. Note that the Bellman optimality equation does not refer to a
specific policy, it has a unique solution independent from one. It can be seen
as an equation system, which can be solved when the dynamics of the environment
\(p\) are known. Similar Bellman equations to Equations (\ref{eq-bellman}) and
(\ref{eq-bellman-optimality}) can also be formed for \(\qpi(s,a)\) and
\(\qstar(s,a)\). Bellman equations form the basis for computing and approximating
value functions and were an important milestone in RL history. Most RL methods
are \emph{approximately} solving the Bellman optimality equation, by using
experienced state transitions instead of expected transition probabilities. The
most common methods will be explored in the following chapters.

\subsubsection{Dynamic Programming \label{sec-dp}}
\label{sec:orgacd9d1c}
\emph{Dynamic Programming} (DP) is a method to compute optimal policies, the primary
goal of every RL method. DP makes use of value functions to facilitate the
search for good policies. Once an optimal value function, (i.e., one that
satisfies the Bellman optimality equation) is found, optimal policies can be
easily obtained. Despite the limited utility of DP in real-world settings, it
provides the theoretical foundation for all other RL methods. In fact, all of
the RL methods try to achieve the same goal, but without the assumption of a
perfect model of the environment and less computational effort. Because DP assumes
full knowledge of the environment, it is known as \emph{planning}, in which optimal
solutions are \emph{computed}. In \emph{control} problems (Chapter \ref{sec-td-learning}),
optimal solutions are \emph{learned} from an unknown environment.

The two most popular DP algorithms that compute optimal policies are called
\emph{policy iteration} and \emph{value iteration}. These methods perform "sweeps" through
the whole state set and update the estimated value of each state via an
\emph{expected update} operation. In policy iteration, a value function for a given
policy \(\vpi\) needs to be computed first, a step called \emph{policy evaluation}. A
sequence of approximated value functions \(\{v_k\}\) are updated using the Bellman
equation for \(\vpi\) (Eq. \ref{eq-bellman}) until convergence to \(\vpi\) is
achieved. After computing the value function for a given policy, it is possible
to modify the policy and see if the value \(\vpi(s)\) for a given state increases
(\emph{policy improvement}). A way of doing this, is evaluating the action-value
function \(\qpi(s,a)\) by \emph{greedily} taking the best short-term action \(a\!\in\!A\)
at a given timestep. Alternating between these two steps monotonically improves
the policies and the value functions until they converge to the optimum. This
algorithm is called \emph{policy iteration}:
\begin{equation}
    \pi_0 \xrightarrow{\text{ E }} v_{\pi_0} \xrightarrow{\text{ I }}
    \pi_1 \xrightarrow{\text{ E }} v_{\pi_1} \xrightarrow{\text{ I }}
    \pi_2 \xrightarrow{\text{ E }} \hdots \xrightarrow{\text{ I }}
    \pi_* \xrightarrow{\text{ E }} \vstar,
\end{equation}
where \(\xrightarrow{\text{ E }}\) denotes a policy evaluation step,
\(\xrightarrow{\text{ I }}\) denotes a policy improvement step. \(\pi_*\) and
\(\vstar\) are the optimal policy and optimal value function, respectively. Note
that in each iteration of the policy iteration algorithm, a policy evaluation
has to be performed, which requires multiple sweeps through the state space. In
\emph{value iteration}, the policy evaluation step is stopped after one sweep. In
this case, the two previous steps can be combined into one single update step:
\begin{equation}
\begin{split}
    v_{k+1}(s) &\defeq \max_a \EE{}{R_{t+1}+\gamma \vstar(S_{t+1})|S_t\!=\!s, A_t\!=a} \\
    &= \max_{a}\sum_{s',r}{p(s',r|s,a)}\bigg[r+\gamma v_k(s')\bigg],
\end{split}
\end{equation}
where \(a\!\in\!\A\), \(s,s'\!\in\!\S\), \(r\!\in\!\R\). It can be shown, that for any
given \(v_0\), the sequence \({v_k}\) converges to the optimal value function
\({\vstar}\). In value iteration, the Bellman optimality equation
(\ref{eq-bellman-optimality}) is simply turned into an update rule. Both of the
algorithms can be effectively used to compute optimal values and value function
in finite MDPs with a perfect model of the environment.

\subsubsection{Temporal-Difference Learning \label{sec-td-learning}}
\label{sec:orgc4a603f}
The previous chapter dealt with solving a \emph{planning} problem, that is computing
an optimal solution (i.e., an optimal policy \(\pi_*\)) of an MDP when a perfect
model of the environment is known. In the following chapters, we will look at
\emph{model-free} prediction and \emph{model-free} control. As opposed to planning,
model-free methods learn from experience and require no prior knowledge of the
environment. Remarkably, these methods can still achieve optimal behavior.

The \emph{TD prediction problem} is concerned with estimating state-values \(\vpi\)
using past experiences of following a given policy \(\pi\). TD methods update an
estimate \(V\) of \(\vpi\) in every timestep. At time \(t\!+\!1\) they immediately
perform an update operation on \(V(S_t)\). Because of the step-by-step nature of
TD learning, it is categorized as \emph{online learning}. Also note that TD methods
perform update operations on value estimates based on other learned estimates, a
procedure called \emph{bootstrapping}. In simple TD prediction, the
value estimates \(V\) are updated as follows:
\begin{equation} \label{eq-td-prediction}
    V(S_t) \leftarrow V(S_t) + \alpha\big[R_{t+1}+\gamma V(S_{t+1}) - V(S_t)\big],
\end{equation}
where \(\alpha\) is a constant \emph{step-size} parameter and \(\gamma\) is the
\emph{discount rate}. Here, the update of the state-value is performed using the
observed reward \(R_{t+1}\) and the estimated value \(V(S_{t+1})\).

When a model is not available, it is useful to estimate \emph{action-values}, instead
of \emph{state-values}. If the environment is completely known, it is possible for
the agent to look one step ahead and select the best action. Without that
knowledge, the value of each action in a given state needs to be estimated. The
latter constitutes a problem, since not every \emph{state-action} pair will be
visited when the agent follows a deterministic policy. A deterministic policy
\(\pi(a|s)\) returns exactly one action given the current state, hence the agent
will only observe returns for one of the actions. In order to evaluate the value
function for all \emph{state-action} pairs \(\qpi\), continuous \emph{exploration} needs to
be ensured. In other words, the agent has to explore state-action pairs which
are seemingly disadvantageous given the current policy. This dilemma is also
known as the \emph{exploration-exploitation} trade-off. One way to achieve exploration
is using \emph{stochastic} policies for the action selection. Stochastic policies
have a non-zero probability of selecting each action in each state. A typical
stochastic policy is the \emph{\(\epsilon\)-greedy policy}, which selects the action with
the highest estimated value, except for a probability \(\epsilon\), it selects an
action at random.
\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\linewidth]{fig/on-policy.png}
\caption[On-policy control with Sarsa]{On-policy control with Sarsa \cite{sutton18_reinf}. \protect\footnotemark \label{fig-sarsa}}
\end{figure}
\footnotetext{The in-text figure of \textbf{Chapter 5.3} from "Reinforcement Learning: An Introduction" by Richard S. Sutton and Andew G. Barto is licencsed under CC BY-NC-ND 2.0 (https://creativecommons.org/licenses/by-nc-nd/2.0/)}

There are two approaches to make use of stochastic policies to ensure all
actions are chosen infinitely often. On-policy methods improve the (stochastic)
decision policy, by continually estimating \(\qpi\) in regard to \(\pi\), while
simultaneously driving \(\pi\) towards \(\qpi\), e.g., with a \(\epsilon\)-greedy action
selection. Figure \ref{fig-sarsa} depicts this learning process. Off-policy
methods improve the deterministic decision policy, by using a second stochastic
policy to generate behavior. The first policy is becoming the optimal policy by
evaluating the exploratory behavior of the second policy. Off-policy approaches
are considered more powerful than on-policy approaches and have a variety of
additional use cases. On the other side, they often have a higher variance and
take more time to converge to an optimum.

A popular on-policy TD control method is Sarsa, developed by
\textcite{rummery94_q}. In the prediction step, the action-value function
\(\qpi(s,a)\) of all actions and states has to be estimated for the current
policy \(\pi\). The estimation can be done similar to TD prediction of state
values (Eq. \ref{eq-td-prediction}). Instead of considering state transitions,
state-action transitions are considered in this case. The update rule is
constructed as follows:
\begin{equation}
    Q(S_t, A_t) \leftarrow Q(S_t,A_t) + \alpha\big[R_{t+1}+\gamma Q(S_{t+1},A_{t+1}) - Q(S_t, A_t)\big]
\end{equation}
After every transition from a state \(S_t\), an update operation using the events
\((S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1})\) is performed. This quintuple also
constituted the name Sarsa. The on-policy control step of the algorithm is
straightforward, and uses a \(\epsilon\)-greedy policy improvement, as described in
the previous paragraph. It has been shown that Sarsa converges to the optimal
policy \(\pi_*\) under the assumption of infinite visits to all state-action
pairs.

A breakthrough in RL has been achieved when \textcite{watkins92_q_learn} developed
the \emph{off-policy} TD control algorithm, called Q-learning. The update rule is
defined as follows:
\begin{equation}
    Q(S_t, A_t) \leftarrow Q(S_t,A_t) + \alpha\big[R_{t+1}+\gamma\max_a Q(S_{t+1},a) - Q(S_t, A_t)\big]
\end{equation}
Here, the estimated action-values \(Q\) are updated towards the highest estimated
action-value of the next time step. In this way, \(Q\) directly approximates the
optimal action-value function \(q_*\), independently of the policy the agent
follows. Due to this simplification, Q-learning is a widely used model-free
method, and its convergence can be proved easily.


This chapter covered the most important RL methods. They work online, learn from
experience, and can be easily applied to real-world problems with low
computational effort. Moreover, the mathematical complexity of the presented
approaches is limited, and they can be easily implemented into computer
programs. Temporal-Difference learning is a \emph{tabular} method, in which Q-values
are stored and updated in a lookup table. If the state and action spaces are
continuous or the number of states and actions is very large, a table
representation is computational infeasible and the speed of convergence is
drastically reduced. In this case, a \emph{function approximator} can replace the
lookup table. The next chapter will briefly cover function approximation, as
well as other advancements in RL.
\subsubsection{Approximation Methods}
\label{sec:orgbe637a3}
Up to this point, only tabular RL methods have been covered, which form the
theoretical foundation of RL in general. But in many real-world use cases, the
state space is enormous and it is improbable to find an optimal value function
with tabular methods. Not only is it a problem to store such a large table in
the memory, but also would it take an almost infinite amount of time to fill
every entry with meaningful results. Contrarily, \emph{function approximation} tries
to find a function that approximates the optimal value function as closely as
possible, with limited computational resources. The experience with a small
subset of visited states is generalized to approximate values of the whole state
set. Function approximation has been widely studied in supervised machine
learning: Gradient methods, as well as linear and non-linear models have shown
good results for RL.

The approximated value of a state \(s\) is denoted as the parameterized
functional form \(\hat v(s,\w)\!\approx\!\vpi(s)\), given a weight vector
\(\w\!\in\!\Re^d\). Function approximation methods are approximating \(\vpi\) by
learning (i.e., adjusting) the weight vector \(\w\) from the experience of following
the policy \(\pi\). By assumption, the dimensionality \(d\) of \(\w\) is much lower than
the number of states, which is the reason for the desired generalization effect:
Adjusting one weight affects the values of many states. However, optimizing
an estimate for one state negatively affects the accuracy of the estimates for
other states. This effect motivates the specification of a state distribution
\(\mu(s)\), which represents the importance of the prediction error for each state.
In on-policy prediction, \(\mu(s)\) is often selected to be proportion of time spend
in each state \(s\). The prediction error of a state is defined as the squared
difference between the predicted (i.e., approximated) value \(\hat v(s,\w)\) and
the true value \(\vpi(s)\). Consequently, the objective function of the supervised
learning problem can be defined as the \emph{Mean Squared Value Error} \(\MSVEm\),
which weights the prediction error with the state distribution \(\mu(s)\):
\begin{equation}
    \MSVEm(\w) \defeq \sum_{s\in\S}{\mu(s)\bigg[\vpi(s)-\hat v(s,\w)\bigg]^2}, \text{ where } \w\in\Re^d
\end{equation}
Minimizing \(\MSVEm\) in respect to \(\hat v\) will yield a value function, which
facilitates finding a better policy --- the primary goal of RL. Remember that \(\hat v\)
can take any form of a linear or non-linear function of the state \(s\).

In practice, deep artificial neural networks networks (ANNs) have shown great
success as function approximators, which coined the term \emph{Deep Reinforcement
Learning}
\cite{mnih15_human_level_contr_throug_deep_reinf_learn,silver16_master_game_go_with_deep}.
A simple feedforward ANN can be found in Figure \ref{fig-ann}. ANNs have the
advantage that they can theoretically approximate any continuous function by
adjusting the connection weights of the network \(\w\in\Re^{d\times d}\)
\cite{cybenko89_approx_by_super_sigmoid_funct}. Advancements in the field of \emph{Deep
Learning} facilitated remarkable performance improvements in RL applications.
Despite that, the RL theory is mostly limited to tabular and linear
approximation methods. Refer to \textcite{bengio09_learn_deep_archit_ai} for a
comprehensive review of deep learning methods.
\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\linewidth]{fig/ann.png}
\caption[Artificial Neural Network]{A sample ANN consisting of four input nodes, two fully connected hidden layers and two output nodes \cite{sutton18_reinf}. \protect\footnotemark \label{fig-ann}}
\end{figure}
\footnotetext{\textbf{Figure 9.14} from "Reinforcement Learning: An Introduction" by Richard S. Sutton and Andew G. Barto is licencsed under CC BY-NC-ND 2.0 (https://creativecommons.org/licenses/by-nc-nd/2.0/)}
\subsubsection{Further Topics}
\label{sec:org22023a6}
The previous chapters provided a detailed overview of the most important
concepts and mathematical foundations in RL. In the research there are many more
topics that were not covered here. \emph{Eligibility traces} offer a way to more
general learning and faster convergence rates. Almost any TD method can be
extended to use eligibility traces, a popular methods is called Watkins's Q(\(\lambda\))
\cite{watkins89_learn_from_delay_rewar}. \emph{Fitted-Q Iteration} \cite{ernst03_iterat}
combined Q-learning and fitted value iteration with batch-mode RL. In
batch-mode the whole dataset is available offline, contrary to online RL where
the data is acquired by the agent's action in its the environment.
\emph{Actor-critic} methods \cite{sutton84_tempor_credit_assig_reinf_learn} directly
learn a parameterized policy instead of action-values, which inherently allow
continuous state spaces and learning appropriate levels of exploration.
Simultaneously to learning the policy, they approximate a state-value function,
which serves as a "critic" to the learned policy, the "actor". In the current
theory most RL models are single-agent models. For certain real-world
applications multi-agent RL algorithms are necessary to coordinate interaction
between the agents. When multiple learning agents interact with a non-stationary
environment, convergence and stability are a serious problem. \emph{W-learning}
\cite{humphrys96_action_selec_method_using_reinf_learn} is an multi-agent approach
that aims to solve these difficulties.

\clearpage

\section{Empirical Setting}
\label{sec:org6da82fe}
This research is embedded in the German carsharing and electricity markets.
Germany is a suitable testbed, since it has a comparably high share of
renewables in its energy mix and is pushing for an energy turnaround (German:
\emph{Energiewende}) since 2010 \cite{bmu10_energ_concep_envir_sound_reliab_affor}
The high renewable energy content in the energy mix causes electricity prices to
be volatile, which makes Germany an attractive location for the use of
VPPs.

Germany is home to the carsharing providers Car2Go\footnote{\url{https://www.car2go.com}} and DriveNow\footnote{\url{https://www.drive-now.com}},
which operate large EV fleets across the globe. It has been argued that electric
carsharing can simultaneously solve several traditional mobility and
environmental problems and are an important element of future smart cities
\cite{firnkorn15_free_float_elect_carsh_fleet_smart_cities}. Further, it is widely
regarded that the future of mobility will be electric, shared, smart and
eventually autonomous \cite{burns13_sustain_mobil,sterling18_three_revol}.
Carsharing providers are already contributing to the first two points by
operating large fleets of electric vehicles. This research addresses the third
point: Using electric carsharing fleets to smartly participate in electricity
markets. Carsharing providers, like Car2Go and DriveNow, operate their
carsharing fleets in a free-float model, which allows customers to pick up and
drop vehicles at any place within the operating zone of the provider. Customers
pay by the minute and are offered incentives to park the EVs at charging
stations at the end of their trip.

We obtained real-world trip data from Daimler's carsharing service Car2Go.
Additionally, we collected freely available balancing market data from the GCRM
platform website \url{https://regelleistung.net}. The data of the EPEX Spot market
have kindly been provided by ProCom GmbH\footnote{\url{https://procom-energy.de}} for research purposes. In the
next chapters the different datasets are described, as well the most important
processing steps outlined.

\subsection{Electronic Vehicle Fleet Data}
\label{sec:org50a310a}
The Car2Go dataset consists of GPS data of around 500 Smart ED3 Fortwo vehicles in
Stuttgart. These subcompact cars are equipped with a 17.6kWh battery and a
standard 3.3kW on-board charger. They fully charge in about six to seven hours
and can reach a maximum driving distance of 145km according to the manufacturer.
When equipped with an additional 22kW fast charger the charging time reduces to
about an hour.

In Table \ref{table-car2go-raw} the raw data is displayed, as we have obtained it
by Car2Go. The dataset contains spatio-temporal attributes, such as timestamp,
coordinates, and the address of the EVs in 5 minute intervals. Additionally,
status attributes of the interior and exterior are given (not displayed).
Especially relevant for our research is the state of charge (\(SoC\), in \%) and
information whether the EV is plugged into one of the 380 charging stations in
Stuttgart. Note that the data only contain EVs that are \emph{available for rent},
i.e., they are not currently rented out by a customer. EVs which are parked at a
charging station are also not available until they have charged up approximately
70 SoC. Individual trips have to be reconstructed using the GPS data of the
cars. The following preprocessing steps have been taken to prepare the data for
further analysis. Table \ref{table-car2go-processed} depicts the dataset after all
processing steps.

\begin{enumerate}
\item Drop unused data columns

\begin{itemize}
\item \emph{ID}: Number plate is already a unique identifier for every EV.
\item \emph{Address}: Different addresses were given from same coordinates. \emph{Latitude,
Longitude} was used for locational data instead.
\item \emph{Interior, Exterior}: Status attributes were not used in the analysis of
this research. Although they could form interesting features for rental
predictions.
\item \emph{Engine Type}: All EVs in Stuttgart are electric vehicles.
\end{itemize}
\item Decrease GPS resolution to 10 meters

The GPS accuracy of private industry sensors is approximately 5 meters under
open sky, and worse near buildings, bridges and trees\footnote{See \url{https://www.gps.gov/systems/gps/performance/accuracy}, accessed
23\textsuperscript{th} February 2019.}. Rental trips are
identified by changing GPS locations of the EV (See next point). To reduce
the number of false identified trips, due to GPS measurement errors, the resolution
is decreased.
\item Determine rental trips

We infer that a customer rented an EV, if the position coordinates change
between two data points of the same EV (see Table \ref{table-car2go-raw}, 4\textsuperscript{th}
to 5\textsuperscript{th} row). Note that we assume that customer do not undertake trips,
which begin and end at the exact same location.
\item Infer charging stations

The GPS location of the EVs is matched with the GPS locations, where an EV
has been charged at least once in the dataset. We observed that the raw data
do not show EVs that are parked at charging stations, but are not plugged in.
This research assumes that all EVs, which are parked at charging stations are
also plugged in. That is a valid assumption, since in Germany cars are only
allowed to park at charging station if they are connected to it.
\item Clean data
\begin{itemize}
\item \emph{Service trips}: 999 rental trips were removed that had a trip duration
longer than the maximum allowed rental time of two days. We assume that
these trips were \emph{service trips} undertaken by Car2Go. When the EVs
returned with a higher SoC (e.g., they have been charged at the car repair
shop), the previous trip had to be altered to end at a charging station to
ensure charging consistency.
\item \emph{Incorrectly charged EVs}: 999 EVs were removed that show incorrect
charging behavior. The data of these EVs showed an increase of more than
20\% SoC between trips or on trips, while not being located at a charging
station.
\end{itemize}
\end{enumerate}

\begin{sidewaystable}[htbp]
    \caption{Sample Raw Car2Go Data in Stuttgart \label{table-car2go-raw}}
    \centering
    \begin{tabular}{c|ccccccc}
      \hline
      \hline
      Number Plate & Timestamp & Latitude & Longitude & Street & Zip Code & Charging & SoC (\%)\\
      \hline
      S-GO2471 & 24.12.2017 20:00 & 9.19121 & 48.68895 & Parkplatz Flughafen & 70692 & no & 94\\
      S-GO2471 & \ldots{} & \ldots{} & \ldots{} & \ldots{} & \ldots{} & \ldots{}. & \ldots{}\\
      S-GO2471 & 24.12.2017 20:05 & 9.19121 & 48.68895 & Parkplatz Flughafen & 70692 & no & 94\\
      S-GO2471 & 24.12.2017 20:10 & 9.19121 & 48.68895 & Parkplatz Flughafen & 70692 & no & 94\\
      S-GO2471 & 24.12.2017 23:05 & 9.15922 & 48.78848 & Salzmannweg 3 & 70192 & no & 71\\
      S-GO2471 & 24.12.2017 23:10 & 9.15922 & 48.78848 & Salzmannweg 3 & 70192 & no & 71\\
      S-GO2471 & 25.12.2017 00:40 & 9.17496 & 48.74928 & Felix-Dahn-Str. 45 & 70597 & yes & 62\\
      S-GO2471 & 25.12.2017 00:45 & 9.17496 & 48.74928 & Felix-Dahn-Str. 45 & 70597 & yes & 64\\
      S-GO2471 & \ldots{} & \ldots{} & \ldots{} & \ldots{} & \ldots{} & \ldots{}. & \ldots{}\\
      S-GO2471 & 25.12.2017 06:50 & 9.17496 & 48.74928 & Felix-Dahn-Str. 45 & 70597 & no & 100\\
      S-GO2471 & 25.12.2017 08:25 & 9.2167 & 48.78742 & Friedenaustraße 25 & 70188 & no & 42\\
      \hline
      \hline
    \end{tabular}

    \bigskip\bigskip  % provide some separation between the two tables

    \caption{Sample Processed Car2Go Trip Data in Stuttgart \label{table-car2go-processed}}
    \centering
    \begin{tabular}{cc|ccccc}
      \hline
      \hline
      Number Plate & Trip & Start Time & Start Latitude & Start Longitude & Start SoC (\%)\\
      \hline
      S-GO2471 & 1 & 24.12.2017 20:10 & 9.19121 & 48.68895 & 94\\
      S-GO2471 & 2 & 24.12.2017 23:10 & 9.15922 & 48.78848 & 71\\
      S-GO2471 & 3 & 25.12.2017 00:50 & 9.17496 & 48.74928 & 66\\
      \hline
      Number Plate & Trip & End Time & End Latitude & End Longitude & End SoC (\%)\\
      \hline
      S-GO2471 & 1 & 24.12.2017 23:05 & 9.15922 & 48.78848 & 71\\
      S-GO2471 & 2 & 25.12.2017 00:40 & 9.17496 & 48.74928 & 62\\
      S-GO2471 & 3 & 25.12.2017 03:25 & 9.2167 & 48.78742 & 42\\
      \hline
      Number Plate & Trip & Trip Duration (min) & Trip Distance (km) & Trip Charge (\%) & End Charging\\
      \hline
      S-GO2471 & 1 & 175 & 33.35 & 23 & no\\
      S-GO2471 & 2 & 90 & 13.05 & 9 & yes\\
      S-GO2471 & 3 & 155 & 29 & 20 & no\\
      \hline
      \hline
    \end{tabular}
\end{sidewaystable}

\subsection{Balancing Market Data}
\label{sec:org6121ea0}
In this research, we use market balancing data from the German secondary reserve
market. The following chapter will give an overview of the dataset and
preprocessing steps that were taken. The data encompasses weekly lists of
anonymized bids between 01.06.2016 and 01.01.2018 and a dataset of activated
control reserve in Germany during the same period. For a detailed description
about the market design of balancing markets refer to Chapter
\ref{sec-balancing-market}.

The bidding data consists of the traded electricity product, the offered
capacity \(\Pb{}\) (MW), the capacity price \(\cp{}\) (\(\emw\)), and the
energy price \(\ep{}\) (\(\emwh\)) of each bid. Four different products
are traded, which are a combination of positive control reserve (feed
electricity into the grid) or negative control reserve (take electricity from
the grid) and the provided time segment (peak or non-peak hours). Since negative
prices are allowed on the secondary operating reserve market, the payment
direction is included as well. Moreover, information about the amount of
capacity that was accepted, i.e., either partially or fully, is listed. Bids,
which were not accepted by the TSOs are not listed. An exemplary excerpt of the
dataset is displayed in Table \ref{table-operating-reserve}.

\begin{longtable}{c|ccccc}
\caption[Secondary Operating Reserve Market Data]{List of Bids of the German Secondary Reserve Market for the tender period 04.12.2017 - 11.12.2017. \label{table-operating-reserve}}
\\
\hline
\hline
Product & Capacity Price & Energy Price & Payment & Offered & Accepted\\
\hline
\endfirsthead
\multicolumn{6}{l}{Continued from previous page} \\
\hline

Product & Capacity Price & Energy Price & Payment & Offered & Accepted \\

\hline
\endhead
\hline\multicolumn{6}{r}{Continued on next page} \\
\endfoot
\endlastfoot
\hline
NEG-HT & 0 & 1.1 & TSO to bidder & 5 & 5\\
NEG-HT & 10.73 & 251 & TSO to bidder & 15 & 15\\
NEG-HT & 200.3 & 564 & TSO to bidder & 22 & 22\\
\ldots{} & \ldots{} & \ldots{} & \ldots{} & \ldots{} & \ldots{}\\
NEG-NT & 0 & 21.9 & Bidder to TSO & 5 & 5\\
NEG-NT & 0 & 22.4 & Bidder to TSO & 5 & 5\\
\ldots{} & \ldots{} & \ldots{} & \ldots{} & \ldots{} & \ldots{}\\
POS-NT & 696.6 & 1200 & TSO to bidder & 5 & 5\\
POS-NT & 717.12 & 1210 & TSO to bidder & 10 & 7\\
\hline
\hline
\end{longtable}

In this study, we assume that bidding on 15-minute intervals in secondary
operating reserve auctions will be possible in future energy markets. As
mentioned in Chapter \ref{sec-balancing-market}, the market design of the GCRM
secondary operating reserve tender was adjusted in 2017. Daily tenders with
4-hour bidding intervals were introduced in favor of weekly tenders with only
two time segments. This change represents the trend by the TSOs to change the
market design in order to better include RES into the operating reserve markets
\cite{agricola14_dena_ancil_servic_study}. Due to the volatility of renewable
electricity generation, providers are naturally dependent on accurate short-term
forecasts, which are only possible with short tender periods and fine-grained
bidding intervals.

In order to estimate the upper bound of profits that the EV fleet can earn by
participating in the secondary operating reserve market, the \emph{critical prices}
\(\ccp{}\) and \(\cep{}\) were determined for each auctioned interval. Following
\textcite{brandt17_evaluat_busin_model_vehic_grid_integ}, we define \(\ccp{}\)
(\(\emw\)) as the capacity price of the bid that was just barely
accepted, whereas \(\cep{}\) (\(\emwh\)) is the highest energy price
that was payed for activated control reserve during that interval. For every
15-minute interval within the given tender period of one week, the activated
control reserve in that interval was matched with the accepted bids in that
tender period. At the point where supply, i.e., offered capacity of bids, met
demand, i.e., activated control reserve, the critical price \(\cep{}\) was
determined.

\emph{Example}: The assumed critical prices for the secondary operating reserve
tender interval of the 6\textsuperscript{th} December 2017 between 08:00 and 08:15 are obtained
as follows: Three suppliers submitted a reserve capacity of 5MW, 15MW and 22MW
respectively (see Table \ref{table-operating-reserve}). The critical capacity
price \(\ccp{}\!=\!200.3\emw\) is determined by the capacity price of the last
(third) accepted bid in that time segment. The TSO reported that 18MW of control
reserve were activated between 08:00 and 08:15. Hence, the second bid determines
the critical energy price \(\cep{}\!=\!251\emwh\), as control reserve capacity
gets activated according to ascending order of the submitted energy prices. In
this example the second bidder would get compensated with:
\(R\!=\!R^c + R^e\!=\!(10.73\emw\!\times\!15\mw) +
(251\emwh\!\times13\mw\!\times\!0.25\text{h})\!=\! 976.7\eur\). Note that
the second bidder get compensated for providing 13MW for 15 minutes (0.25h),
instead of the submitted 15MW, since in total only 18MW of control capacity was
activated, which was partly fulfilled by the 5MW of the first bidder.

\subsection{Spot Market Data}
\label{sec:org942f23e}
The data from the EPEX Spot Intraday Continuous encompass order books and
executed trades from 01.06.2016 until 01.01.2018. The list of trades contain
information on the unit price \(\up{}\) (\(\emwh\)), the quantity (kW)
and the traded product (hourly, quarterly or block). In this research, we focus
on quarterly product times (15-minute intervals), as they provide the highest
flexibility. Fleet controllers can promptly react to fluctuant electricity
demand of the EV fleet by accurately adjusting the bid quantities. Future
research could also consider other electricity products if lower prices justify
decreased flexibility at that point in time. Additionally, the TSOs of the buyer
and seller are listed in the dataset. They are only relevant if special
conditions between TSO apply, e.g., when delivering electricity to other
countries.

On the spot market, electricity trades can have a very short lead time of up to
5 minutes before delivery (see Table \ref{table-spot-market}). This market
characteristic is beneficial for our proposed trading strategy, since it allows
the EV to procure electricity in almost real time. The controller can submit
bids to the market, with accurate estimations of available charging capacity up
to five minute-ahead. Similarly to the balancing market, the critical price
\(\cup{}\) has to be determined for all intraday trading intervals.
\(\cup{}\) is defined as the lowest unit price of all executed trades
\(\cal{T}\) in a bidding interval:
\begin{equation*}
    \cup{} \defeq \min_{t \in \cal{T}} \up{t}
\end{equation*}
Note that \(\cup{}\) is essentially the optimal bidding price, it provides
on upper bound for the fleets profits when bidding on the intraday market. We
assume that bids with  \(p_u \geq \cup{}\) will always get matched. In
reality this is not always the case, since trades are executed immediately and
it is not guaranteed that unmatched asks at \(\cup{}\) exist at the time
of commitment. For a detailed description of the intraday continuous market see
Chapter \ref{sec-spot-market}.

{\captionsetup[table]{aboveskip=0.5cm}
\begin{sidewaystable}[hp]
\caption{List of Trades of the EPEX Spot Intraday Continuous Market \label{table-spot-market}}
\centering
\begin{tabular}{c|cccccccc}
\hline
\hline
Execution time & ID & Unit Price & Quantity & Buyer Area & Seller Area & Product & Product Time & Delivery Date\\
\hline
2017-12-04 06:54:55 & 8031392 & 51.00 & 5500 & Amprion & Amprion & Q & 07:15 - 07:30 & 2017-12-04\\
2017-12-04 06:53:26 & 8031391 & 59.00 & 10000 & TenneT & TenneT & Q & 07:15 - 07:30 & 2017-12-04\\
2017-12-04 06:53:26 & 8031390 & 58.90 & 10000 & TenneT & TenneT & Q & 07:15 - 07:30 & 2017-12-04\\
2017-12-04 06:53:15 & 8031389 & 52.30 & 7000 & 50Hertz & 50Hertz & Q & 07:15 - 07:30 & 2017-12-04\\
2017-12-04 06:53:13 & 8031386 & 59.00 & 500 & TenneT & TenneT & Q & 07:15 - 07:30 & 2017-12-04\\
2017-12-04 06:53:13 & 8031387 & 51.00 & 3600 & Amprion & Amprion & Q & 07:15 - 07:30 & 2017-12-04\\
2017-12-04 06:53:13 & 8031388 & 52.00 & 1400 & Amprion & Amprion & Q & 07:15 - 07:30 & 2017-12-04\\
2017-12-04 06:53:02 & 8031385 & 58.90 & 11000 & TenneT & TenneT & Q & 07:15 - 07:30 & 2017-12-04\\
2017-12-04 06:52:38 & 8031380 & 60.00 & 10000 & Amprion & Amprion & Q & 07:15 - 07:30 & 2017-12-04\\
2017-12-04 06:52:38 & 8031381 & 57.50 & 8000 & Amprion & Amprion & Q & 07:15 - 07:30 & 2017-12-04\\
2017-12-04 06:52:38 & 8031382 & 58.00 & 2000 & Amprion & Amprion & Q & 07:15 - 07:30 & 2017-12-04\\
2017-12-04 06:52:38 & 8031383 & 58.90 & 4000 & TenneT & TenneT & Q & 07:15 - 07:30 & 2017-12-04\\
2017-12-04 06:52:38 & 8031384 & 60.00 & 4000 & Amprion & Amprion & Q & 07:15 - 07:30 & 2017-12-04\\
2017-12-04 06:52:27 & 8031379 & 52.30 & 8000 & 50Hertz & 50Hertz & Q & 07:15 - 07:30 & 2017-12-04\\
2017-12-04 06:51:33 & 8031378 & 66.00 & 5000 & TransnetBW & TransnetBW & Q & 07:15 - 07:30 & 2017-12-04\\
2017-12-04 06:51:28 & 8031377 & 54.00 & 8000 & Amprion & Amprion & Q & 07:15 - 07:30 & 2017-12-04\\
2017-12-04 06:51:24 & 8031376 & 54.00 & 7000 & TenneT & TenneT & Q & 07:15 - 07:30 & 2017-12-04\\
2017-12-04 06:49:34 & 8031375 & 51.00 & 4000 & TenneT & TenneT & Q & 07:15 - 07:30 & 2017-12-04\\
2017-12-04 06:49:26 & 8031374 & 54.00 & 5000 & 50Hertz & 50Hertz & Q & 07:15 - 07:30 & 2017-12-04\\
2017-12-04 06:49:23 & 8031373 & 55.10 & 8000 & 50Hertz & 50Hertz & Q & 07:15 - 07:30 & 2017-12-04\\
\hline
\hline
\end{tabular}
\end{sidewaystable}
}

\clearpage

\section{Model: FleetRL}
\label{sec:orgfd42345}
The following chapter will introduce the model of this research. In its essence,
we propose a solution for EV fleet providers to utilize a VPP portfolio to
profitably provide balancing services to the grid on multiple markets. A control
mechanism procures energy from electricity markets, allocates available EVs to
VPPs, and intelligently dispatches EVs to charge the acquired amount of energy.
The model uses a RL agent that learns an optimal bidding strategy by interacting
with the electricity markets and reacts to changing rental demand of the EV
fleet. This chapter is structured as follows: The information assumptions are
listed first, the control mechanism is explained next, and finally the RL
approach is described in detail. A table of the used notation in this
chapter can be found in Table \ref{table-notation}.

We formulate the problem as a \emph{controlled EV charging} problem. The EV fleet
operator represents the \emph{controller}, which aims to charge the fleet at minimal
costs. First, the controller predicts the amount of energy it can charge in a
given \emph{market period} \(h\). The length of the market period \(\Delta h\) and the
market closing time depend on the considered electricity market. Second, the
controller places bids on one or multiple markets to procure the predicted
amount of energy. Lastly, at electricity delivery time, the controller
communicates with the EV fleet to control the charging in real-time. Online EV
\emph{control periods} \(t\) are typically shorter than market periods. In the
empirical case that we consider, the market periods are 15 minutes long, while
the EV control periods last 5 minutes. Nonetheless, the presented approach
generalizes to other period lengths. During each control period, the controller
has to take decisions which individual EVs it should dispatch to charge the
procured amount of electricity. In times of unforeseen rental demand, this
decision implies trading off commitments to the markets with compromising
customer mobility by refusing customer rentals.


\begin{longtable}{p{0.11\linewidth}|p{0.75\linewidth}|c}
\caption[Table of Notation]{Table of Notation \label{table-notation}}
\\
\hline
\hline
Symbol & Description & Unit\\
\hline
\endfirsthead
\multicolumn{3}{l}{Continued from previous page} \\
\hline

Symbol & Description & Unit \\

\hline
\endhead
\hline\multicolumn{3}{r}{Continued on next page} \\
\endfoot
\endlastfoot
\hline
\(t\) & Control period. & -\\
\(h\) & Market period. & -\\
\(T\) & Number of control periods in a market period. & -\\
\(H\) & Number of market periods in day. & -\\
\(N_h\) & Total number of market periods. & -\\
\(\Delta t\) & Length of control period. & hours\\
\(\Delta h\) & Length of market period. & hours\\
\hline
\(\Pb{h}\) & Amount of balancing power offered on the balancing market. & kW\\
\(\cp{h}\) & Capacity price offered on the balancing market. & \(\emw\)\\
\(\ep{h}\) & Energy price offered on the balancing market. & \(\emwh\)\\
\(\ccp{h}\) & Critical capacity price in market period \(h\). & \(\emw\)\\
\(\cep{h}\) & Critical energy price in market period \(h\). & \(\emwh\)\\
\hline
\(\Pi{h}\) & Amount of power offered for the unit on the intraday market. & kW\\
\(\up{h}\) & Unit price offered on the intraday market. & \(\emwh\)\\
\(\cup{h}\) & Critical unit price in market period \(h\). & \(\emwh\)\\
\hline
\(\Eb{h}\) & Amount of energy charged from balancing market in market period h. & MWh\\
\(\Ei{h}\) & Amount of energy charged from the intraday market in market period h. & MWh\\
\(p^{i}\) & Industry tariff & \(\ekwh\)\\
\hline
\(\fP{t}\) & Amount of available fleet charging power in control period \(t\). & kW\\
\(\fPhat{t}\) & Predicted amount of available fleet charging power in control period \(t\). & kW\\
\hline
\(\Cb{h}(P)\) & Cost function for procuring electricity from the balancing market. & \(\eur\)\\
\(\Ci{h}(P)\) & Cost function for procuring electricity from the intraday market. & \(\eur\)\\
\(\oc\) & Opportunity costs of lost rental of EV \(i\) in control period \(t\). & \(\eur\)\\
\(\beta_h\) & Imbalance costs in market period \(h\). & \(\eur\)\\
\hline
\(\lb{h}\) & Balancing market risk factor. & -\\
\(\li{h}\) & Intraday market risk factor. & -\\
\(\theta_{\lambda}\) & Set of risk factors for all market periods \(h\!\in\!\{1,...,N_h\}\). & -\\
\(\Cf{}(\theta_{\lambda})\) & Cost function for the fleets total costs over all market periods \(h\). & \(\eur\)\\
\(\Cf{h}\) & Total accumulated fleet costs until market period \(h\). & \(\eur\)\\
\hline
\(i\) & Electric Vehicle. & -\\
\(\c\) & Dummy variable if EV is connected to a charging station. & 0/1\\
\(\omega_{i}\) & Amount of electricity stored in EV. & \(\kwh\)\\
\(\Omega\) & Maximum battery capacity of EV. & \(\kwh\)\\
\(\delta\) & Charging power of EV at the charging station. & \(\kw\)\\
\(\F\) & Set of all EVs in the fleet & -\\
\hline
\end{longtable}

\subsection{Required Information Assumptions \label{sec-model-assumptions}}
\label{sec:orgeece0fa}
The following information is assumed to be available:

\begin{enumerate}
\item The controller is able to forecast the mobility demand of the EV fleet at
different time-horizons based on historical data. More specifically, it can
predict the amount of plugged-in EVs and consequently the available charging
power \(P^{fleet}_t\) of the fleet at control period \(t\). The prediction's
accuracy is increasing with shorter time horizons, from uncertain predictions
one week ahead to very accurate predictions 30 minutes ahead. Past research
presented successful mobility demand forecast algorithms in the context of
free-float carsharing
\cite{kahlen18_elect_vehic_virtual_power_plant_dilem,kahlen17_fleet,wagner16_in_free_float}.
\item The controller is able to forecast electricity prices of spot and balancing
markets based on historical data. More specifically, it can estimate the
critical prices \(\ccp{h}\), \(\cep{h}\), and \(\cup{h}\) for each market period
with perfect accuracy. The critical prices form an essential piece of
information for the proposed bidding strategy; bids equal or below the
critical price will get accepted and result in successful electricity
procurement. Electricity price forecasting is an extensively studied
research area, with well-advanced prediction algorithms
\cite{weron14_elect_price_forec,avci18_manag_elect_price_model_risk}.
\end{enumerate}
We are confident that taking the above assumptions is viable, assuming available
forecasting information is common practice in the VPP and EV fleet charging
literature, see e.g.:
\textcite{brandt17_evaluat_busin_model_vehic_grid_integ,vandael15_reinf_learn_heuris_ev_fleet,mashhour11_biddin_strat_virtual_power_plant_1,tomic07_using_fleet_elect_drive_vehic_grid_suppor,pandzic13_offer_model_virtual_power_plant}.

\subsection{Control Mechanism}
\label{sec:orgd9db871}
The central control mechanism constitutes the core of this research. It can be
seen as a decision support system that can be deployed at a EV fleet operator to
control the charging its fleet. Figure \ref{fig-control-mechanism} depicts the
control mechanism, which is divided into three distinct phases:

The first phase, \emph{Bidding Phase I}, takes place just before the closing time of
the balancing market, once every week (e.g., Wednesdays at 3pm at the GCRM). In
this phase, the controller can place bids for every market period \(h\) of the
following week on the balancing market. The second phase, \emph{Bidding Phase II},
takes places in every market period of \(\Delta{h}\!=\!15\) minutes. At this
point, the controller has the opportunity to place bids for the market period 30
minutes ahead. By submitting bids 30 minutes ahead of time, the controller
assures that the bid will be matched until the lead time of the market (e.g, 5
minutes on the EPEX Spot Intraday Continuous). The third phase, \emph{Dispatch
Phase}, takes places in every control period of \(\Delta{t}\!=\!5\) minutes. In
this phase the controller has to dispatch available EVs to charge the procured
electricity from the markets. This phase involves allocating individual EVs to
the VPP and eventually refusing customer rentals to assure that all commitments
can be fulfilled.

The following chapters will highlight the important parts of the various phases
and provide detailed explanation and mathematical formulations.

\begin{figure}[p]
\centering
\includegraphics[width=1.05\linewidth]{./fig/control_mechanism.png}
\caption[FleetRL Control Mechanism]{FleetRL Control Mechanism \label{fig-control-mechanism}}
\end{figure}

\subsubsection{Fleet Charging Power Prediction}
\label{sec:org5dbd178}
In a first step, the controller has to predict the available fleet charging
power \(\fPhat{h}\) for all market periods \(h\) of the next week. The actual
available fleet charging power \(\fP{t}\) in a control period \(t\) is given by the
number of EVs that are connected to a charging station, with enough free battery
capacity to charge the next control period \(t\!+\!1\). As mentioned in the
Chapter \ref{sec-model-assumptions}, the controller is able to predict the
available fleet charging power \(\fPhat{t}\) for all control periods \(t\) with
different levels of accuracy dependent on the time horizon of \(t\).

When the controller procures electricity from the markets, the fleet has to
charge with the committed charging power during all \(T\) control periods of the
market period \(h\).
To minimize the risk of not being able to charge the committed amount of energy
during the whole market period, and consequently causing imbalance costs, the
predicted fleet charging power in a market period is defined as the minimal
predicted fleet charging power of all \(T\) control periods in a market period.
\begin{equation}
    \fPhat{h} \defeq \min_{n \in \{1, .., T\}} \fPhat{t + n} \text{ ,}
\end{equation}
where \(h\) is the market period of interest and \(t\) its first control period.

\subsubsection{Market Decision}
\label{sec:orgabb6f90}
In a second step, the controller has to decide from which market it should
procure the desired amount of energy. Therefore, it compares the costs for
charging electricity from the balancing market and the intraday market. The cost
function for charging electricity from the balancing market is defined as follows:
\begin{equation} \label{eq-cost-balancing}
\begin{split}
    \Cb{h}(P) &\defeq -(P\!\times\!10^{-3} \times \ccp{h}) + (\Eb{h} \times \cep{h}) \\
    &= -(P\!\times\!10^{-3} \times \ccp{h}) + (P \frac{\Delta h}{10^{3}} \times \cep{h}) \text{ ,}
\end{split}
\end{equation}
where \(P\) (kW) is the amount of offered balancing power. The first term of the
equation corresponds to the compensation the controller retrieves for keeping
the balancing capacity available, while the second term corresponds to the costs
for charging the activated balancing energy \(\Eb{h}\) (MWh). Energy is power over
time, hence \(\Eb{h}\) can be substituted with \(P\) times the market periods length
\(\Delta{h}\), divided by the unit conversion from kW to MW. As mentioned in the
Chapter \ref{sec-model-assumptions}, the critical prices \(\ccp{}, \cep{}, \cup{}\)
are assumed to be available for all market periods. Note that the critical
energy price \(\cep{}\!\in\!\Re\), can also take negative values, resulting in
profits for the fleet, while the critical capacity price \(\ccp{}\!\in\! \Re^+_0\)
can not take negative values and therefore always results in profits for the
fleet.

The cost function for charging from the intraday market is defined similarly:
\begin{equation}
\begin{split}
    \Ci{h}(P) &\defeq \Ei{h} \times \cup{h} \\
    &= P \frac{\Delta h}{10^{3}}\times \cup{h}
\end{split}
\end{equation}
Again, depending on the market situation, \(\cup{}\!\in\!\Re\) can be either
negative or positive, resulting in costs or profits for the fleet. Contrarily to
the balancing market, on the intraday market the fleet does not get compensated
for keeping the charging power available; only the charged energy affects the
costs. If the costs for charging from the balancing market 7 days ahead
\(\Cb{h+(7\!\times\!H)}(\fPhat{h+(7\!\times\!H)})\) are higher than the costs of
charging from the intraday market at the same market period \(\Ci{h +
(7\!\times\!H)}(\fPhat{h+(7\!\times\!H)})\), the controller does not place bids
on the balancing market.

\subsubsection{Determining the Bidding Quantity}
\label{sec:orgbccbd5f}
In a third step, the controller has to take a decision on the amount of energy
it should procure from the markets. Determining the bidding quantity is a
central piece of the controlled charging problem. The bidding quantity
determines the profits that can be made, by charging at a cheaper market price
than the flat industry tariff. In order to maximize its profits, the controller
aims to procure as much electricity as possible from the markets. In order to
optimally place bids on the electricity markets, it needs to balance the risk of
(a) procuring more energy that it can maximally charge and (b) not procuring
enough energy from the market to sufficiently charge the fleet.

In the first case (a), the fleet is facing costs of compromising customer
mobility, or worse, high imbalance penalties from the markets. Renting out EVs
is considerably more profitable than using EVs as a VPP to participate on the
electricity markets. Refusing customer rentals, in order to fulfill market
commitments, induces opportunity costs of lost rentals \(\rho\) on the fleet.
Imbalance costs \(\beta\) occur, when the fleet can not charge the committed
amount energy at all, even with refusing rentals. In the second case (b), the
fleet also faces opportunity costs of lost rentals when individual EVs do now
have enough SoC for planned trips of arriving customers.

The controller faces additional risks by bidding one week ahead on the balancing
market, in contrast to only 30 minutes ahead on the intraday market, as the
predictions of available charging power are more uncertain with the larger time
horizon. To account for all the mentioned risks, we introduce a \emph{risk factor}
\(\lambda \in \Re_{0 \leq \lambda \leq 1}\), where \(\lambda = 0\) indicates no
risk, and \(\lambda = 1\) indicates a high risk. The controller determines the
bidding quantity \(\Pb{h}\) by discounting the predicted available fleet charging
power \(\fPhat{h}\) with the possible risk \(\lambda_{h}\) of imbalance or
opportunity costs:
\begin{equation} \label{eq-model-pb}
  \Pb{h} \defeq
  \begin{cases}
    0, & \text{if}\ \Cb{h}(\fPhat{h}) \geq \Eb{h}10^3 \times p^{i}\\
    0, & \text{if}\ \Cb{h}(\fPhat{h}) \geq \Ci{h}(\fPhat{h})\\
    \fPhat{h} \times (1\!-\!\lb{h}), & \text{otherwise}
  \end{cases}
\end{equation}
where \(h\) is the market period of interest one week ahead. If the controller can
buy electricity at the intraday market at a lower price, it does not place a bid
at the balancing market. If the controller can charge cheaper at the regular
industry tariff \(p^{i}\), it does not place a bid either. In all other cases, the
controller submits \(\Pb{h}\) to the market.

The bidding quantity for the intraday market \(\Pi{h}\) depends on the previously
committed charging power \(\Pb{h}\) and the newly predicted charging power
\(\fPhat{h}\):
\begin{equation} \label{eq-model-pi}
  \Pi{h} \defeq
  \begin{cases}
    0, & \text{if}\ \Ci{h}(\fPhat{h}\!-\!\Pb{h}) \geq \Ei{h}10^3 \times p^{i}\\
    (\fPhat{h}\!-\!\Pb{h}) \times (1\!-\!\li{h}), & \text{otherwise}
  \end{cases}
\end{equation}
where \(h\) is the market period of interest 30 minutes ahead. Note that any
amount of electricity that the controller procured from the balancing market,
does not need to be bought from intraday market for the same market period.
Since the predicted charging power \(\fPhat{h}\) is expected to be more
accurate 30 minutes ahead than one week ahead, the controller is able to correct
bidding errors it made in the first decision phase, and optimally charge the
whole EV fleet.

\subsubsection{Dispatching Electronic Vehicle Charging}
\label{sec:orgb5b342a}
In the last step, at electricity delivery time, the EVs have to be assigned to
the VPP and be \emph{dispatched} to charge. Therefore the controller first needs to
detect how many EVs are eligible to be used as VPP per control period \(t\). EVs
are eligible if they (a) are connected to a charging station (\(\c\) = 1), and (b)
have enough free battery storage available (\(\Omega\!-\!\omega_{i}\)) to charge
the next control period. Hence, the VPP is defined as:
\begin{equation}
    V\!P\!P \defeq \{i\in\F \;|\; \c = 1 \vee \Omega\!-\!\omega_{i}\!\geq\!\gamma\Delta{t}\} \text{ ,}
\end{equation}
where \(\gamma\Delta{t}\) (kWh) denotes the amount of energy that can be charged
with the charging speed of \(\gamma\) (kW) in control period \(t\). \(\gamma\) is
limited by either the EVs build-in charger, or the charging power of the
connected charging station. In this model we assume \(\gamma\) is equal for all
considered EVs and charging stations.

Remember that the fleet has to provide the committed charging power
\(\Pb{h}\!+\!\Pi{h}\) across all control periods \(t\) of the market period \(h\),
independent of which individual EVs are actually charging the electricity. This
fact allows the controller to dynamically dispatch EVs every control period and
react to unforeseen rental demand. If a customer want to rent out an EV that is
assigned to the VPP, the controller only has to refuse the rental, if no other
EV is available to charge instead. When no replacement EV is available, the
controller has to account for lost rental profits \(\oc\). If the VPPs total
amount of available charging power \(\vpp{t}\!\times\!\gamma\) is not sufficient to
provide the total market commitments \(\Pb{h}\!+\!\Pi{h}\), the fleet gets charged
imbalance costs \(\beta_{h}\). Otherwise all the committed energy can be charged
by the VPP.

\subsubsection{Evaluating the Bidding Risk}
\label{sec:org84ea1c7}
The controllers central goal is to choose the risk factors \(\lb{h}\), \(\li{h}\)
for every market period \(h\), that minimize the cost of charging, while avoiding
the risks of lost rental profits \(\oc\) or imbalance costs \(\beta_h\). The total
fleet costs are defined as follows:
\begin{equation} \label{eq-model-fleetcosts}
    \Cf{}(\theta_{\lambda}) \defeq \sum^{N_h}_h
    \bigg[ \Cb{h}(\Pb{h}) + \Ci{h}(\Pi{h}) + \beta_{h}
    + \sum_t^{T} \sum_i^{|\F|} \oc \bigg] \text{ ,}
\end{equation}
where \(\theta_{\lambda}\!\in\!\Re_{0 \leq \lambda \leq 1}^{2 \times N_h}\) is the
matrix of the risk factors \(\lb{h}\), \(\li{h}\) for all considered market periods
\(N_h\). \(\F\) denotes the set of all EVs \(i\) in the fleet and \(|\F|\) the fleet
size. The costs for charging \(\Cb{h}(\Pb{h})\), \(\Ci{h}(\Pi{h})\) are clearly
dependent on the chosen risk factors \(\lb{h}\), \(\li{h}\) (see Eq.
\ref{eq-model-pb}, Eq. \ref{eq-model-pi}). In summary, the problem can be formulated
as minimizing the total costs of the fleet, by choosing the optimal set of risk
factors \(\theta_{\lambda}\):
\begin{equation}
\begin{aligned}
    & \underset{\theta_{\lambda}}{\text{minimize}}
    && \Cf{}(\theta_{\lambda}) \\
    & \text{subject to}
    && 0 \leq \lb{h} \leq 1, \; \forall \lb{h} \in \theta_{\lambda}\\
    &&& 0 \leq \li{h} \leq 1, \; \forall \li{h} \in \theta_{\lambda}\\
\end{aligned}
\end{equation}
Solving this optimization problem with common methods like stochastic
programming is a difficult task, assuming that complete information of available
charging power and future electricity market prices is not always available.
Since one goal of this research is to develop a model that can be applied to
previously unknown settings and learn from uncertain environments, as mobility
and electricity markets, we chose to solve the problem with a RL learning
approach that is explained in detail in Chapter \ref{sec-model-rl}.

\subsubsection{Example}
\label{sec:org3e2665a}
At 3pm on the 9\textsuperscript{th} of August 2017, the controller enters the first bidding
phase for procuring electricity one week ahead, the market period \(h\) =
\emph{16.08.2017 15:00-15:15}. It predicts that at that point in time 250 EVs are connected
to a charging station, resulting in 900kW available fleet charging power
(\(\fPhat{h}\!=\!900\kw\)), given the charging power of 3.6kW per EV. Assuming the
available critical prices \(\ccp{h}\!=\!5\emw\), \(\cep{h}\!=\!-10\emwh\), and
\(\cup{h}\!=\!10\emwh\) for that market period, the controller now evaluates the
cheapest charging option. The flat industry electricity tariff is assumed to be
\(p_i\!=\!0.15\ekwh\). The costs for charging with the maximal amount of power
\(\fPhat{h}\) from the balancing market (\(\Cb{h}(900\kw)\!=\!-6.25\eur\)) are less
than charging from the intraday market (\(\Ci{h}(900\kw)\!=\!2.25\eur\)) or
charging at the industry tariff
(\(900\kw\!\times\!0.25\text{h}\!\times\!0.15\ekwh\!=\!33.75\eur\)). In this
example, by choosing the cheapest option, the balancing market, the fleet
operator will even get compensated for charging its fleet.

In the next step, the controller has to submit bids to the balancing market. The
RL agent determined that the risk of bidding on the balancing market is
\(\lb{h}\!=\!0.3\). Consequently, the controller sets the bidding quantity to
\(\Pb{h}\!=\!\fPhat{h}\!\times\!(1\!-\!\lb{h})\!=\!900\kw\!\times0.7\!=\!630\kw\) and
submits a bid to the market. Since we are assuming that bids at the critical
price, will always get accepted, the controller procures 630kW from the
balancing market and updates its account with \(\Cb{h}(630\kw)\!=\!-4.725\eur\).

One week later, at 2:30pm on the 16\textsuperscript{th} of August 2017, the controller enters
the second bidding phase. With a time horizon of 30 minutes, it predicts less
available fleet charging power of \(\fPhat{h}\!=\!810\kw\) for the same market
period \emph{16.08.2017-15:00}. By trading at the intraday market, the controller can
now charge the remaining available EVs with a low risk of procuring more energy
than it can maximally charge. At this point in time, the RL agent determines a
remaining risk of \(\li{h}\!=\!0.05\), and sets the bidding quantity to
\(\Pi{h}\!=\!(810\kw\!-\!630\kw)\!\times\!(1\!-\!0.05)\!=\!171\kw\). Hence, the
controller procures 171kW from the intraday market and updates its account with
\(\Ci{h}(171\kw)\!=\!0.4275\eur\).

At electricity delivery time, the 16\textsuperscript{th} of August 2017 at 3:00pm, the
controller detects 255 available EVs; EVs which are connected to a charging
station and have enough battery capacity left to be charged in the next control
period. It assigns 223 EVs to provide the committed 801kW charging power for the
market period time \(\Delta h\) of 15 minutes. During that time, three customers
want to rent out EVs that are allocated to the VPP. The first two rentals are
accepted, because two other EVs are available to charge instead. The third
rental has be to refused, since no EV is remaining as substitution. The
controller has to account for the opportunity costs of the lost rental
\(\oc\).
\subsection{Reinforcement Learning Approach \label{sec-model-rl}}
\label{sec:org2b924ea}
In the following chapter the developed RL approach is outlined. First, we define
the charging problem as a MDP, and second, the used learning algorithm is
explained.

Remember that the goal of the controlled charging problem is to choose a set of
risk factors \(\theta_{\lambda}\) that minimize the fleets total costs across all
market periods. The controller is able to influence the charging costs, by
setting the risk factors \(\li{h}\), \(\lb{h}\), which determine the bidding
quantities \(\Pb{h}\), \(\Pi{h}\) that the controller submits to the balancing and
intraday market. The RL agent decides on the risk factors (i.e., takes an
action) based on the observed state \(\S\) every timestep. Bids are submitted
every market period \(h\), thus the \emph{timestep} (also called iteration) of the RL
problem is set to \(h\), with a length of \(\Delta{h}\). The optimal set of risk
factors are learned by the RL agent through estimating a policy \(\pi(a|s)\) that
maps every state \(s\in\S\) to an action \(a\in\A\).
\subsubsection{Markov Decision Process Definition}
\label{sec:org0608427}

MDPs are defined by the state space \(\S\), the action space \(\A\), a set of reward
signals \(\R\) and the state-transition probabilities \(p(s'|a,s)\). When
\(p(s'|a,s)\) is unknown, as it is in our case, it is possible to use a
\emph{model-free} approach (see Chapter \ref{sec-td-learning}). The state space
compromises the observed information the agent uses to decide on the action it
is going to take. We observed the following factors that are associated with the
bidding risk:
\begin{enumerate}
\item The bidding period's time of the day

In times of volatile customer rental demand (e.g., rush hour), the
uncertainty on the guaranteed amount of available EVs increases. Bidding for
these periods involves more risk of not being able to fulfill market
commitments.
\item The current and estimated future size of the VPP

Large VPPs benefit from the \emph{risk-pooling} effect \cite{kahlen17_fleet}.
Intuitively that means, larger VPPs are exposed to smaller risks: They have
an increased probability that "lost" charging power, due to unforeseen
rentals, can be substituted by the other EVs of the VPP.
\end{enumerate}
Since forecasts of available charging power are already available, we define the
predicted VPP size \(\vpphat{h}\) as the as the necessary amount of EVs to
provide the predicted charging power \(\fPhat{}\) in time period \(h\):
\begin{equation}
    \vpphat{h} \defeq \left\lceil\frac{\fPhat{h}}{\gamma}\right\rceil \text{ ,}
\end{equation}
where \(\gamma\) is the charging power per EV. The state space is then defined as
the set of all valid values of the elements of the following tuple:
\begin{equation}
    \S \defeq \left\langle t(h), \vpp{h}, \vpphat{h+2}, \vpphat{h+(7\!\times\!H)}\right\rangle \text{ ,}
\end{equation}
where:
\begin{itemize}
\item \(t(h)\) is the current daytime in hours, with discrete values in the range
\(\big[0,\;23\big] \in \Ne\).
\item \(|VPP|_t\) is the current VPP size, with discrete values in the range
\(\big[0,\;|\F|\big] \in \Ne\).
\item \(\vpphat{h+2}\) is the predicted VPP size 30 minutes ahead, with discrete values in the range
\(\big[0,\;|\F|\big] \in \Ne\).
\item \(\vpphat{h+(7\!\times\!H)}\) is the predicted VPP size 7 days ahead, with discrete
values in the range \(\big[0,\;|\F|\big] \in \Ne\).
\end{itemize}
The state space encompasses \(|\F|^3\!\times\!24\) states. Assuming a fleet size
\(|\F|\) of 500 EVs that are \(3\!\times\!10^9\) states.
The agent's action is choosing the risk that is associated with bidding on the
electricity markets each market period \(h\). Hence, the action space is
constituted by all valid values of the risk factors \(\lb{h},\li{h}\):
\begin{equation}
    \A \defeq \left\{\lb{h},\li{h} \in \Re_{0 \leq \lambda \leq 1} \right\} \text{ ,}
\end{equation}
where:
\begin{itemize}
\item \(\lb{h}\) is the risk factor for bidding on the balancing market 7 days ahead,
with discrete values in the range \(\big[0,1\big]\) in 0.05 increments.
\item \(\li{h}\) is the risk factor for bidding on the intraday market 30 minutes
ahead, with discrete values in the range \(\big[0,1\big]\) in 0.05 increments.
\end{itemize}
The action space encompasses \(20^2 = 400\) actions. The state space and action
space were consciously discretized to facilitate faster convergence rates of the
learning process. Convergence with continuous spaces is theoretically feasible
with current function approximation methods, but is computational more complex
\cite{sutton18_reinf}.

The reward signal is defined as the total fleet costs that occurred in the last
timestep:
\begin{equation}
    R_{h+1} = \Cf{h} - \Cf{h-1} \text{ ,}
\end{equation}
where \(\Cf{h}\) are the total accumulated fleet costs until the market period
\(t\). The agent's actions determine the bidding quantities, which lead to profits
from charging cheaper than the industry tariff, and possibly cause imbalance or
lost rental costs. See \eqref{eq-model-fleetcosts} for a formulation the cost
function. The occurred costs associated with the action are presented to the
agent in the form of a positive or a negative reward signal. The particular
challenge in the proposed RL problem is the significantly delayed reward.
Choosing an action \(a\) in timestep \(h\) will have an effect on the reward 672
timesteps (\(7 \text{ days}\!\times\!24\text{ hours}\!\times\!4\text{ market
periods of 15 minutes}\)) later, due to the week ahead commitments on the
balancing market.

\subsubsection{Learning Algorithm}
\label{sec:orgbe85ef5}
This research proposes to solve the layed out RL problem,  with the Double Deep
Q-Network (DDQN) algorithm, presented by
\textcite{hasselt16_deep_reinf_learn_doubl_q_learn}. DDQN is a state-of-the-art
model-free RL approach, that combines the popular Deep Q-Network (DQN),
originally proposed by \textcite{mnih15_human_level_contr_throug_deep_reinf_learn}
with Double Q-Learning \cite{hasselt10_doubl_q}. DDQN have shown to reduce DQN's
overoptimistic estimates of action-values, resulting in more stable and reliable
learning results. Combined with a dueling network architecture, proposed by
\textcite{wang15_duelin_networ_archit_deep_reinf_learn}, this approach outperforms
existing approaches for deep RL. Dueling networks, lead to faster convergence
rates in control problems with large action spaces than a traditional single
stream approach. This is especially beneficial for our proposed RL problem, as
the action space, with 400 possible actions, is comparably large in comparison
to classical control problems. The dueling architecture consists of two streams
to separately estimate the state-value and the action advantages, which are
later combined into Q-values (see Figure \ref{fig-model-dueling}, single (top) vs.
duel architecture). The separated streams allow to learn which states are
valuable without having to learn each state-action interaction individually. A
general state-value is learned that can be shared across many different actions,
leading to faster convergence \cite{wang15_duelin_networ_archit_deep_reinf_learn}.

\begin{figure}[h]
\centering
\includegraphics[width=0.95\linewidth]{./fig/ddqn.pdf}
\caption[Dueling DDQN Architecture]{The dueling DDQN architecture \cite{wang15_duelin_networ_archit_deep_reinf_learn} \label{fig-model-dueling}}
\end{figure}

Our agent uses the dueling DDQN algorithm with a standard neural network
architecture, similar to the one depicted in Figure \ref{fig-ann}. It consist of
four input nodes (number of states), three fully-connected hidden layers with
ReLU activation functions, and a linear output layer with two nodes (number of
actions).
\begin{itemize}
\item Epsilon or Boltzman Policy?
\end{itemize}
The RL agent was implemented with the neural networks API Keras\footnote{\url{https://www.keras.io}}\textsuperscript{,}\,\footnote{\url{https://github.com/keras-rl/keras-rl}},
which runs on top of TensorFlow, the de-facto standard for robust and scalable
ML in industry and research \cite{abadi16_tensor}. We used Google
Colaboratory\footnote{\url{https://colab.research.google.com}} to train and evaluate the
agent. This shared research environment provides free access to Nvidia Tesla K80
GPU, with 2880 \(\times\) 2 CUDA cores and 12GB GDDR5 VRAM for 12 hours
consecutive usage. Additionally it is equipped with a Intel(R) Xeon(R) CPU @
2.30GHz (1 core, 2 threads), with 45MB cache, and over 12GB available memory.

\begin{itemize}
\item Hyperparameters? - In results or here?
\item Training time/timesteps - Results or here?
\end{itemize}

\section{Simulation Platform: FleetSim}
\label{sec:org4601219}
\begin{itemize}
\item Simulation Platform to allow evaluating the performance of intelligent agents
in smart charging/balancing the grid of EV fleets. Allows to test out bidding strategies
and control mechanisms in a realistic EV fleet setting.
\item Real life comparison graph: 10\%.
\end{itemize}
\subsection{Event-based Simulation}
\label{sec:orgb56c0a2}
\begin{itemize}
\item Not only t+1
\item Event e.g. denying rental, charge/little charge/no-charge has effects for the
whole simulation
\item Simpy
\item Python
\end{itemize}
\subsection{Architecture / Components}
\label{sec:orgecf31ba}
\begin{figure}[htbp]
\centering
\includegraphics[width=1\linewidth]{./fig/simulation_platform.png}
\caption[FleetSim Architecture]{Architecture of FleetSim \label{fig-fleetsim}}
\end{figure}

\subsection{Modular Expandability}
\label{sec:org0bbc58e}
\begin{itemize}
\item Plug-in different Market designs
\item Use different real-world data
\item Change Fleet parameters
\item Develop new strategy
\item
\end{itemize}
\section{Results}
\label{sec:orge040338}
\subsection{Simulation Settings}
\label{sec:orgcd39f7d}
\begin{itemize}
\item Results heavily dependent on industry charging price, since on average the
balancing prices are 50\% cheaper, and intraday 30\% cheaper.
\item BMWi. (n.d.). Prices of electricity for the industry in Germany from 2008 to
2017 (in euro cents per kilowatt hour). In Statista - The Statistics Portal.
Retrieved March 18, 2019, from
\url{https://www.statista.com/statistics/595803/electricity-industry-price-germany/}.
\end{itemize}
\subsection{FleetRL}
\label{sec:orga294131}
\begin{itemize}
\item long-delayed rewards make RL hard (!?)
\item Compare RL Algos eg. Q-learning vs DDQN --> deep learning makes difference in
practice for complex system
\end{itemize}
\subsection{Sensitivity Analysis}
\label{sec:org34bf5e3}
\begin{itemize}
\item Prediction Accuracy
\end{itemize}

\begin{itemize}
\item Charging infrastructure
\end{itemize}
\section{Conclusion}
\label{sec:org67fd005}
\subsection{Contribution}
\label{sec:orgcc8d89b}
\begin{itemize}
\item Compare to most simular studies:
\end{itemize}
\cite{kahlen18_elect_vehic_virtual_power_plant_dilem,vandael15_reinf_learn_heuris_ev_fleet} etc..
\begin{itemize}
\item Business model for EV fleet owners with better results than previous studies
\item Environmental impact by providing balancing power
\item Decision Support System for controlled EV charging from multiple markets
\item RL Algorithm that is designed to work in previously unknown environments and
thus suited to deploy in real life settings of all kinds of EV fleets in all
kinds of cities. E.g. scooters also?
\item Event-based Simulation Platform to evaluate bidding strategies and RL agents,
facilitate research
\end{itemize}

\subsection{Limitations}
\label{sec:org7ace271}
\begin{itemize}
\item Model:
\begin{itemize}
\item Bidding Mechanism: one week ahead, always accepted
\item Policy \& Regulation: EVs not allowed to provide balancing power, minimum
bidding quantities 1MW.
\item Markets: Fleet is a price-taker, what about larger fleets? Simulate market influence
\end{itemize}
\item RL: See \cite{vazquez-canteli19_reinf_learn_deman_respon} conclusion for
limitations.
\end{itemize}
\subsection{Future Research}
\label{sec:orgbe0d8a5}
\begin{itemize}
\item Model: Current market design, i.e. daily w/ 4h slots. German "Mischpreisverfahren"
\item RL: Long-delayed rewards, different reward structure, memory based
\end{itemize}

\clearpage
\bibliography{bibliography/references}
\bibliographystyle{apacite}
\end{document}
