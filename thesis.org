#+TITLE: Reinforcement Learning Portfolio Optimization of Electric Vehicle Virtual Power Plants
#+AUTHOR:Tobias Richter

#+INCLUDE: "./header.org"
#+INCLUDE: "./chapters/introduction.org"
#+INCLUDE: "./chapters/background.org"
#+INCLUDE: "./chapters/empirical-setting.org"
#+INCLUDE: "./chapters/model.org"

* Results
# NOTE: 15%

The following chapter will cover the main results of this research. First, the
simulation environment is presented. Second, we examine the economic
sustainability of an integrated bidding strategy. Third, the RL approach that
aims to optimize the VPP portfolio is evaluated. In the last section, we will
perform sensitivity analyses on the limiting algorithmic factor, the prediction
accuracy, and a limiting physical factor, the charging infrastructure.

** Simulation Environment
# NOTE: V2G?
As part of this research, we developed an event-based simulation platform called
/FleetSim/. The platform allows researchers to develop and test out different
smart charging and bidding strategies in realistic environment based on
real-world data. In FleetSim, intelligent agents (called controllers) centrally
control the charging of an EV fleet, they are responsible to sufficiently charge
their vehicles to satisfy real mobility demand. At the same time, the agents can
create VPP of EVs, provide balancing services to the grid, and take part in
electricity trading. Trips are simulated on an individual level, for example,
not charging an individual EV at a particular point in time, can cause a whole
series of lost rentals, due to an insufficient amount of battery for the next
arriving customers. The agents are evaluated based on the profits of charging
the fleet cheaper than the industry tariff, the costs of losing rentals and the
imbalance they cause if they can not provide market commitments. Additionally,
FleetSim facilitates easy sensitivity analyses, adaption to future market
designs, and integration of novel data sets through its modular architecture and
expandable design (see Figure ref:fig-fleetsim). We consider FleetSim as a
research platform for sustainable and smart mobility similar to PowerTac
cite:ketter16_multiagent_comp_gaming. It builds on SimPy[fn:1], a process-based
discrete-event simulation framework. FleetSim is available open source[fn:2] and
can be readily installed as a Python package.

#+CAPTION[FleetSim Architecture]: Architecture of FleetSim label:fig-fleetsim
#+ATTR_LATEX: :width 1\linewidth :placement [hp]
[[./fig/simulation-platform.png]]

In order to simplify comparability and focus to real-world applicability of the
analysis, we set the same parameters for all conducted experiments (see Table
ref:table-sim-params). They are corresponding to the real Car2Go specifications
described in Chapter ref:sec-data-car2go. Further, we fixed the unknown
prediction accuracy of the fleets available charging power $\fPhat{}$ to an
estimate of modern forecast algorithms performance. The impact of the
predictions uncertainty on the results will later be determined in a sensitivity
analysis.

#+CAPTION[Simulation Parameters]: Simulation Parameters label:table-sim-params
#+ATTR_LATEX: :align lr :placement [hp]
|---------------------------------------------+--------------------------------------|
|---------------------------------------------+--------------------------------------|
| Parameter                                   | Value                                |
|---------------------------------------------+--------------------------------------|
| EV battery capacity ($\Omega$)              | 17.6 $\kwh$                          |
| EV charging power   ($\gamma$)              | 3.6 $\kw$                            |
| EV range                                    | 145 km                               |
| Industry electricity price  ($p^{ind}$)     | 0.15[fn:3] $\ekwh$                   |
| EV rental tariff                            | 0.24[fn:4] $\frac{\eur}{\text{min}}$ |
| EV long distance fee ($>\text{200 km}$)     | 0.29[fn:4] $\frac{\eur}{\text{km}}$  |
|---------------------------------------------+--------------------------------------|
| Prediction accuracy $\fPhat{}$ week ahead   | 70%                                  |
| Prediction accuracy $\fPhat{}$ 30 min ahead | 90%                                  |
|---------------------------------------------+--------------------------------------|
|---------------------------------------------+--------------------------------------|

** Integrated Bidding Strategy
# NOTE: Additional bar-plot with monthly increasing profits?
Research Question 1 examines whether a fleet operator can use a VPP portfolio of
EVs to profitably bid on multiple electricity markets. In Chapter
ref:sec-model-mechanism, we proposed a central control mechanism that charges
the fleet with an integrated bidding strategy. The following section evaluates
the results of the control mechanism in the simulation environment.

Table ref:table-sim-stats shows the descriptive statistics of the fleet
utilization during a simulation run, with data from June 1, 2016 to January
1, 2018. It can be observed that (a) the volatility of EVs parked at a charging
station is remarkably high (large standard deviation), and (b) the fraction of
EVs that can be utilized for VPP activities is diminishing low (3.55%). It is
apparent that a high uncertainty and the low share of EVs that can possibly
generate profits are challenging the economic sustainability of our proposed
model. Figure ref:fig-fleet-utilization shows that despite a changing rental
behavior throughout the day (e.g., rush hour peaks between 7:00-9:00 and
17:00-19:00), the amount of EVs that can utilized for VPP activities is
comparably stable throughout the day.

#+CAPTION[Fleet Utilzation]: Daily fleet utilization (average, standard deviation) from June 1, 2016 to January 1, 2018. The blue error band is illustrating the large volatility in the amount of EVs that get parked at a charging station. The share of EVs that can be used as VPP is on average only 3.55% of the fleet's size. Most of the EVs are either not connected to a charging station or are already fully charged. label:fig-fleet-utilization
#+ATTR_LATEX: :width 1\linewidth :placement [h]
[[./fig/fleet-utilization.png]]

#+BEGIN_SRC python :exports none
return(round((13.84 / 389.64),4) * 100)
#+END_SRC

#+RESULTS:
: 3.55
#+CAPTION[Fleet Statistics]: Fleet Statistics. label:table-sim-stats
#+ATTR_LATEX: :align lr :placement [hp]
|---------------------------------+----------------------------|
|---------------------------------+----------------------------|
| Statistic                       | Value                      |
|---------------------------------+----------------------------|
| Fleet size                      | 508                        |
| EVs available (min, max, *std*) | 389.64 (165, 496, *49.18*) |
| EVs connected (min, max, *std*) | 61.23 (34, 290, *61.11*)   |
| VPP EVs (min, max, *std*)       | 13.84 (0, 94, *9.01*)      |
|---------------------------------+----------------------------|
|---------------------------------+----------------------------|

We defined several "naive" bidding strategies to evaluate and benchmark the
performance of our developed model. The strategies are naive in that sense that
they are assuming a fixed risk associated with bidding at a specific electricity
market. As opposed to the developed RL agent, they do not take information of
their environment into account and adjust the bidding quantities dynamically.
Instead, the controller discounts the predicted amount of available charging
power with a fixed risk factor $\lambda$ (see \eqref{eq-model-pb} and
\eqref{eq-model-pi}). Naturally, the controller estimates a higher risk for
bidding on the balancing market week ahead than on the intraday market 30
minutes ahead. We defined following types of strategies:
1) Risk-averse ($\lb{}\!=\!0.5$, $\li{}\!=\!0.3$)

   The controller avoids denying rentals and causing imbalances at all costs. In
   order to not commit more charging power that it can provide, it places only
   bids for conservative amounts of electricity on the markets. The risk-averse
   strategies /Balancing/ and /Intraday/ are comparable to similar strategies
   developed by
   textcite:kahlen17_fleet,kahlen18_elect_vehic_virtual_power_plant_dilem.

2) Risk-seeking ($\lb{}\!=\!0.2$, $\li{}\!=\!0.0$)

   The controller aims to maximize its profits by trading as much electricity on
   the markets as possible. It strives to fully utilize the VPP and allocate a
   high percentage of available EVS to charge from the markets. Due to the
   rental uncertainty and a low estimated risk, the controller is prone to
   offering more charging power to the markets that it can provide. This may
   lead to lost rental costs or even imbalances.

3) Full information

   The optimal strategy to solve the controlled charging problem. The controller
   knows the bidding risks in advance and places the perfect bids on the
   markets. In other words, it charges the maximal amount of electricity from
   the markets without having to deny rentals or causing imbalances due to
   prediction uncertainties.

# NOTE: Mention numbers?
In Table ref:table-profits, the simulation results of all tested strategies are
listed. As expected, the developed integrated bidding strategies outperform
their single market counterparts. The controller is able to capitalize on the
most favorable market conditions and better utilizes the VPP by buying more
electricity from the markets than charging the EVs regularly. The integrated
strategies are resulting in 49%-54% more profits for the fleet than the single
market strategies.

A controller bidding according to the /Integrated (risk-averse)/ strategy, pays
on average $0.10 \ekwh$ less than other risk-averse strategies. A controller
with an /Integrated (risk-seeking)/ strategy, is even more profitable, despite
having to account for lost rental profits. On the other side, the controller
caused imbalances (highlighted red) which lead to high (unknown) market
penalties or even exclusion from bidding activities. For this reason, imbalances
need to be avoided, regardless of potential profits from a higher VPP
utilization. We expect that the proposed RL agent learns a bidding strategy,
which avoids imbalances while increasing profits at the same time. The upper
bound of the optimal strategy /Integrated (full information)/.

# TODO: Explain VPP Utilization?
# TODO: Risk-seeking should be lower, here: profits from buying energy, which
# could be charged from imbalances or lost rentals are still included.
#+LATEX: {\captionsetup[table]{aboveskip=0.5cm}
#+CAPTION[Bidding strategy outcomes]: Outcomes of naive bidding strategies over a 1.5 year period. Integrated bidding strategies outperform single market strategies. label:table-profits
#+ATTR_LATEX: :float sideways :align l|cccccc :placement [hp]
|                                     | \thead{Balancing\\(risk-averse)} | \thead{Intraday\\(risk-averse)} | \thead{Integrated\\(risk-averse)} | \thead{Integrated\\(risk-seeking)} | \thead{Integrated\\(full information)} |
|-------------------------------------+----------------------------------+---------------------------------+-----------------------------------+------------------------------------+----------------------------------------|
|-------------------------------------+----------------------------------+---------------------------------+-----------------------------------+------------------------------------+----------------------------------------|
| VPP utilization (%)                 |                               39 |                              47 |                                62 |                                 81 |                                     71 |
| Energy bought (MWh)                 |                              803 |                             985 |                              1292 |                               1681 |                                   1473 |
| Energy charged regularly (MWh)      |                             1278 |                            1096 |                               789 |                                400 |                                    608 |
| Lost rental profits (1000 \eur)     |                                0 |                               0 |                                 0 |                              15.47 |                                      0 |
| No. Lost rentals                    |                                0 |                               0 |                                 0 |                               1237 |                                      0 |
| Imbalances (MWh)                    |                                0 |                               0 |                                 0 |              \textcolor{red}{1.01} |                                      0 |
| Average electricity price ($\ekwh$) |                                - |                               - |                                 - |                                  - |                                      - |
| Gross profit increase (1000 \eur)   |                            43.62 |                           45.08 |                           *67.04* |                            *72.51* |                                  77.36 |
|-------------------------------------+----------------------------------+---------------------------------+-----------------------------------+------------------------------------+----------------------------------------|
|-------------------------------------+----------------------------------+---------------------------------+-----------------------------------+------------------------------------+----------------------------------------|
# #+TBLFM: @2=round(100*round(@3/(@3+@4),2))
# ::@10=100* round((@9/17707.85),4)
#+LATEX:}

#+BEGIN_SRC python :exports none
return(67.04 / 45.08)
#+END_SRC

#+RESULTS:
: 1.4871339840283941

#+BEGIN_SRC python :exports none
return(87.98 - 15.47)
#+END_SRC

#+RESULTS:
: 72.51

** Reinforcement Learning Portfolio Optimization
# NOTE: What about confidence intervals - Mean?
# TODO: No mention of risk factors except for summary. Include numbers or graphs
# of risks factors?

Research Question 2 investigates whether a RL agent can optimize the integrated
bidding strategy by dynamically adjusting the bidding quantities. The bidding
quantities $\Pb{}, \Pi{}$ are based on the evaluated risk associated with
bidding on the individual electricity markets. In Chapter ref:sec-model-rl, we
introduced a RL approach that learns the risk factors $\lb{}$, $\li{}$ based on
its observed environment and received reward signals. In Appendix
ref:app-rl-hyperparams, the hyperparameters are presented which we used to train
the dueling DDQN algorithm and solving the controlled charging problem under
uncertainty. The values were determined manually through experimentation for the
best results. The speed of convergence was also used as a criterion, since the
training environment Google Colaboratory only allows up to 12 hours of computing
time.

Further, the imbalance costs $\beta$ were set to an artificially high value to
incentivize the agent to learn to always avoid imbalances. Whenever the agents
takes an action that causes imbalances (i.e., bid too much electricity), it will
receive a highly negative reward signal, leading to a low estimated Q-value of
that chosen action in a specific state.

#+CAPTION[Comparison of gross profit results]: Comparison of gross profits and traded electricity between the proposed optimized integrated strategy and the other three naive charging strategies. The RL algorithm improves the achieved gross profit increase of the integrated bidding strategy on average by 16% and accomplishes nearly optimal results when compared to the benchmark strategy. label:fig-rl-profits
#+ATTR_LATEX: :width 1\linewidth :placement [h]
[[./fig/rl-results.png]]

In Figure ref:fig-rl-profits, the performance of the optimized integrated
bidding strategy is presented. The proposed RL algorithm increases the gross
profits of the fleet on average (n=5) by approximately 72-75% when compared with
the naive single market strategies and by approximately 16% when compared with
the naive integrated strategy. In none of the tested strategies the controller
procured more electricity from the market that it can charge. To reach the
optimal solution the gross profits would need to be increased by 1.5% further.
Similarly, the RL algorithm increases the amount of electricity that the fleet
charges from the electricity markets while avoiding imbalance at the same time.

In another experiment, we evaluated the performance of the proposed RL algorithm
in comparison to other RL algorithms with a simpler architecture. In particular,
we were interested what impact modern advances in deep RL have on the ability to
quickly learn to improve the agents policy, while still achieving good results
after the whole training period. This question is especially relevant for the
case, when no prior training for the fleet controller is possible and the agent
has to quickly learn to avoid procuring more energy from the markets that it can
charge. Therefore, we removed the notion of imbalance costs and changed the
simulation setup to instantly stop the training episode when imbalances occur.
In this way, the agents learns to maximize its reward while circumventing
imbalances at all costs. The agent achieves a higher reward the longer it trades
electricity on the markets without committing to charge more electricity than it
can. We compared the DQN algorithm
cite:mnih15_human_level_contr_throug_deep_reinf_learn with the Double DQN
algorithm cite:hasselt16_deep_reinf_learn_doubl_q_learn with and without the
dueling architecture cite:wang15_duelin_networ_archit_deep_reinf_learn. In
Figure ref:fig-rl-learning, the average (n=5) learning performances of the
different RL approaches are displayed.

#+CAPTION[Comparison of RL algorithm learning performance]: Comparison of the learning performance between the proposed RL algorithm and the other three simpler algorithms, averaged over 5 training attempts. Each training period is performed in 1.5 years simulation time with real world data. The dueling DDQN algorithm (dark blue line) learns faster, and achieves better end results than prior algorithms. label:fig-rl-learning
#+ATTR_LATEX: :width 1\linewidth :placement [h]
[[./fig/rl-learning.png]]

The experiment shows that the dueling DDQN algorithm learns the fastest and
shows a large increase in mean reward per action after roughly 60 episodes
(about 227 days of simulation time) of training. The dueling DDQN algorithm
shows the largest reward increase and highest reward per action after the whole
training period, which makes it the best algorithm to solve the charging
problem. Despite that it still has a larger mean absolute error than the DDQN
algorithm, indicating that it is more likely to cause imbalances with the
dueling architecture than without. None of the algorithms determined a policy
that never caused imbalances after training on the full 1.5 years of simulation
time (about one hour computing time). In other words, without prior training
with existing data the RL agent would need more than one and a half years to
learn to avoid imbalances. A possible explanation is the problem of learning
from long delayed rewards, first discussed by
textcite:watkins89_learn_from_delay_rewar. Long delayed rewards increase the
difficulty of RL problems, since the agent needs to connect occurring decision
outcomes to specific actions way back in the past. In the case of the presented
controlled fleet charging problem, this effect is especially pronounced because
a negative reward signal (caused imbalances) can occur up to 672 timesteps (one
week) after the agent decided on the bidding quantity for the balancing market,
whereas the reward signals from the intraday market occur almost immediately
after 2 timesteps (30 minutes).

In summary, both experiments show that our approach is able to learn a
profit-maximizing bidding strategy under varying circumstances, without using
any a priori information about the EV rental patterns. The proposed control
mechanism improves existing approaches and the RL agent can successfully
optimize the VPP portfolio strategy by estimating the risk that is associated
with bidding on the markets.

** Sensitivity Analysis
# NOTE: Prediction Accuracy of available EV capacity
# NOTE: Balancing accuracy always lower than intraday! Otherwise no sense
# NOTE: Do predict electricity prices?

# NOTE: Results heavily dependent on industry charging price, since on average
# the balancing prices are 50% cheaper, and intraday 30% cheaper. Perform a run?
The uncertainty of the available fleet charging power forecast plays an
important role in determining the optimal bidding quantity. If the fleet
controller is certain about the number of connected EVs that it can use for VPP
activities in the future, it can aggressively trade the available charging power
on the markets, without having to worry to turn away customers or facing risk of
not being able to charge to committed amount. Since our model assumes


*** Prediction Accuracy
- Compare profits of (50%, 60%, 70%, 80%, 90%, 100%)

*** Charging infrastructure
- Fastchargers (22kw)
- Tesla (Bigger battery, big charger)
- Car batteries
- More Charging Stations (infer charging stations)

#+LATEX: \clearpage

* Conclusion
# NOTE 5%
# NOTE Sec: Setting
Integrating volatile renewable energy sources into the electricity system
imposes challenges on the electricity grid. In order to ensure grid stability
and avoid blackouts, balancing power is needed to match electricity supply and
demand. Balancing power can be provided by VPPs that generate or consume energy
within a short period of time and offer these services on the electricity
markets cite:pudjianto07_virtual_power_plant_system_integ. EV fleet operators
can utilize idle vehicles to form VPPs and offer available EV battery capacity
as balancing power. The fleet can offer balancing services directly via tender
auctions on the balancing market or via continuous trades on the intraday
market, where participants procure or sell energy to self-balance their
portfolios cite:pape16_are_fundam_enoug. Both markets have complementary
properties in terms of their price level and lead time, which motivates the
creation of a VPP portfolio to profitably participate in both markets and extend
the business model if the fleet.

# NOTE Sec: Problem
# NOTE Mention risks? Mention portfolio (optimization)?
However, there are certain risks associated  with this business model extension.
EVs can only be allocated to the VPP portfolio if they are connected to a
charging station and have sufficient free battery capacity available. This
uncertainty makes it difficult for fleet operators to estimate the size of the
VPP and calculate the optimal bidding quantities. Moreover, if fleet operators
offer more balancing power than they can provide, they face high imbalance
penalties from the markets. For a sustainable business model fleet operators
also need to balance VPP activities with their primary offering, customer
mobility. Denying customer rentals to ensure that the fleet can fulfill the
market commitments, results in opportunity costs of lost rentals that compromise
the profitability of the fleet.

In the following chapter, we will summarize the conducted research of this
thesis to solve the aforementioned challenges. Furthermore, the contribution of
this work is outlined and the main results are presented and discussed. Finally,
we list specific limitations of this study and give insights on further areas of
research.
** Contribution
# NOTE Sec: What we have done
#     1. Model (Control mechanism)
#     2. Simulation Platform
#     3. Integrated bidding strategy
#     4. RL Agent that optimizes strategy by determining risk
The core contributions of this thesis are the following: First, we
conceptualized a DSS for controlled EV charging under uncertainty. The DSS
constitutes the core of a business model extension for fleet operators to manage
a VPP portfolio of EV batteries. A controlled charging problem has been
mathematically formulated and a control mechanism introduced that solves the
proposed problem. Second, we developed an event-based simulation platform that
facilitates fleet management research based on real-world data. We evaluated
various baseline bidding strategies within the simulation platform and tested
out the behavior of intelligent agents in the context of controlled EV charging.
Third, we proposed a novel integrated bidding strategy that offers balancing
services of a VPP portfolio to multiple electricity markets simultaneously.
Instead of submitting only conservative amounts of battery capacity to a single
market, like previous studies
cite:kahlen18_elect_vehic_virtual_power_plant_dilem,kahlen17_fleet did, our
proposed strategy aims to maximize profits by aggressively procuring electricity
from the multiple markets without causing imbalances.
Forth, we proposed the usage of a RL Agent that learns to optimize the VPP
portfolio. The agent dynamically determines the risks of bidding on the markets,
depending available and predicted fleet information. Therefore a MDP has been
defined and designed to work in previously unknown environments and uncertain
conditions. We designed the RL agent with the focus on real-world applicability,
generalizability and fast convergence rates. We expect the proposed approach to
work with a variety of different EVs (e.g., electronic bikes) and
independent of the fleets geographical location.

# - not limited to the training data set

# - obtained better results than similar studies in the
# field. (how much and which?)

# TODO: Double check numbers!
We evaluated the proposed method with real-world carsharing data from Car2go in
Stuttgart and German electricity market data from June 2017 till January 2018.
The results show that our approach would increase the gross profits of the EV
fleet by roughly 78 000 $\eur$ over the 1.5 year period. Free float carsharing

At the same time the VPP activities have an environmental impact by reducing
CO_2 through more efficient use of renewables.

# Insights:
# - RL works good
# - Prediction is important

# NOTE Sec: What we have found
- What we have shown/found (include key numbers)
  - 1. RQ
    - Integrated improves existing approaches
    - Mitigate risk & improve profits through exploitation of different market setups
  - 2. RQ
    - RL Algorithm that
    - Online learning algorithm (difference? -- real data?)
    - RL can learn to dynamically adjust bidding quantities by learning risk
      associated with bidding on each market. (What are the risks?)
    - RL Architecture/Advancements matters
    - RL takes a long time to learn
    - Charging infrastructure & Advancement in battery technology/charging
       technology matters

# NOTE Sec: Discuss !
Discuss:
- In free float carsharing, mobility demand patterns are extremely uncertain.
  $\rightarrow$ lower bound.

- Compare to other studies!
  - Fleet Charging
    - No uncertainty
    - Only one market
    - No sensitivity on accuracy prediction (We found very important)
  - RL
  - Other approaches (VPP, stochastic)
       cite:kahlen18_elect_vehic_virtual_power_plant_dilem (no imbalance costs!)
       cite:vandael15_reinf_learn_heuris_ev_fleet

- Discuss results?
  - Gross profits small - Mention before?
  - Policy
  - Investment
  - V2G?
  - Market Setup


** Limitations
# We don't take market dynamics into account
# # NOTE: Citatation from kahlen
# The fleet controller offers bids and asks for every time interval. These offers
# contain both a quantity and a reservation price, which depends on the state of
# charge of the EV storage, as well as on the battery costs. However, the market
# may or may not accept these offers depending on the composition of the offer
# prices from the fleet owner and other market participants. The market auction
# mechanism ultimately decides when EVs will charge and discharge.

- Model:
  - Bidding Mechanism: one week ahead, always accepted
  - Policy & Regulation: EVs not allowed to provide balancing power, minimum
    bidding quantities 1MW.
  - Markets: Fleet is a price-taker, what about larger fleets? Simulate market influence
  - Deny rentals only in the same market period (More deny, less imbalance)
- RL: See cite:vazquez-canteli19_reinf_learn_deman_respon conclusion for
  limitations.
  - Training time in real time. Generalization to other cities?
** Future Research

- Model:
    - Investigate modern/current market design, that changed their bidding
      mechanisms to to better integrate renewable energy generators.
      - Daily/Day-ahead tenders with 4 hour market periods.

      Mischpreisverfahren

       i.e. daily w/ 4h slots. German "Mischpreisverfahren"
- RL: Long-delayed rewards, different reward structure, memory based
- Prediction Algorithms improvement, reference to sensitivity analysis

#+LATEX: \clearpage
#+LATEX: \appendix
* Appendix: Data Tables label:app-data-tables
#+LATEX: \renewcommand\thetable{\thesection.\arabic{table}}
#+LATEX: \setcounter{table}{0}

#+LATEX: {\captionsetup[table]{aboveskip=0.5cm}
#+CAPTION: List of Trades of the EPEX Spot Intraday Continuous Market
#+ATTR_LATEX: :float sideways :align c|cccccccc :placement [hp]
|---------------------+---------+------------+----------+------------+-------------+---------+---------------+---------------|
|---------------------+---------+------------+----------+------------+-------------+---------+---------------+---------------|
| Execution time      |      ID | Unit Price | Quantity | Buyer Area | Seller Area | Product | Product Time  | Delivery Date |
|---------------------+---------+------------+----------+------------+-------------+---------+---------------+---------------|
| 2017-12-04 06:54:55 | 8031392 |      51.00 |     5500 | Amprion    | Amprion     | Quarter | 07:15 - 07:30 |    2017-12-04 |
| 2017-12-04 06:53:26 | 8031391 |      59.00 |    10000 | TenneT     | TenneT      | Quarter | 07:15 - 07:30 |    2017-12-04 |
| 2017-12-04 06:53:26 | 8031390 |      58.90 |    10000 | TenneT     | TenneT      | Quarter | 07:15 - 07:30 |    2017-12-04 |
| 2017-12-04 06:53:15 | 8031389 |      52.30 |     7000 | 50Hertz    | 50Hertz     | Quarter | 07:15 - 07:30 |    2017-12-04 |
| 2017-12-04 06:53:13 | 8031386 |      59.00 |      500 | TenneT     | TenneT      | Quarter | 07:15 - 07:30 |    2017-12-04 |
| 2017-12-04 06:53:13 | 8031387 |      51.00 |     3600 | Amprion    | Amprion     | Quarter | 07:15 - 07:30 |    2017-12-04 |
| 2017-12-04 06:53:13 | 8031388 |      52.00 |     1400 | Amprion    | Amprion     | Quarter | 07:15 - 07:30 |    2017-12-04 |
| 2017-12-04 06:53:02 | 8031385 |      58.90 |    11000 | TenneT     | TenneT      | Quarter | 07:15 - 07:30 |    2017-12-04 |
| 2017-12-04 06:52:38 | 8031380 |      60.00 |    10000 | Amprion    | Amprion     | Quarter | 07:15 - 07:30 |    2017-12-04 |
| 2017-12-04 06:52:38 | 8031381 |      57.50 |     8000 | Amprion    | Amprion     | Quarter | 07:15 - 07:30 |    2017-12-04 |
| 2017-12-04 06:52:38 | 8031382 |      58.00 |     2000 | Amprion    | Amprion     | Quarter | 07:15 - 07:30 |    2017-12-04 |
| 2017-12-04 06:52:38 | 8031383 |      58.90 |     4000 | TenneT     | TenneT      | Quarter | 07:15 - 07:30 |    2017-12-04 |
| 2017-12-04 06:52:38 | 8031384 |      60.00 |     4000 | Amprion    | Amprion     | Quarter | 07:15 - 07:30 |    2017-12-04 |
| 2017-12-04 06:52:27 | 8031379 |      52.30 |     8000 | 50Hertz    | 50Hertz     | Quarter | 07:15 - 07:30 |    2017-12-04 |
| 2017-12-04 06:51:33 | 8031378 |      66.00 |     5000 | TransnetBW | TransnetBW  | Quarter | 07:15 - 07:30 |    2017-12-04 |
| 2017-12-04 06:51:28 | 8031377 |      54.00 |     8000 | Amprion    | Amprion     | Quarter | 07:15 - 07:30 |    2017-12-04 |
| 2017-12-04 06:51:24 | 8031376 |      54.00 |     7000 | TenneT     | TenneT      | Quarter | 07:15 - 07:30 |    2017-12-04 |
| 2017-12-04 06:49:34 | 8031375 |      51.00 |     4000 | TenneT     | TenneT      | Quarter | 07:15 - 07:30 |    2017-12-04 |
| 2017-12-04 06:49:26 | 8031374 |      54.00 |     5000 | 50Hertz    | 50Hertz     | Quarter | 07:15 - 07:30 |    2017-12-04 |
| 2017-12-04 06:49:23 | 8031373 |      55.10 |     8000 | 50Hertz    | 50Hertz     | Quarter | 07:15 - 07:30 |    2017-12-04 |
|---------------------+---------+------------+----------+------------+-------------+---------+---------------+---------------|
|---------------------+---------+------------+----------+------------+-------------+---------+---------------+---------------|
#+LATEX:}
#+LATEX: \clearpage

* Appendix: Data Preprocessing label:app-data-processing
The raw Car2Go dataset contained about 63 million data points (7.82 GB) and has
been processed to only contain about 1.25 thousand data points (100 MB), while
still compromising all necessary information. The following preprocessing steps
have been taken to prepare the data for further analysis:

1) Determine rental trips

   Rental trips were inferred based on changing GPS coordinates between two data
   points of the same EV (see Table ref:table-car2go-raw, 4^{th} to 5^{th} row).
   Since the GPS accuracy of industry sensors is approximately 5 meters under
   open sky, and worse near buildings, bridges, and trees[fn:5], measurement
   errors occur. To reduce the number of falsely identified trips, a decreased
   resolution of a 10-meter radius was calculated. Note that we hereby assume
   that customers do not undertake trips, which begin and end at the same
   location. In total, 1246040 customer rentals were determined.

2) Determine trip distances

   The trip distances were inferred based on the difference in SoC level before
   and after the rental and the average energy consumption per kilometer.
   Although the real distance that can be covered with a certain amount of
   energy depends on driving style and traffic situation, we are confident that
   the deviation will be small, since all trips take place within the same urban
   space.

3) Determine rental profits

   The rental profits were inferred based on the trip duration and the estimated
   trip distance. Car2Go has a minutely pricing scheme with an additional long
   distance fee that applies when the customer drives more than a specified
   range. The rental profits form an important piece of information for our
   model since they determine the opportunity cost $\rho$ the fleet controller
   has to account for if it denies the rental in favor of VPP activities.

4) Clean data
   - /Service trips/: 3308 rental trips were removed that had a trip duration
     longer than the maximum allowed rental time of two days. We assume that
     these trips were /service trips/ undertaken by Car2Go. When the EVs
     returned with a higher SoC (e.g., they have been charged at the car repair
     shop), the previous trip had to be altered to end at a charging station to
     ensure charging consistency.
   - /Incorrectly charged EVs/: 52 EVs were removed that showed incorrect
     charging behavior. These EVs reported a battery level increase of more than 20%
     between trips or during trips, while not being located at a charging station.

#+LATEX: \clearpage
* Appendix: DDQN Hyperparameters label:app-rl-hyperparams
#+LATEX: \setcounter{table}{0}
#+CAPTION[Hyperparameters for the dueling network DDQN algorithm]: Hyperparameters for the dueling network DDQN algorithm.
#+ATTR_LATEX: :environment longtable :align lr :placement [hp]
|--------------------------------+--------------------------------------|
|--------------------------------+--------------------------------------|
| Hyperparameter                 |                                Value |
|--------------------------------+--------------------------------------|
| Target network update interval |                                 0.01 |
| Optimizer                      |              Adam cite:kingma14_adam |
| Batch size                     |                                   32 |
| Learning rate                  |                              10^{-3} |
| Replay buffer size             |                                 10^5 |
| Warm-up steps                  |                                 1000 |
| Training steps                 |                                 10^5 |
| Discount factor $\gamma$       |                                 0.99 |
| Reward normalization           |                      r $\in$ [-1, 1] |
| Gradient clipping              | Gradient in [-1, 1] for output layer |
| Network architecture           |      See Appendix ref:rl-network-def |
|--------------------------------+--------------------------------------|
| \epsilon-greedy annealing      |               Linear over 10^5 steps |
| \epsilon-greedy minimum value  |                                  0.1 |
|--------------------------------+--------------------------------------|
|--------------------------------+--------------------------------------|
#+LATEX: \clearpage
* Appendix: DDQN Network Definition label:rl-network-def

#+INCLUDE: "./template/keras.tex" export latex

#+LATEX: \clearpage
bibliography:~/uni/ma-thesis/bibliography/references.bib
bibliographystyle:apacite
* Footnotes

[fn:1] https://pypi.org/project/simpy/

[fn:2] https://github.com/indyfree/fleetsim

[fn:3] Average prices of electricity for the industry with an annual consumption
of 500 MWh - 2000 MWh in Germany 2017 cite:bmwi.19_prices_german.

[fn:4] Rental fees according to the Car2Go pricing scheme. See
https://www.car2go.com/media/data/germany/legal-documents/de-de-pricing-information.pdf,
accessed 15^th March 2019.

[fn:5] See https://www.gps.gov/systems/gps/performance/accuracy, accessed
23^{th} February 2019.
