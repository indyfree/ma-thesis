#+TITLE: Reinforcement Learning Portfolio Optimization of Electric Vehicle Virtual Power Plants
#+AUTHOR:Tobias Richter

#+INCLUDE: "./header.org"
#+INCLUDE: "./chapters/introduction.org"
#+INCLUDE: "./chapters/background.org"
#+INCLUDE: "./chapters/empirical-setting.org"
#+INCLUDE: "./chapters/model.org"

* Results
# NOTE: 15%

The following chapter will cover the main results of this research. First, the
simulation environment is presented. Second, we examine the economic
sustainability of an integrated bidding strategy. Third, the RL approach that
aims to optimize the VPP portfolio is evaluated. In the last section, we will
perform sensitivity analyses on the limiting algorithmic factor, the prediction
accuracy, and a limiting physical factor, the charging infrastructure.

** Simulation Environment
# NOTE: V2G?
As part of this research, we developed an event-based simulation platform called
/FleetSim/. The platform allows researchers to develop and test out different
smart charging and bidding strategies in realistic environment based on
real-world data. In FleetSim, intelligent agents (called controllers) centrally
control the charging of an EV fleet, they are responsible to sufficiently charge
their vehicles to satisfy real mobility demand. At the same time, the agents can
create VPP of EVs, provide balancing services to the grid, and take part in
electricity trading. Trips are simulated on an individual level, for example,
not charging an individual EV at a particular point in time, can cause a whole
series of lost rentals, due to an insufficient amount of battery for the next
arriving customers. The agents are evaluated based on the profits of charging
the fleet cheaper than the industry tariff, the costs of losing rentals and the
imbalance they cause if they can not provide market commitments. Additionally,
FleetSim facilitates easy sensitivity analyses, adaption to future market
designs, and integration of novel data sets through its modular architecture and
expandable design (see Figure ref:fig-fleetsim). We consider FleetSim as a
research platform for sustainable and smart mobility similar to PowerTac
cite:ketter16_multiagent_comp_gaming. It builds on SimPy[fn:1], a process-based
discrete-event simulation framework. FleetSim is available open source[fn:2] and
can be readily installed as a Python package.

#+CAPTION[FleetSim Architecture]: Architecture of FleetSim label:fig-fleetsim
#+ATTR_LATEX: :width 1\linewidth :placement [hp]
[[./fig/simulation_platform.png]]

In order to simplify comparability and focus to real-world applicability of the
analysis, we set the same parameters for all conducted experiments (see Table
ref:table-sim-params). They are corresponding to the real Car2Go specifications
described in Chapter ref:sec-data-car2go. Further, we fixed the unknown
prediction accuracy of the fleets available charging power $\fPhat{}$ to an
estimate of modern forecast algorithms performance. The impact of the
predictions uncertainty on the results will later be determined in a sensitivity
analysis.

#+CAPTION[Simulation Parameters]: Simulation Parameters label:table-sim-params
#+ATTR_LATEX: :environment longtable :align lr :placement [hp]
|---------------------------------------------+--------------------------------------|
|---------------------------------------------+--------------------------------------|
| Parameter                                   | Value                                |
|---------------------------------------------+--------------------------------------|
| EV battery capacity ($\Omega$)              | 17.6 $\kwh$                          |
| EV charging power   ($\gamma$)              | 3.6 $\kw$                            |
| EV range                                    | 145 km                               |
| Industry electricity price  ($p^{ind}$)     | 0.15[fn:3] $\ekwh$                   |
| EV rental tariff                            | 0.24[fn:4] $\frac{\eur}{\text{min}}$ |
| EV long distance fee ($>\text{200 km}$)     | 0.29[fn:4] $\frac{\eur}{\text{km}}$  |
|---------------------------------------------+--------------------------------------|
| Prediction accuracy $\fPhat{}$ week ahead   | 70%                                  |
| Prediction accuracy $\fPhat{}$ 30 min ahead | 90%                                  |
|---------------------------------------------+--------------------------------------|
|---------------------------------------------+--------------------------------------|

** Integrated Bidding Strategy
# NOTE: Additional bar-plot with monthly increasing profits?
Research Question 1 examines whether a fleet operator can use a VPP portfolio of
EVs to profitably bid on multiple electricity markets. In Chapter
ref:sec-model-mechanism, we proposed a central control mechanism that charges
the fleet with an integrated bidding strategy. The following section evaluates
the results of the control mechanism in the simulation environment.

Table ref:table-sim-stats shows the descriptive statistics of the fleet
utilization during a simulation run, with data from June 1, 2016 to January
1, 2018. It can be observed that (a) the volatility of EVs parked at a charging
station is remarkably high (large standard deviation), and (b) the fraction of
EVs that can be utilized for VPP activities is diminishing low (3.55%). It is
apparent that a high uncertainty and the low share of EVs that can possibly
generate profits are challenging the economic sustainability of our proposed
model. Figure ref:fig-fleet-utilization shows that despite a changing rental
behavior throughout the day (e.g., rush hour peaks between 7:00-9:00 and
17:00-19:00), the amount of EVs that can utilized for VPP activities is
comparably stable throughout the day.

#+CAPTION[Fleet Utilzation]: Daily fleet utilization (average, standard deviation) from June 1, 2016 to January 1, 2018. The blue error band is illustrating the large volatility in the amount of EVs that get parked at a charging station. The share of EVs that can be used as VPP is on average only 3.55% of the fleet's size. Most of the EVs are either not connected to a charging station or are already fully charged. label:fig-fleet-utilization
#+ATTR_LATEX: :width 1\linewidth :placement [h]
[[./fig/fleet-utilization.png]]

#+BEGIN_SRC python :exports none
return(round((13.84 / 389.64),4) * 100)
#+END_SRC

#+RESULTS:
: 3.55
#+CAPTION[Fleet Statistics]: Fleet Statistics. label:table-sim-stats
#+ATTR_LATEX: :environment longtable :align lr :placement [hp]
|---------------------------------+----------------------------|
|---------------------------------+----------------------------|
| Statistic                       | Value                      |
|---------------------------------+----------------------------|
| Fleet size                      | 508                        |
| EVs available (min, max, *std*) | 389.64 (165, 496, *49.18*) |
| EVs connected (min, max, *std*) | 61.23 (34, 290, *61.11*)   |
| VPP EVs (min, max, *std*)       | 13.84 (0, 94, *9.01*)      |
|---------------------------------+----------------------------|
|---------------------------------+----------------------------|

We defined several "naive" bidding strategies to evaluate and benchmark the
performance of our developed model. The strategies are naive in that sense that
they are assuming a fixed risk associated with bidding at a specific electricity
market. As opposed to the developed RL agent, they do not take information of
their environment into account and adjust the bidding quantities dynamically.
Instead, the controller discounts the predicted amount of available charging
power with a fixed risk factor $\lambda$ (see \eqref{eq-model-pb} and
\eqref{eq-model-pi}). Naturally, the controller estimates a higher risk for
bidding on the balancing market week ahead than on the intraday market 30
minutes ahead. We defined following types of strategies:
1) Risk-averse ($\lb{}\!=\!0.5$, $\li{}\!=\!0.3$)

   The controller avoids denying rentals and causing imbalances at all costs. In
   order to not commit more charging power that it can provide, it places only
   bids for conservative amounts of electricity on the markets. The risk-averse
   strategies /Balancing/ and /Intraday/ are comparable to similar strategies
   developed by
   textcite:kahlen17_fleet,kahlen18_elect_vehic_virtual_power_plant_dilem.

2) Risk-seeking ($\lb{}\!=\!0.2$, $\li{}\!=\!0.0$)

   The controller aims to maximize its profits by trading as much electricity on
   the markets as possible. It strives to fully utilize the VPP and allocate a
   high percentage of available EVS to charge from the markets. Due to the
   rental uncertainty and a low estimated risk, the controller is prone to
   offering more charging power to the markets that it can provide. This may
   lead to lost rental costs or even imbalances.

3) Full information

   The optimal strategy to solve the controlled charging problem. The controller
   knows the bidding risks in advance and places the perfect bids on the
   markets. In other words, it charges the maximal amount of electricity from
   the markets without having to deny rentals or causing imbalances due to
   prediction uncertainties.

# NOTE: Mention numbers?
In Table ref:table-profits, the simulation results of all tested strategies are
listed. As expected, the developed integrated bidding strategies outperform
their single market counterparts. The controller is able to capitalize on the
most favorable market conditions and better utilizes the VPP by buying more
electricity from the markets than charging the EVs regularly. The integrated
strategies are resulting in 49%-54% more profits for the fleet than the single
market strategies.

A controller bidding according to the /Integrated (risk-averse)/ strategy, pays
on average $0.10 \ekwh$ less than other risk-averse strategies. A controller
with an /Integrated (risk-seeking)/ strategy, is even more profitable, despite
having to account for lost rental profits. On the other side, the controller
caused imbalances (highlighted red) which lead to high (unknown) market
penalties or even exclusion from bidding activities. For this reason, imbalances
need to be avoided, regardless of potential profits from a higher VPP
utilization. We expect that the proposed RL agent learns a bidding strategy,
which avoids imbalances while increasing profits at the same time. The upper
bound of the optimal strategy /Integrated (full information)/.

# TODO: Explain VPP Utilization?
# TODO: Risk-seeking should be lower, here: profits from buying energy, which
# could be charged from imbalances or lost rentals are still included.
#+LATEX: {\captionsetup[table]{aboveskip=0.5cm}
#+CAPTION[Bidding strategy outcomes]: Outcomes of naive bidding strategies over a 1.5 year period. Integrated bidding strategies outperform single market strategies. label:table-profits
#+ATTR_LATEX: :float sideways :align l|cccccc :placement [hp]
|                                     | \thead{Balancing\\(risk-averse)} | \thead{Intraday\\(risk-averse)} | \thead{Integrated\\(risk-averse)} | \thead{Integrated\\(risk-seeking)} | \thead{Integrated\\(full information)} |
|-------------------------------------+----------------------------------+---------------------------------+-----------------------------------+------------------------------------+----------------------------------------|
|-------------------------------------+----------------------------------+---------------------------------+-----------------------------------+------------------------------------+----------------------------------------|
| VPP utilization (%)                 |                               39 |                              47 |                                62 |                                 81 |                                     71 |
| Energy bought (MWh)                 |                              803 |                             985 |                              1292 |                               1681 |                                   1473 |
| Energy charged regularly (MWh)      |                             1278 |                            1096 |                               789 |                                400 |                                    608 |
| Lost rental profits (1000 \eur)     |                                0 |                               0 |                                 0 |                              15.47 |                                      0 |
| No. Lost rentals                    |                                0 |                               0 |                                 0 |                               1237 |                                      0 |
| Imbalances (MWh)                    |                                0 |                               0 |                                 0 |              \textcolor{red}{1.01} |                                      0 |
| Average electricity price ($\ekwh$) |                                - |                               - |                                 - |                                  - |                                      - |
| Gross profit increase (1000 \eur)   |                            43.62 |                           45.08 |                           *67.04* |                            *72.51* |                                  77.36 |
|-------------------------------------+----------------------------------+---------------------------------+-----------------------------------+------------------------------------+----------------------------------------|
|-------------------------------------+----------------------------------+---------------------------------+-----------------------------------+------------------------------------+----------------------------------------|
# #+TBLFM: @2=round(100*round(@3/(@3+@4),2))
# ::@10=100* round((@9/17707.85),4)
#+LATEX:}

#+BEGIN_SRC python :exports none
return(67.04 / 45.08)
#+END_SRC

#+RESULTS:
: 1.4871339840283941

#+BEGIN_SRC python :exports none
return(87.98 - 15.47)
#+END_SRC

#+RESULTS:
: 72.51

** Reinforcement Learning Portfolio Optimization
# NOTE: What about confidence intervals - Mean?

Research Question 2 investigates whether a RL agent can optimize the integrated
bidding strategy by dynamically adjusting the bidding quantities. The bidding
quantities $\Pb{}, \Pi{}$ are based on the evaluated risk associated with
bidding on the individual electricity markets. In Chapter ref:sec-model-rl, we
introduced a RL approach that learns the risk factors $\lb{}$, $\li{}$ based on
its observed environment and received reward signals. In Appendix ref:app-b, the
hyperparameters are presented which we used to train the dueling DDQN algorithm
and solving the controlled charging problem under uncertainty. The values were
determined manually through experimentation for the best results. The speed of
convergence was also used as a criterion, since the training environment, Google
Colaboratory, only allows up to 12 hours of computing time.

Further, the imbalance costs $\beta$ were set to an artificially high value of
10000 $\emwh$ to let the agent learn to always avoid imbalances. Whenever the
agents takes an action that causes imbalances, it will receive a highly negative
reward signal, leading to low estimated values of that chosen action in a
specific state.

#+CAPTION[Fleet Profits]: Accumulated gross profit increase and charged electricity over time. The developed RL algorithm learned to optimized the presented integrated bidding strategy while still avoiding imbalances. label:fig-rl-profits
#+ATTR_LATEX: :width 1\linewidth :placement [h]
[[./fig/rl-results.png]]

*** Results:
1) Profits
   - Graph: Cost evolution of the RL portfolio optimization approach benchmarked
     against naive strategies (w/ error band)
   - Graph cumulative (charging) profits over 1 simulation run, w/ learned RL agent (over
     how many iterations?)
   - Regular charging comparison? (VPP share is too low?)
   - Graph: Cumulative charging costs over simulation run (days)
     Fig. 4. Comparison of charging costs between the proposed novel charging
     strategy and other three charging strategies. The average cost values and the
     variance intervals of the methods are presented. The proposed novel method
     reduces the costs of charging by roughly 50% when compared with the
     conventional charging method and by roughly 10% when compared with a
     daily optimal charging strategy. To reach the global optimal solution the cost
     would need to be reduced by X% further.

2) Learning
   - Graph: Learning over iterations (DQN vs Q-Learning)
   - Compare RL Algos eg. Q-learning vs DDQN Reward per timestep or Profits
   - \rightarrow deep learning makes difference in practice for complex system.
   # - https://github.com/dennybritz/reinforcement-learning/blob/master/TD/Q-Learning%20Solution.ipynb
   # - https://gist.github.com/vihar/dcb1272a04b98ce3fdb2c109af7eaa21#file-q-learning-py

3) Actions
   - Prob distribution of optimal actions for the fleet during weekdays. cite:vaya14_optim
   - What actions/risk were chosen on average?

*** Problems:
- long-delayed rewards make RL hard (!?)
- positive reward from buying immediately vs negative rewards up to 7 days.
- Imbalance costs need to bet set manually, episodic vs continuous env. Stop
  when encountering imbalance costs.

*** Summary:
- In summary, all four experiments show that our approach is
  able to learn a cost-effective day-ahead schedule under varying
  circumstances, without using any a priori information about
  the EVs.

** Sensitivity Analysis
# NOTE: Prediction Accuracy of available EV capacity
# NOTE: Balancing accuracy always lower than intraday! Otherwise no sense
# NOTE: Do predict electricity prices?

# NOTE: Results heavily dependent on industry charging price, since on average
# the balancing prices are 50% cheaper, and intraday 30% cheaper. Perform a run?

*** Prediction Accuracy
- Compare profits of (50%, 60%, 70%, 80%, 90%, 100%)

*** Charging infrastructure
- Fastchargers (22kw)
- Car batteries
- More Charging Stations (infer charging stations)

*** Bidding Mechanism
# NOTE: Relaxed minimum bidding assumption of 1MW to no limit.
- Minimal bidding quantities (100kw, 500kw, 1MW)

* Conclusion
# NOTE 5%
** Contribution

1. Problem setting
   - Balancing power is needed to integrate renewable into the grid
   - Balancing power can be supplied a) via the balancing market by the TSOs and
     b) via the open intraday market (citation, why that is balancing too)
   - EV fleets can provide (negative) balancing power with the batteries of
     their vehicles
   - Problem) For a sustainable business model EV fleet operators need to
     balance providing their main offering customer mobility versus
   - Problem: a) Customer mobility vs VPP b) over-commitments to the markets

2. What we have done
   - Decision Support System for controlled EV charging from multiple markets,

   - Evaluated based on real world carsharing fleet data from Car2go in the
     period from June 2017 to January 2018 . (Car2Go *very*
     uncertain rental demand!)
   - Business model for EV fleet owners with better results than previous studies
   - Environmental impact by providing balancing power
   - Developed event-based simulation platform to evaluate bidding strategies and RL agents,
     facilitate research based on real world data(!)
   - Developed Control mechanism (DSS) that "Under uncertainty"
   - Developed a RL Agent that
     - designed to work in previously unknown
       environments and thus
     - suited to deploy in real life settings of all kinds
       of EV fleets in all kinds of cities. E.g. scooters also (not tested, but expected)

3. What we have shown (include key numbers)
   - 1. RQ
     - Integrated improves existing approaches
     - Mitigate risk & improve profits through exploitation of different market setups
   - 2. RQ
     - RL Algorithm that
     - Online learning algorithm (difference? -- real data?)
     - RL can learn to dynamically adjust bidding quantities by learning risk
       associated with bidding on each market. (What are the risks?)
     - RL Architecture matters
     - Charging infrastructure & Advancement in battery technology/charging
        technology matters

4. Compare to other studies
   - Fleet Charging
     - No uncertainty
     - Only one market
     - No sensitivity on accuracy prediction (We found very important)
   - RL
   - Other approaches (VPP, stochastic)
        cite:kahlen18_elect_vehic_virtual_power_plant_dilem
        cite:vandael15_reinf_learn_heuris_ev_fleet

5. Discuss results?
   - Gross profits small - Mention before?
   - Policy
   - Investment
   - V2G?
   - Market Setup


** Limitations
# We don't take market dynamics into account
# # NOTE: Citatation from kahlen
# The fleet controller offers bids and asks for every time interval. These offers
# contain both a quantity and a reservation price, which depends on the state of
# charge of the EV storage, as well as on the battery costs. However, the market
# may or may not accept these offers depending on the composition of the offer
# prices from the fleet owner and other market participants. The market auction
# mechanism ultimately decides when EVs will charge and discharge.

- Model:
  - Bidding Mechanism: one week ahead, always accepted
  - Policy & Regulation: EVs not allowed to provide balancing power, minimum
    bidding quantities 1MW.
  - Markets: Fleet is a price-taker, what about larger fleets? Simulate market influence
  - Deny rentals only in the same market period (More deny, less imbalance)
- RL: See cite:vazquez-canteli19_reinf_learn_deman_respon conclusion for
  limitations.
  - Training time in real time. Generalization to other cities?
** Future Research

- Model:
    - Investigate modern/current market design, that changed their bidding
      mechanisms to to better integrate renewable energy generators.
      - Daily/Day-ahead tenders with 4 hour market periods.

      Mischpreisverfahren

       i.e. daily w/ 4h slots. German "Mischpreisverfahren"
- RL: Long-delayed rewards, different reward structure, memory based
- Prediction Algorithms improvement, reference to sensitivity analysis

#+LATEX: \clearpage
#+LATEX: \appendix
* Appendix label:app-a
#+LATEX: \renewcommand\thetable{\thesection.\arabic{table}}
#+LATEX: \setcounter{table}{0}

#+LATEX: {\captionsetup[table]{aboveskip=0.5cm}
#+CAPTION: List of Trades of the EPEX Spot Intraday Continuous Market label:table-spot-market
#+ATTR_LATEX: :float sideways :align c|cccccccc :placement [hp]
|---------------------+---------+------------+----------+------------+-------------+---------+---------------+---------------|
|---------------------+---------+------------+----------+------------+-------------+---------+---------------+---------------|
| Execution time      |      ID | Unit Price | Quantity | Buyer Area | Seller Area | Product | Product Time  | Delivery Date |
|---------------------+---------+------------+----------+------------+-------------+---------+---------------+---------------|
| 2017-12-04 06:54:55 | 8031392 |      51.00 |     5500 | Amprion    | Amprion     | Quarter | 07:15 - 07:30 |    2017-12-04 |
| 2017-12-04 06:53:26 | 8031391 |      59.00 |    10000 | TenneT     | TenneT      | Quarter | 07:15 - 07:30 |    2017-12-04 |
| 2017-12-04 06:53:26 | 8031390 |      58.90 |    10000 | TenneT     | TenneT      | Quarter | 07:15 - 07:30 |    2017-12-04 |
| 2017-12-04 06:53:15 | 8031389 |      52.30 |     7000 | 50Hertz    | 50Hertz     | Quarter | 07:15 - 07:30 |    2017-12-04 |
| 2017-12-04 06:53:13 | 8031386 |      59.00 |      500 | TenneT     | TenneT      | Quarter | 07:15 - 07:30 |    2017-12-04 |
| 2017-12-04 06:53:13 | 8031387 |      51.00 |     3600 | Amprion    | Amprion     | Quarter | 07:15 - 07:30 |    2017-12-04 |
| 2017-12-04 06:53:13 | 8031388 |      52.00 |     1400 | Amprion    | Amprion     | Quarter | 07:15 - 07:30 |    2017-12-04 |
| 2017-12-04 06:53:02 | 8031385 |      58.90 |    11000 | TenneT     | TenneT      | Quarter | 07:15 - 07:30 |    2017-12-04 |
| 2017-12-04 06:52:38 | 8031380 |      60.00 |    10000 | Amprion    | Amprion     | Quarter | 07:15 - 07:30 |    2017-12-04 |
| 2017-12-04 06:52:38 | 8031381 |      57.50 |     8000 | Amprion    | Amprion     | Quarter | 07:15 - 07:30 |    2017-12-04 |
| 2017-12-04 06:52:38 | 8031382 |      58.00 |     2000 | Amprion    | Amprion     | Quarter | 07:15 - 07:30 |    2017-12-04 |
| 2017-12-04 06:52:38 | 8031383 |      58.90 |     4000 | TenneT     | TenneT      | Quarter | 07:15 - 07:30 |    2017-12-04 |
| 2017-12-04 06:52:38 | 8031384 |      60.00 |     4000 | Amprion    | Amprion     | Quarter | 07:15 - 07:30 |    2017-12-04 |
| 2017-12-04 06:52:27 | 8031379 |      52.30 |     8000 | 50Hertz    | 50Hertz     | Quarter | 07:15 - 07:30 |    2017-12-04 |
| 2017-12-04 06:51:33 | 8031378 |      66.00 |     5000 | TransnetBW | TransnetBW  | Quarter | 07:15 - 07:30 |    2017-12-04 |
| 2017-12-04 06:51:28 | 8031377 |      54.00 |     8000 | Amprion    | Amprion     | Quarter | 07:15 - 07:30 |    2017-12-04 |
| 2017-12-04 06:51:24 | 8031376 |      54.00 |     7000 | TenneT     | TenneT      | Quarter | 07:15 - 07:30 |    2017-12-04 |
| 2017-12-04 06:49:34 | 8031375 |      51.00 |     4000 | TenneT     | TenneT      | Quarter | 07:15 - 07:30 |    2017-12-04 |
| 2017-12-04 06:49:26 | 8031374 |      54.00 |     5000 | 50Hertz    | 50Hertz     | Quarter | 07:15 - 07:30 |    2017-12-04 |
| 2017-12-04 06:49:23 | 8031373 |      55.10 |     8000 | 50Hertz    | 50Hertz     | Quarter | 07:15 - 07:30 |    2017-12-04 |
|---------------------+---------+------------+----------+------------+-------------+---------+---------------+---------------|
|---------------------+---------+------------+----------+------------+-------------+---------+---------------+---------------|
#+LATEX:}
#+LATEX: \clearpage

* Appendix label:app-b
#+LATEX: \setcounter{table}{0}

#+CAPTION[Hyperparameters for the dueling network DDQN algorithm]: Hyperparameters for the dueling network DDQN algorithm. label:table-results-params
#+ATTR_LATEX: :environment longtable :align lr :placement [hp]
|--------------------------------+--------------------------------------|
|--------------------------------+--------------------------------------|
| Hyperparameter                 | Value                                |
|--------------------------------+--------------------------------------|
| Target network update interval | 10 000                               |
| Optimizer                      | Adam cite:kingma14_adam              |
| Batch size                     | 32                                   |
| Learning rate                  | 10^{-4}                              |
| Replay buffer size             | 40^6                                 |
| Warm-up steps                  | 50 000                               |
| Training steps                 | 10^6                                 |
| Discount factor $\gamma$       | 0.99                                 |
| Reward normalization           | r $\in$ [-1, 1]                      |
| Gradient clipping              | Gradient in [-1, 1] for output layer |
| Network architecture           | [TODO]                               |
|--------------------------------+--------------------------------------|
| \epsilon-greedy annealing      | Linear over 10^6 steps               |
| \epsilon-greedy minimum value  | 0.1                                  |
|--------------------------------+--------------------------------------|
|--------------------------------+--------------------------------------|

#+LATEX: \clearpage
bibliography:~/uni/ma-thesis/bibliography/references.bib
bibliographystyle:apacite
* Footnotes

[fn:1] https://pypi.org/project/simpy/

[fn:2] https://github.com/indyfree/fleetsim

[fn:3] Average prices of electricity for the industry with an annual consumption
of 500 MWh - 2000 MWh in Germany 2017 cite:bmwi.19_prices_german.

[fn:4] Rental fees according to the Car2Go pricing scheme. See
https://www.car2go.com/media/data/germany/legal-documents/de-de-pricing-information.pdf,
accessed 15^th March 2019.
