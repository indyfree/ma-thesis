* Model
We start by giving a high-level overview of the model, which proposes a solution
for utilizing a VPP portfolio of EVs to profitably provide balancing services to
the grid. A control mechanism procures energy from multiple electricity markets,
allocates available EVs to the VPP, and intelligently dispatches EVs to charge
the acquired amount of energy. The model employs a RL agent that learns an
optimal bidding strategy by interacting with the electricity markets and reacts
to changing rental demands of the EV fleet.

# NOTE: Section: Problem Description
We formulate the research problem as a /controlled EV charging/ problem. The EV
fleet operator represents the /controller/, which aims to charge the fleet at
minimal costs. Therefore, it first predicts the amount of energy it can charge
in a given /market period/ $h$. The length of the market period $\Delta h$ and
the market closing time depend on the considered electricity market. Second, the
controller places bids on one or multiple markets to procure the predicted
amount of energy. Lastly, at electricity delivery time, the controller
communicates with the EV fleet to control the charging in real-time. EV /control
periods/ $t$ are typically shorter than market periods. In the empirical case
that we consider, the market periods are 15 minutes long, while the EV control
periods last 5 minutes. Nonetheless, the presented approach generalizes to other
period lengths. During each control period, the controller has to take decisions
which individual EVs it should dispatch to charge the procured amount of
electricity. In times of unforeseen rental demand, this decision implies trading
off commitments to the markets with compromising customer mobility by refusing
customer rentals.

The rest of the chapter is structured as follows: The information and market
assumptions are listed first. Next, we lay out the developed control mechanism,
which we also illustrate with a comprehensive example. Finally, the RL approach
is described, where we give details about the defined MDP and the employed
learning algorithm. The required notation for the mathematical formulations are
displayed in Table ref:table-notation.

#+CAPTION[Table of Notation]: Table of Notation label:table-notation
#+ATTR_LATEX: :environment longtable :align p{0.11\linewidth}|p{0.75\linewidth}|c :placement [hp]
|---------------------------+-----------------------------------------------------------------------------------------------------------+---------|
|---------------------------+-----------------------------------------------------------------------------------------------------------+---------|
| Symbol                    | Description                                                                                               | Unit    |
|---------------------------+-----------------------------------------------------------------------------------------------------------+---------|
| $t$                       | Control period                                                                                            | -       |
| $h$                       | Market period                                                                                             | -       |
| $T$                       | Number of control periods in a market period                                                              | -       |
| $H$                       | Number of market periods in day                                                                           | -       |
| $N_h$                     | Total number of market periods                                                                            | -       |
| $\Delta t$                | Length of control period                                                                                  | hours   |
| $\Delta h$                | Length of market period                                                                                   | hours   |
|---------------------------+-----------------------------------------------------------------------------------------------------------+---------|
| $\Pb{h}$                  | Amount of balancing power offered on the balancing market                                                 | kW      |
| $\ccp{h}$                 | Critical capacity price in market period $h$                                                              | $\emw$  |
| $\cep{h}$                 | Critical energy price in market period $h$                                                                | $\emwh$ |
| $\Eb{h}$                  | Amount of energy charged from balancing market in market period h                                         | MWh     |
| $\Pi{h}$                  | Amount of power offered for the unit on the intraday market                                               | kW      |
| $\cup{h}$                 | Critical unit price in market period $h$                                                                  | $\emwh$ |
| $\Ei{h}$                  | Amount of energy charged from the intraday market in market period h                                      | MWh     |
|---------------------------+-----------------------------------------------------------------------------------------------------------+---------|
| $\fP{t}$                  | Amount of available fleet charging power in control period $t$                                            | kW      |
| $\fPhat{t}$               | Predicted amount of available fleet charging power in control period $t$                                  | kW      |
|---------------------------+-----------------------------------------------------------------------------------------------------------+---------|
| $\Cb{h}(P)$               | Cost function for procuring electricity from the balancing market dependent on the offered charging power | $\eur$  |
| $\Ci{h}(P)$               | Cost function for procuring electricity from the intraday market dependent on the offered charging power  | $\eur$  |
| $\oc$                     | Opportunity costs of lost rental of EV $i$ in control period $t$                                          | $\eur$  |
| $\beta_h$                 | Imbalance costs in market period $h$                                                                      | $\eur$  |
|---------------------------+-----------------------------------------------------------------------------------------------------------+---------|
| $\lb{h}$                  | Balancing market risk factor                                                                              | $[0,1]$ |
| $\li{h}$                  | Intraday market risk factor                                                                               | $[0,1]$ |
| $\theta_{\lambda}$        | Set of risk factors for all market periods $h\!\in\!\{1,\hdots,N_h\}$                                     | -       |
| $\Cf{}(\theta_{\lambda})$ | Cost function for the fleets total costs over all market periods dependet on the chosen risk factors      | $\eur$  |
| $\Cf{h}$                  | Total accumulated fleet costs until market period $h$                                                     | $\eur$  |
|---------------------------+-----------------------------------------------------------------------------------------------------------+---------|
| $i$                       | Electric vehicle                                                                                          | -       |
| $\F$                      | Set of all EVs in the fleet                                                                               | -       |
| $\c$                      | Dummy variable indicating if EV $i$ is connected to a charging station                                    | 0/1     |
| $\omega_{i}$              | Amount of electricity stored in the battery of EV $i$                                                     | $\kwh$  |
| $\Omega$                  | Maximum battery capacity of the considered EV model                                                       | $\kwh$  |
| $\delta$                  | Charging power of the considered EV model                                                                 | $\kw$   |
| $p^{ind}$                 | Industry electricity tariff                                                                               | $\ekwh$ |
|---------------------------+-----------------------------------------------------------------------------------------------------------+---------|

** Assumptions label:sec-model-assumptions

In order to evaluate and operationalize our model, the following assumptions
about the available information and the electricity market mechanisms were
taken:

- I-1. :: Mobility demand information

     The controller is able to forecast the mobility demand of the EV fleet with
     different time-horizons based on historical data. More specifically, the
     controller can predict the amount of plugged-in EVs and consequently the
     available charging power $P^{fleet}_t$ of the fleet at control period $t$.
     The prediction accuracy is increasing with shorter time horizons, from
     uncertain predictions one week ahead to very accurate predictions 30
     minutes ahead. Past research presented such mobility demand forecast
     algorithms in the context of free-float carsharing
     cite:kahlen18_elect_vehic_virtual_power_plant_dilem,kahlen17_fleet,wagner16_in_free_float.

- I-2. :: Critical electricity price information

     The controller is able to forecast electricity prices of spot and balancing
     markets based on historical data. More specifically, the controller can
     estimate the critical prices $\ccp{h}$, $\cep{h}$, and $\cup{h}$ for each
     market period with perfect accuracy (see Chapter ref:sec-data-balancing and
     Chapter ref:sec-data-intraday for the critical price definitions).
     Electricity price forecasting is an extensively studied research area with
     well-advanced prediction algorithms
     cite:weron14_elect_price_forec,avci18_manag_elect_price_model_risk.
We are confident that making the above information assumptions is feasible.
Assuming available forecasting information is common practice in the VPP and EV
fleet charging literature, see for example
textcite:vandael15_reinf_learn_heuris_ev_fleet,
textcite:mashhour11_biddin_strat_virtual_power_plant_1,
textcite:tomic07_using_fleet_elect_drive_vehic_grid_suppor, and
textcite:pandzic13_offer_model_virtual_power_plant.

- M-1. :: Balancing market mechanism

     We assume that the controller is able to submit bids of any quantity for
     single 15-minute market periods 7 days ahead. Since the critical capacity
     and energy prices are available by I-2, the controller submits bids in the
     form $(\Pb{},\ccp{},\cep{})$. By construction of the critical prices, the
     submitted bid will always get accepted by the TSOs and the offered
     balancing capacity fully activated.

- M-2. :: Intraday market mechanism

     The controller submits bids in the form $(\Pi{},\cup{})$ to the intraday
     market 30 minutes ahead. We assume that the order to buy will always get
     matched until the minimal lead time of the trade (e.g., 5 minutes on the
     EPEX Spot Intraday Continuous). In reality, this is not always the case
     since trades are executed immediately and it is not guaranteed that a
     matching order to sell is submitted between the bidding time and the
     minimal lead time.
In summary, we are assuming that the controller always submits the optimal bid
at the right time. In other words, every bid leads to the successful procurement
of the desired amount of electricity. This assumption provides an upper bound
for the fleet profits from trading EV battery storage on the electricity
markets. However, this upper bound is only influenced by the accuracy of the
electricity price forecasting algorithm, which we take as given by I-2.
Incorporating the electricity forecasting aspect into our work, would well
exceed the scope of this thesis. Furthermore, we assume that the controller is a
price-taker. Due to the limited size of its bids, it is lacking the market share
to influence prices on the markets. Similar assumptions have been made by
textcite:brandt17_evaluat_busin_model_vehic_grid_integ and
textcite:vandael15_reinf_learn_heuris_ev_fleet.

** Control Mechanism label:sec-model-mechanism
# TODO: Charging power or fleet capacity??

# NOTE: Embed/Mention DSS? What about Smart Charging?
The control mechanism constitutes the core of this research. It can be seen as a
decision support system that can be deployed at an EV fleet operator to centrally
control the charging of its fleet. Figure ref:fig-control-mechanism depicts the
control mechanism, which is divided into three distinct phases:

# TODO:Other figure, with phase timeline?
The first phase, /Bidding Phase I/, takes place just before the closing time of
the balancing market, once every week (e.g., Wednesdays at 3pm at the GCRM). In
this phase, the controller can place bids for every market period $h$ of the
following week on the balancing market. The second phase, /Bidding Phase II/,
takes places in every market period of $\Delta{h}\!=\!15$ minutes. At this
point, the controller has the opportunity to place bids to the intraday market
for the market period 30 minutes ahead. The third phase, /Dispatch Phase/, takes
places in every control period of $\Delta{t}\!=\!5$ minutes. In this phase the
controller has to dispatch available EVs to charge the procured electricity from
the markets. The phase involves allocating individual EVs to the VPP and
potentially refusing customer rentals to assure that all market commitments can
be fulfilled.

The following chapters will highlight the important parts of the three phases
and provide detailed explanation and mathematical formulations.

#+CAPTION[Control Mechanism]: Control Mechanism label:fig-control-mechanism
#+ATTR_LATEX: :width 1.05\linewidth :placement [p]
[[../fig/control-mechanism.png]]

# TODO: Figure:
# - What about regular charging?
# - What about not enough SoC for trip?
# - Allocate costs/profits from charging/trip?

# NOTE: Add timeline week ahead. Place bids week ahead for all following
# market periods m.
# TODO: Mention Market closure times in Background section?
# NOTE: Intro two decision phases

# TODO: Bids always get accepted & activated on Balancing market, mention prices?
# TODO: Generalilzation, lead time, trades always get matched on intraday, prices?
# NOTE: Mention here that we assume to be accepted/activated for balancing market?
# NOTE: Mention here that we bid with the critical prices?

*** Fleet Charging Power Prediction

# for all market periods of the next week.
In a first step, the controller has to predict the available fleet charging
power for the market period of interest (see\circled{A} in Figure
ref:fig-control-mechanism). The actual available fleet charging power $\fP{t}$
in a control period $t$ is given by the number of EVs that are connected to a
charging station, with enough free battery capacity to charge the next control
period $t\!+\!1$.

When the controller procures electricity from the markets, the fleet has to
charge with the committed charging power during all control periods of the
market period $h$, otherwise imbalance costs occur. To minimize the risk of not
being able to charge the committed amount of energy during the whole market
period, the predicted fleet charging power in a market period is defined as the
minimal predicted fleet charging power of all control periods in that market
period:
\begin{equation}
    \fPhat{h} \defeq \min_{n \in \{1, .., T\}} \fPhat{t + n} \text{ ,}
\end{equation}
where $h$ is the market period of interest, $t$ its first control period and $T$
the number of control periods in a market period.



*** Market Decision
In a second step, the controller has to decide from which market it should
procure the desired amount of energy (see\circled{B} in Figure
ref:fig-control-mechanism). Therefore, it compares the costs for charging
electricity from the balancing market with the costs for charging from the
intraday market. The cost function for procuring electricity from the balancing
market is defined as follows:
\begin{equation} \label{eq-cost-balancing}
\begin{split}
    \Cb{h}(P) &\defeq -(P\!\times\!10^{-3} \times \ccp{h}) + (\Eb{h} \times \cep{h}) \\
    &= -(P\!\times\!10^{-3} \times \ccp{h}) + (P \frac{\Delta h}{10^{3}} \times \cep{h}) \text{ ,}
\end{split}
\end{equation}
where $P$ (kW) is the amount of offered balancing power. The first term of the
equation corresponds to the compensation the controller retrieves for keeping
the balancing capacity available, while the second term corresponds to the costs
for charging the activated balancing energy $\Eb{h}$ (MWh). Energy is power over
time, hence $\Eb{h}$ can be substituted with $P$ times the market periods length
$\Delta{h}$, divided by the unit conversion term from kW to MW. Note that the
critical energy price $\cep{}\!\in\!\Re$, can also take negative values,
resulting in profits for the fleet, while the critical capacity price
$\ccp{}\!\in\! \Re^+_0$ is never negative and therefore never results in
costs for the fleet.

The cost function for charging from the intraday market is defined similarly to
\eqref{eq-cost-balancing}:
\begin{equation}
\begin{split}
    \Ci{h}(P) &\defeq \Ei{h} \times \cup{h} \\
    &= P \frac{\Delta h}{10^{3}}\times \cup{h}
\end{split}
\end{equation}
Again, depending on the market situation, $\cup{}\!\in\!\Re$ can either be
negative or positive, resulting in costs or profits for the fleet. Contrarily to
the balancing market, on the intraday market the fleet does not get compensated
for keeping the charging power available; only the charged energy affects the
costs. If the costs for charging from the balancing market 7 days ahead
$\Cb{h+(7\!\times\!H)}(\fPhat{h+(7\!\times\!H)})$ are higher than the costs of
charging from the intraday market at the same market period $\Ci{h +
(7\!\times\!H)}(\fPhat{h+(7\!\times\!H)})$, the controller does not procure
electricity from the balancing market.

*** Determining the Bidding Quantity
In a third step, the controller has to take a decision on the amount of energy
it should procure from the markets (see\circled{C} in Figure
ref:fig-control-mechanism). Determining the bidding quantity is the core
challenge of the controlled charging problem. The bidding quantity determines
the profits that can be made by charging at a cheaper market price than the
flat industry tariff. On one hand, the controller aims to maximize its profits
by procuring as much electricity as possible from the markets. On the other
hand, it needs to balance the risk of (a) procuring more energy that it can
maximally charge and (b) not procuring enough energy from the market to
sufficiently charge the fleet.

In case (a), the fleet is facing costs of compromising customer mobility, or
worse, high imbalance penalties from the markets. Renting out EVs is
considerably more profitable than using their batteries as a VPP. Refusing
customer rentals, in order to fulfill market commitments, induces opportunity
costs of lost rentals $\rho$ on the fleet. Imbalance costs $\beta$ occur, when
the fleet can not charge the committed amount energy at all, even with refusing
rentals. In case (b), the fleet also faces opportunity costs of lost rentals
when individual EVs do not have enough SoC for planned trips of arriving
customers.

The controller faces additional risks by bidding one week ahead on the balancing
market, in contrast to only 30 minutes ahead on the intraday market: Predictions
of available charging power are more uncertain with the larger time horizon. To
account for all mentioned risks, we introduce a /risk factor/ $\lambda \in
\Re_{0 \leq \lambda \leq 1}$, where $\lambda\!=\!0$ indicates no risk, and
$\lambda\!=\!1$ indicates a high risk. The controller determines the bidding
quantity $\Pb{h}$ by discounting the predicted available fleet charging power
$\fPhat{h}$ with the possible risk $\lambda_{h}$ of imbalance or opportunity
costs:
\begin{equation} \label{eq-model-pb}
  \Pb{h} \defeq
  \begin{cases}
    0, & \text{if}\ \Cb{h}(\fPhat{h}) \geq \Eb{h}10^3 \times p^{ind}\\
    0, & \text{if}\ \Cb{h}(\fPhat{h}) \geq \Ci{h}(\fPhat{h})\\
    \fPhat{h} \times (1\!-\!\lb{h}), & \text{otherwise}
  \end{cases}
\end{equation}
where $h$ is the market period of interest one week ahead. If the controller can
buy electricity at the intraday market at a lower price, it does not place a bid
at the balancing market. If the controller can charge cheaper at the regular
industry tariff $p^{ind}$, it does not place a bid either. In all other cases, the
controller submits $\Pb{h}$ to the market.

The bidding quantity for the intraday market $\Pi{h}$ depends on the previously
committed charging power $\Pb{h}$ and the newly predicted charging power
$\fPhat{h}$:
\begin{equation} \label{eq-model-pi}
  \Pi{h} \defeq
  \begin{cases}
    0, & \text{if}\ \Ci{h}(\fPhat{h}\!-\!\Pb{h}) \geq \Ei{h}10^3 \times p^{ind}\\
    (\fPhat{h}\!-\!\Pb{h}) \times (1\!-\!\li{h}), & \text{otherwise}
  \end{cases}
\end{equation}
where $h$ is the market period of interest 30 minutes ahead. Note that any
amount of electricity that the controller procured from the balancing market
$\Pb{h}$, does not need to be bought from intraday market for the same market
period. Since the predicted charging power $\fPhat{h}$ is expected to be more
accurate 30 minutes ahead than one week ahead, the controller is able to correct
bidding errors it made in the first decision phase, and optimally charge the
whole EV fleet.

*** Dispatching Electronic Vehicle Charging
# TODO: Explain $\Omega\!-\!\omega_{i}$
In the last step, at electricity delivery time, the EVs have to be assigned to
the VPP and be /dispatched/ to charge (see\circled{D} in Figure
ref:fig-control-mechanism). Therefore the controller needs to detect how many
EVs are eligible to be used as VPP in the control period $t$. An EV $i$ is
eligible if (a) it is connected to a charging station ($\c$ = 1), and (b) it has
enough free battery storage available ($\Omega\!-\!\omega_{i}$) to charge the
next control period. Hence, the VPP is defined as:
\begin{equation}
    V\!P\!P \defeq \{i\in\F \;|\; \c = 1 \vee \Omega\!-\!\omega_{i}\!\geq\!\gamma\Delta{t}\} \text{ ,}
\end{equation}
where $\gamma\Delta{t}$ (kWh) denotes the amount of energy that can be charged
with the charging speed of $\gamma$ (kW) in control period $t$. $\gamma$ is
limited by either the EVs build-in charger, or the charging power of the
connected charging station. In this model we assume $\gamma$ is equal for all
considered EVs and charging stations. /Example:/ Assuming a charging power of
$\gamma\!=\!3.3\kw$, an EV battery capacity of $\Omega\!=\!17.6\kwh$, and
control periods of 5 minutes, the amount of energy charged in one control period
is $3.3\kw\!\times\frac{5}{60}\text{h}\!=\!0.275\kwh$. Hence, the maximal
battery capacity to be eligible for VPP use is $17.6-\!0.275\!=\!17.325\kwh$.

#+BEGIN_SRC python :exports none
return(3.3 * (5/60))
#+END_SRC

#+RESULTS:
: 0.27499999999999997

#+BEGIN_SRC python :exports none
return(17.6 - 0.275)
#+END_SRC

#+RESULTS:
: 17.325000000000003

# TODO: How do we known $\oc$?
Remember that the fleet has to provide the total committed charging power
$\Pb{h}\!+\!\Pi{h}$ across all control periods $t$ of the market period $h$,
independent of which individual EVs are actually charging the electricity. This
fact allows the controller to dynamically dispatch EVs every control period and
react to unforeseen rental demand. If a customer wants to rent out an EV that is
assigned to the VPP, the controller only has to refuse the rental, if no other
EV is available to charge instead. When no replacement EV is available, the
controller has to account for lost rental profits $\oc$. If the VPPs total
amount of available charging power $\vpp{t}\!\times\!\gamma$ is not sufficient to
provide the total market commitments $\Pb{h}\!+\!\Pi{h}$, the fleet gets charged
imbalance costs $\beta_{h}$. Otherwise all the committed energy can be charged
by the VPP.

*** Evaluating the Bidding Risk
The controllers main goal is to choose the risk factors $\lb{h}$, $\li{h}$
for every market period $h$, that minimize the cost of charging, while avoiding
the risks of lost rental profits $\oc$ or imbalance costs $\beta_h$. The total
fleet costs are defined as follows:
\begin{equation} \label{eq-model-fleetcosts}
    \Cf{}(\theta_{\lambda}) \defeq \sum^{N_h}_h
    \bigg[ \Cb{h}(\Pb{h}) + \Ci{h}(\Pi{h}) + \beta_{h}
    + \sum_t^{T} \sum_i^{|\F|} \oc \bigg] \text{ ,}
\end{equation}
where $\theta_{\lambda}\!\in\!\Re_{0 \leq \lambda \leq 1}^{2 \times N_h}$ is the
matrix of the risk factors $\lb{h}$, $\li{h}$ for all considered market periods
$N_h$. $\F$ denotes the set of all EVs $i$ in the fleet and $|\F|$ the fleet
size. The costs for charging $\Cb{h}(\Pb{h})$, $\Ci{h}(\Pi{h})$ are clearly
dependent on the chosen risk factors $\lb{h}$, $\li{h}$ (see
\eqref{eq-model-pb} and \eqref{eq-model-pi}). In summary, the problem can be
formulated as minimizing the total costs of the fleet, by choosing the optimal
set of risk factors $\theta_{\lambda}$:
\begin{equation}
\begin{aligned}
    & \underset{\theta_{\lambda}}{\text{minimize}}
    && \Cf{}(\theta_{\lambda}) \\
    & \text{subject to}
    && 0 \leq \lb{h} \leq 1, \; \forall \lb{h} \in \theta_{\lambda}\\
    &&& 0 \leq \li{h} \leq 1, \; \forall \li{h} \in \theta_{\lambda}\\
\end{aligned}
\end{equation}
Solving this optimization problem with common methods like stochastic
programming is a difficult task, assuming that complete information of available
charging power and future electricity market prices is not always available.
Since one goal of this research is to develop a model that can be applied to
previously unknown settings and learn from uncertain environments, as mobility
and electricity markets, we chose to solve the problem with a RL approach that
is explained in detail in Chapter ref:sec-model-rl.

*** Example
At 3pm on the 9^{th} of August 2017, the controller enters the first bidding
phase for the market period $h$ = /16.08.2017 15:00-15:15/. It predicts that at
that point in time 250 EVs are connected to a charging station, resulting in
900kW available fleet charging power ($\fPhat{h}\!=\!900\kw$), given the
charging power of 3.6kW per EV. Assuming the available critical prices are
$\ccp{h}\!=\!5\emw$, $\cep{h}\!=\!-10\emwh$, and $\cup{h}\!=\!10\emwh$ in that
market period, the controller now evaluates the cheapest charging option. The
flat industry electricity tariff is assumed to be $p^{ind}\!=\!0.15\ekwh$. The
costs for charging with the maximal predicted amount of available power
$\fPhat{h}$ from the balancing market ($\Cb{h}(900\kw)\!=\!-6.25\eur$) are less
than charging from the intraday market ($\Ci{h}(900\kw)\!=\!2.25\eur$) or
charging at the industry tariff
($900\kw\!\times\!0.25\text{h}\!\times\!0.15\ekwh\!=\!33.75\eur$). In this
example, the fleet operator will even get compensated for charging its fleet, by
choosing the balancing market.

In the next step, the controller has to submit bids to the balancing market. The
RL agent determined that the risk of bidding on the balancing market is
$\lb{h}\!=\!0.3$. Consequently, the controller sets the bidding quantity to
$\Pb{h}\!=\!\fPhat{h}\!\times\!(1\!-\!\lb{h})\!=\!900\kw\!\times0.7\!=\!630\kw$
and submits a bid to the market and updates its account with
$\Cb{h}(630\kw)\!=\!-4.725\eur$.

One week later, 30 minutes before electricity delivery time, the controller
enters the second bidding phase. Due to the short time horizon, it predicts with
high accuracy that only $\fPhat{h}\!=\!810\kw$ is available for the same market
period /16.08.2017-15:00/. By trading at the intraday market, the controller can
now charge the remaining available EVs with a low risk of procuring more energy
than it can maximally charge. At this point in time, the RL agent determines a
remaining risk of $\li{h}\!=\!0.05$, and sets the bidding quantity to
$\Pi{h}\!=\!(810\kw\!-\!630\kw)\!\times\!(1\!-\!0.05)\!=\!171\kw$. The
controller procures 171kW from the intraday market and updates its account with
$\Ci{h}(171\kw)\!=\!0.4275\eur$.

At electricity delivery time, the 16^{th} of August 2017 at 3:00pm, the
controller detects 255 available EVs; EVs which are connected to a charging
station and have enough battery capacity left to be charged in the next control
period. It assigns 223 EVs to provide the total committed 801kW charging power
for the market period time $\Delta h$ of 15 minutes. During that time, three
customers want to rent out EVs that are allocated to the VPP. The first two
rentals are accepted because two other EVs are available to charge instead. The
third rental has be to refused, since no EV is remaining as substitution. The
controller has to account for the opportunity costs of the lost rental $\oc$.

#+BEGIN_SRC python :exports none :var h=15
def bal_cost(p_c, p_e, P):
    return(-(p_c * P * 0.001) + (p_e * P * (h/60) * 0.001))

return(bal_cost(5, -10, 630))
#+END_SRC

#+RESULTS:
: -4.725


#+BEGIN_SRC python :exports none :var h=15
def intraday_cost(p_u, P):
    return((p_u * P * (h/60) * 0.001))

return(intraday_cost(10, 171))
#+END_SRC

#+RESULTS:
: 0.4275

#+BEGIN_SRC python :exports none :var h=15
def industry_costs(p_i, P):
    return((p_i * P * (h/60)) / 100)

return(industry_costs(15, 900))
#+END_SRC

#+RESULTS:
: 33.75

#+BEGIN_SRC python :exports none
def pi(l):
    kw = (810 - 630) * (1 - l)
    return kw

return(pi(0.05))
#+END_SRC

#+RESULTS:
: 171.0

#+BEGIN_SRC python :exports none :var h=15
kw = 630  + 171
return(kw)
#+END_SRC

#+RESULTS:
: 801


# Assume:
# - Bid maximum capacity price?
# - What about balancing capacity prices? Future work?

# Other assumptions:
# 2) EV fleet charges at a fixed industry tariff otherwise

# - Fleet capacity prediction made on non-simulated real-world SoC (w/o smart charging)?
# - Fleet capacity predictions with simulated data?
#   - Provide difference levels & descriptive statistics?


** Reinforcement Learning Approach label:sec-model-rl
In the following chapter the developed RL approach is outlined. First, we define
the charging problem as a MDP, and second, the learning algorithm is explained.
Remember that the goal of the controlled charging problem is to choose a set of
risk factors $\theta_{\lambda}$ that minimize the fleets total costs across all
market periods. The controller is able to influence the costs, by setting the
risk factors $\lb{}$, $\li{}$ each market period $h$. The risk factors influence
the bidding quantities $\Pb{h}$, $\Pi{h}$ that the controller submits to the
balancing and intraday market, which in the end determine the fleet costs. The
RL agent decides on the risk factors (i.e., takes an action) based on the
observed state $\S$ every time step $h$ (usually denoted as $t$ in the RL
literature). The optimal set of risk factors is learned by the RL agent through
estimating a policy $\pi(a|s)$ that maps every state $s\in\S$ to an action
$a\in\A$.
*** Markov Decision Process Definition

MDPs are defined by the state space $\S$, the action space $\A$, a set of reward
signals $\R$ and the state-transition probabilities $p(s'|a,s)$. When
$p(s'|a,s)$ is unknown, as it is in our case, it is possible to use a
model-free approach (see Chapter ref:sec-td-learning). The state space
compromises the observed information the agent uses to decide on the action it
is going to take. We observed the following factors that are associated with the
bidding risk:
1) The bidding period's time of the day

   In times of volatile customer rental demand (e.g., during rush hour), the
   uncertainty on the guaranteed amount of available EVs increases. Bidding for
   these periods involves a higher risk of not being able to fulfill market
   commitments.
2) The current and estimated future size of the VPP

   Large VPPs benefit from the /risk-pooling/ effect cite:kahlen17_fleet.
   Intuitively that means, larger VPPs are exposed to smaller risks: They have
   an increased probability that "lost" charging power, due to unforeseen
   rentals, can be substituted by the EVs of the VPP.

# NOTE: Features as state space?
Since forecasts of available charging power are already available, we define the
predicted VPP size $\vpphat{h}$ as the as the necessary amount of EVs to provide
the predicted charging power $\fPhat{}$ in time period $h$:
\begin{equation}
    \vpphat{h} \defeq \left\lceil\frac{\fPhat{h}}{\gamma}\right\rceil \text{ ,}
\end{equation}
where $\gamma$ is the charging power per EV. /Example:/ When the controller
predicted 910kW available charging power, the estimated future
size of the VPP to charge with the predicted power is  =ceil(= $\!910\kw /
3.6\kw\!$ =)= = 253.


#+BEGIN_SRC python :exports none
import math
def vpp(p, gamma):
    return(math.ceil(p /  gamma))

return(vpp(910, 3.6))
#+END_SRC

#+RESULTS:
: 253


The state space is then defined as
the set of all valid values of the elements of the following tuple:
\begin{equation}
    \S \defeq \left\langle t(h), \vpp{h}, \vpphat{h+2}, \vpphat{h+(7\!\times\!H)}\right\rangle \text{ ,}
\end{equation}
where:
- $t(h)$ is the current daytime in hours, with discrete values in the range
  $\big[0,\;23\big] \in \Ne$.
- $|VPP|_t$ is the current VPP size, with discrete values in the range
  $\big[0,\;|\F|\big] \in \Ne$.
- $\vpphat{h+2}$ is the predicted VPP size 30 minutes ahead, with discrete values in the range
  $\big[0,\;|\F|\big] \in \Ne$.
- $\vpphat{h+(7\!\times\!H)}$ is the predicted VPP size 7 days ahead, with discrete
  values in the range $\big[0,\;|\F|\big] \in \Ne$.
The state space encompasses $|\F|^3\!\times\!24$ states. Assuming a fleet size
$|\F|$ of 500 EVs, the state space consists of $3\!\times\!10^9$ different states.

#+BEGIN_SRC python :exports none
import math
return(24 * math.pow(500,3))
#+END_SRC

#+RESULTS:
: 3000000000.0

The agent takes actions by determining the risk that is associated with bidding
on the electricity markets at each market period $h$. Hence, the action space is
constituted by all combinations of valid values of the risk factors
$\lb{},\li{}$:
\begin{equation}
    \A \defeq \left\{\lb{},\li{} \in \Re_{0 \leq \lambda \leq 1} \right\} \text{ ,}
\end{equation}
where:
- $\lb{}$ is the risk factor for bidding on the balancing market 7 days ahead,
  with discrete values in the range $\big[0,1\big]$ in 0.05 increments.
- $\li{}$ is the risk factor for bidding on the intraday market 30 minutes
  ahead, with discrete values in the range $\big[0,1\big]$ in 0.05 increments.
The action space encompasses $20^2 = 400$ actions. The state space and action
space were consciously discretized to achieve faster learning rates. Convergence
in continuous spaces is theoretically achievable, but computationally more
complex cite:sutton18_reinf. In order to facilitate faster learning in
real-world settings, where long training periods are not desirable, we chose to
not pursue this direction.

# NOTE: Reward structure, possibilities
The reward signal is naturally defined as the fleet costs that occurred in the last
time step. When accumulating the rewards for all time steps, we arrive at the total
fleet costs, which we aim to minimize:
\begin{equation}
    R_{h+1} = \Cf{h} - \Cf{h-1} \text{ ,}
\end{equation}
where $\Cf{h}$ are the total accumulated fleet costs until the market period
$h$. For a complete formulation of the cost function see
\eqref{eq-model-fleetcosts}. The agent's actions directly determine the occurred
costs or profits, and are presented to the agent in form of a positive or
negative reward signal. The particular challenge in the proposed RL problem is
the significantly /delayed reward/. Choosing a risk factor in time step $h$
determines the reward up to 672 time steps later (7 days, with 15-minute time
steps), when the electricity from the balancing market has to be charged.

#+BEGIN_SRC python :exports none
return(7 * 24 * 4)
#+END_SRC

#+RESULTS:
: 672
*** Learning Algorithm
This research proposes to solve the presented RL problem,  with the Double Deep
Q-Network algorithm (DDQN), developed by
textcite:hasselt16_deep_reinf_learn_doubl_q_learn. DDQN is a state-of-the-art,
model-free RL approach that uses a deep neural network as function approximator
to estimate optimal Q-values (see Chapter ref:sec-rl-fa for a explanation of
function approximation methods). It combines the revolutionary Deep Q-Network
(DQN), originally proposed by
textcite:mnih15_human_level_contr_throug_deep_reinf_learn with Double Q-Learning
cite:hasselt10_doubl_q. In Double Q-Learning, experiences are randomly selected
to update two different value functions to select and evaluate actions (in
contrast to just one function for both tasks). DDQN has shown to reduce
overoptimistic action-value estimates of the DQN algorithm, resulting in more
stable and reliable learning results
cite:hasselt16_deep_reinf_learn_doubl_q_learn. Combined with the /dueling
network/ architecture, proposed by
textcite:wang15_duelin_networ_archit_deep_reinf_learn, this approach outperforms
existing deep RL methods. Dueling networks lead to faster convergence rates in
control problems with large action spaces than traditional single stream
approaches. This property is especially beneficial for our proposed RL problem,
as the defined action space (400 possible actions) is comparably large in
comparison to classical control problems. In Figure ref:fig-model-dueling, the
conventional single stream approach (top) versus the dueling architecture
(bottom) is depicted. The dueling architecture consists of a neural network of
any shape with two streams that separately estimate the state-value and the
action advantages. These estimates are later combined into Q-values (see Figure
ref:fig-model-dueling, green layer):
\begin{equation}
    Q(s,a) = V(s) + \left(A(s,a) - \frac{1}{|\A|} \sum_{a'} A(s,a')\right) \text{ ,}
\end{equation}
where $V$ and $A$ are estimates of the value function and action advantages
respectively, represented by the two different streams in the network. By
subtracting the mean action advantages (last term), identifiability ($V$ and $A$
can be recovered, given $Q$) and stability of the optimization is ensured. The
separated streams allow to learn which states are valuable without having to
learn each state-action interaction individually. Like this, a general
state-value is learned that can be shared across many different actions, leading
to faster convergence cite:wang15_duelin_networ_archit_deep_reinf_learn.

#+CAPTION[Dueling Network Architecture]: The dueling network architecture cite:wang15_duelin_networ_archit_deep_reinf_learn label:fig-model-dueling
#+ATTR_LATEX: :width 0.95\linewidth :placement [h]
[[../fig/ddqn.pdf]]


Our agent uses the dueling DDQN algorithm with a standard neural network
architecture, similar to the one depicted in Figure ref:fig-ann. It consist of
four input nodes (number of states), three fully-connected hidden layers with
ReLU activation functions, and a linear output layer with two nodes (number of
actions). Further, an \epsilon-greedy policy with a linear decreasing
exploration rate was used. The RL agent was implemented with the neural networks
API Keras[fn:1][fn:2], which is a high-level abstraction layer of TensorFlow.
TensorFlow is the de-facto standard for robust and scalable machine learning in
industry and research cite:abadi16_tensor. Further, we used the shared research
environment Google Colaboratory to train and evaluate the agent. It offers
free access to computing resources that are optimized for training machine
learning models.[fn:3]

* Footnotes

[fn:1] https://www.keras.io

[fn:2] https://github.com/keras-rl/keras-rl

[fn:3] Google Colaboratory (https://colab.research.google.com) provides a NVIDIA Tesla K80 GPU, with
2880 $\times$ 2 CUDA cores and 12GB GDDR5 VRAM. Additionally, the environment is
equipped with a Intel(R) Xeon(R) CPU @ 2.30GHz (1 core, 2 threads), and 12GB
available memory. Google Colaboratory can be used up to 12 hours of consecutive
training time.

#+LATEX: \clearpage
