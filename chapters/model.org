* Model
We start by giving a high-level overview of the model, which proposes a solution
for utilizing a VPP portfolio of EVs to profitably provide balancing services to
the grid. A control mechanism procures energy from multiple electricity markets,
allocates available EVs to the VPP, and intelligently dispatches EVs to charge
the acquired amount of energy. The model employs an RL agent that learns an
optimal bidding strategy by interacting with the electricity markets and reacts
to changing rental demands of the EV fleet.

# NOTE: Section: Problem Description
We formulate the research problem as a /controlled EV charging/ problem. The EV
fleet operator represents the /controller/, which aims to charge the fleet at
minimal costs. Therefore, it first predicts the amount of energy it can charge
in a given /market period/ $h$. The length of the market period $\Delta h$ and
the market closing time depend on the considered electricity market. Second, the
controller places bids on one or multiple markets to procure the predicted
amount of energy. Lastly, at electricity delivery time, the controller
communicates with the EV fleet to control the charging in real-time. EV /control
periods/ $t$ are typically shorter than market periods. In the empirical case
that we consider, the market periods are 15 minutes long, while the EV control
periods last 5 minutes. Nonetheless, the presented approach generalizes to other
period lengths. During each control period, the controller has to take decisions
which individual EVs it should dispatch to charge the procured amount of
electricity. In times of unforeseen rental demand, this decision implies trading
off commitments to the markets with compromising customer mobility by refusing
customer rentals.

The rest of the chapter is structured as follows: The information and market
assumptions are listed first. Next, we lay out the developed control mechanism,
which we also illustrate with a comprehensive example. Finally, the RL approach
is described, where we give details about the defined MDP and the employed
learning algorithm. The required notation for the mathematical formulations are
displayed in Table ref:table-notation.

#+CAPTION[Table of Notation]: Table of Notation label:table-notation
#+ATTR_LATEX: :environment longtable :align p{0.11\linewidth}|p{0.75\linewidth}|c :placement [hp]
|---------------------------+-----------------------------------------------------------------------------------------------------------+---------|
|---------------------------+-----------------------------------------------------------------------------------------------------------+---------|
| Symbol                    | Description                                                                                               | Unit    |
|---------------------------+-----------------------------------------------------------------------------------------------------------+---------|
| $t$                       | Control period                                                                                            | -       |
| $h$                       | Market period                                                                                             | -       |
| $T$                       | Number of control periods in a market period                                                              | -       |
| $H$                       | Number of market periods in day                                                                           | -       |
| $N_h$                     | Total number of market periods                                                                            | -       |
| $\Delta t$                | Length of control period                                                                                  | hours   |
| $\Delta h$                | Length of market period                                                                                   | hours   |
|---------------------------+-----------------------------------------------------------------------------------------------------------+---------|
| $\Pb{h}$                  | Amount of balancing power offered on the balancing market                                                 | kW      |
| $\ccp{h}$                 | Critical capacity price in market period $h$                                                              | $\emw$  |
| $\cep{h}$                 | Critical energy price in market period $h$                                                                | $\emwh$ |
| $\Eb{h}$                  | Amount of energy charged from balancing market in market period h                                         | MWh     |
| $\Pi{h}$                  | Amount of power offered for the unit on the intraday market                                               | kW      |
| $\cup{h}$                 | Critical unit price in market period $h$                                                                  | $\emwh$ |
| $\Ei{h}$                  | Amount of energy charged from the intraday market in market period h                                      | MWh     |
|---------------------------+-----------------------------------------------------------------------------------------------------------+---------|
| $\fP{t}$                  | Amount of available fleet charging power in control period $t$                                            | kW      |
| $\fPhat{t}$               | Predicted amount of available fleet charging power in control period $t$                                  | kW      |
|---------------------------+-----------------------------------------------------------------------------------------------------------+---------|
| $\Cb{h}(P)$               | Cost function for procuring electricity from the balancing market dependent on the offered charging power | $\eur$  |
| $\Ci{h}(P)$               | Cost function for procuring electricity from the intraday market dependent on the offered charging power  | $\eur$  |
| $p^{ind}$                 | Industry electricity tariff                                                                               | $\ekwh$ |
| $\oc$                     | Opportunity costs of losing rental of EV $i$ in control period $t$                                        | $\eur$  |
| $\beta_h$                 | Imbalance costs in market period $h$                                                                      | $\eur$  |
|---------------------------+-----------------------------------------------------------------------------------------------------------+---------|
| $\lb{h}$                  | Balancing market risk factor                                                                              | $[0,1]$ |
| $\li{h}$                  | Intraday market risk factor                                                                               | $[0,1]$ |
| $\theta_{\lambda}$        | Set of risk factors for all market periods $h\!\in\!\{1,\hdots,N_h\}$                                     | -       |
| $\Cf{}(\theta_{\lambda})$ | Cost function for the fleets total costs over all market periods dependet on the chosen risk factors      | $\eur$  |
| $\Cf{h}$                  | Total accumulated fleet costs until market period $h$                                                     | $\eur$  |
|---------------------------+-----------------------------------------------------------------------------------------------------------+---------|
| $\F$                      | Set of all EVs in the fleet                                                                               | -       |
| $i$                       | Electric vehicle                                                                                          | -       |
| $\c$                      | Dummy variable indicating if EV $i$ is connected to a charging station                                    | 0/1     |
| $\omega_{i}$              | Amount of electricity stored in the battery of EV $i$                                                     | $\kwh$  |
| $\Omega$                  | Maximum battery capacity of the considered EV model                                                       | $\kwh$  |
| $\delta$                  | Charging power of the considered EV model                                                                 | $\kw$   |
|---------------------------+-----------------------------------------------------------------------------------------------------------+---------|

** Assumptions label:sec-model-assumptions

In order to evaluate and operationalize our model, the following assumptions
about the available information and the electricity market mechanisms were
taken:

- I-1. :: Mobility demand information

     The controller is able to forecast the mobility demand of the EV fleet with
     different time-horizons based on historical data. More specifically, the
     controller can predict the amount of plugged-in EVs and consequently the
     available charging power $P^{fleet}_t$ of the fleet in control period $t$.
     The prediction accuracy is increasing with shorter time horizons, from
     uncertain predictions one week ahead to very accurate predictions 30
     minutes ahead. Past research presented such mobility demand forecast
     algorithms in the context of free-float carsharing
     cite:kahlen18_elect_vehic_virtual_power_plant_dilem,kahlen17_fleet,wagner16_in_free_float.

- I-2. :: Critical electricity price information

     The controller is able to forecast electricity prices of spot and balancing
     markets based on historical data. More specifically, the controller can
     estimate the critical prices $\ccp{h}$, $\cep{h}$, and $\cup{h}$ for each
     market period with perfect accuracy (see Chapter ref:sec-data-balancing and
     Chapter ref:sec-data-intraday for the critical price definitions).
     Electricity price forecasting is an extensively studied research area with
     well-advanced prediction algorithms
     cite:weron14_elect_price_forec,avci18_manag_elect_price_model_risk.
We are confident that making the above information assumptions is feasible.
Assuming available forecasting information is common practice in the VPP and EV
fleet charging literature, see for example
textcite:vandael15_reinf_learn_heuris_ev_fleet,
textcite:mashhour11_biddin_strat_virtual_power_plant_1,
textcite:tomic07_using_fleet_elect_drive_vehic_grid_suppor, and
textcite:pandzic13_offer_model_virtual_power_plant.

- M-1. :: Balancing market mechanism

     We assume that the controller is able to submit bids of any quantity for
     single 15-minute market periods 7 days ahead to the balancing market. Since
     the critical capacity and energy prices are available by I-2, the
     controller submits bids in the form $(\Pb{},\ccp{},\cep{})$. By
     construction of the critical prices, the submitted bid will always get
     accepted by the TSOs and the offered balancing capacity fully activated.

- M-2. :: Intraday market mechanism

     The controller submits bids in the form $(\Pi{},\cup{})$ to the intraday
     market 30 minutes ahead. We assume that the order to buy will always get
     matched until the minimal lead time of the trade (e.g., 5 minutes on the
     EPEX Spot Intraday Continuous). In reality, this is not always the case
     since trades are executed immediately and it is not guaranteed that a
     matching order to sell is submitted between the bidding time and the
     minimal lead time.
In summary, we are assuming that the controller always submits the optimal bids
at the right time. In other words, every bid leads to the successful procurement
of the desired amount of electricity. This assumption provides an upper bound
for the fleet profits from trading EV battery storage on the electricity
markets. However, this upper bound is only influenced by the accuracy of the
electricity price forecasting algorithm, which we take as given by I-2.
Incorporating the electricity forecasting aspect into our work, would well
exceed the scope of this thesis. Furthermore, we assume that the controller is a
price-taker. Due to the limited size of its bids, it is lacking the market share
to influence prices on the markets. Similar assumptions have been made by
textcite:brandt17_evaluat_busin_model_vehic_grid_integ and
textcite:vandael15_reinf_learn_heuris_ev_fleet.

** Control Mechanism label:sec-model-mechanism
The control mechanism constitutes the core of this research. It can be seen as a
decision support system that can be deployed at an EV fleet operator to
centrally control the charging of the fleet. Figure ref:fig-control-mechanism
depicts the control mechanism, which is divided into three distinct phases: The
first phase, /Bidding Phase I/, takes place just before the closing time of the
balancing market, once every week (e.g., Wednesdays at 3pm at the GCRM). In this
phase, the controller places bids for every market period $h$ of the following
week on the balancing market. The second phase, /Bidding Phase II/, takes places
in every market period of $\Delta{h}\!=\!15$ minutes. At this point, the
controller has the opportunity to place bids on the intraday market for the
market period 30 minutes ahead. The third phase, /Dispatch Phase/, takes places
in every control period of $\Delta{t}\!=\!5$ minutes. The controller has to
dispatch available EVs to charge the procured electricity from the markets. The
phase involves allocating individual EVs to the VPP and potentially refusing
customer rentals to assure that all market commitments can be fulfilled.

The following sections highlight important parts of the three phases, formalize
the model mathematically and provide detailed explanations and illustrative
examples.

#+CAPTION[Control Mechanism]: The central control mechanism of the fleet. In order to profitably charge the fleet, the controller has to take decisions in three distinct phases before electricity delivery. The controller needs to predict the available charging power, decide on the bidding quantities to submit to the markets, and intelligently dispatch EVs to charge the procured amount of electricity. label:fig-control-mechanism
#+ATTR_LATEX: :width 1.05\linewidth :placement [htbp]
[[../fig/control-mechanism.png]]

*** Fleet Charging Power Prediction

In a first step, the controller has to predict the available fleet charging
power for the market period of interest (see\circled{A} in Figure
ref:fig-control-mechanism). The actual available fleet charging power $\fP{t}$
in a control period $t$ is given by the number of EVs that are connected to a
charging station, with enough free battery capacity to charge the next control
period $t\!+\!1$.

By assumption I-1, the predicted charging power of the fleet is available to the
controller. However, when the controller procures electricity from the markets,
the fleet has to charge with the committed charging power throughout the whole
market period $h$, otherwise imbalance costs occur. To address this risk, we
define the predicted fleet charging power in a market period as the minimal
predicted fleet charging power of all control periods in that market period:
\begin{equation}
    \fPhat{h} \defeq \min_{n \in \{1, \hdots, T\}} \fPhat{t + n} \text{ ,}
\end{equation}
where $h$ is the market period of interest, $t$ its first control period and $T$
the number of control periods in a market period.

*** Market Decision
In a second step, the controller has to decide from which market it should
procure the desired amount of energy (see\circled{B} in Figure
ref:fig-control-mechanism). Therefore, it compares the costs for charging
electricity from the balancing market with the costs for charging from the
intraday market. The cost function for procuring electricity from the balancing
market is defined as follows:
\begin{equation} \label{eq-cost-balancing}
\begin{split}
    \Cb{h}(P) &\defeq -(\frac{P}{10^{3}} \times \ccp{h}) + (\Eb{h} \times \cep{h}) \\
    &= -(\frac{P}{10^{3}} \times \ccp{h}) + (\frac{P\Delta h}{10^{3}} \times \cep{h}) \text{ ,}
\end{split}
\end{equation}
where $P$ (kW) is the amount of offered balancing power. The first term of the
equation corresponds to the compensation the controller retrieves for keeping
the balancing capacity available, while the second term corresponds to the costs
for charging the activated balancing energy $\Eb{h}$ (MWh). Energy is power over
time, hence $\Eb{h}$ can be substituted with $P$ times the market periods length
$\Delta{h}$, divided by the unit conversion term from kW to MW (see
eqref:eq-cost-balancing, second part). Note that the critical energy price
$\cep{}\!\in\!\Re$, can also take negative values, resulting in profits for the
fleet, while the critical capacity price $\ccp{}\!\in\! \Re^+_0$ is never
negative and therefore never results in costs for the fleet. The cost function
for charging from the intraday market is defined similarly to
eqref:eq-cost-balancing:
\begin{equation}
\begin{split}
    \Ci{h}(P) &\defeq \Ei{h} \times \cup{h} \\
    &= \frac{P\Delta h}{10^{3}}\times \cup{h}
\end{split}
\end{equation}
Again, depending on the market situation, $\cup{}\!\in\!\Re$ can either be
negative or positive, resulting in costs or profits for the fleet. Contrarily to
the balancing market, on the intraday market the fleet does not get compensated
for keeping the charging power available, only the amount of charged energy affects the
costs.

The costs of charging influence the controllers market decision and the
composition of the VPP portfolio. Depending on the charging costs and the
associated risks with bidding on the markets the controller decides on the
bidding quantities it should submit to each market. The next section eludes this
core challenge of the controlled charging problem.

*** Determining the Bidding Quantity
In a third step, the controller has to take a decision on the amount of energy
it should procure from the markets (see\circled{C} in Figure
ref:fig-control-mechanism). The bidding quantity determines the profits that can
be made by charging at a cheaper market price than the flat industry tariff. On
one hand, the controller aims to maximize its profits by procuring as much
electricity as possible from the markets. On the other hand, it needs to balance
the risk of (a) procuring more energy that it can maximally charge and (b) not
procuring enough energy from the market to sufficiently charge the fleet.

In case (a), the fleet is facing costs of compromising customer mobility, or
worse, high imbalance penalties from the markets. Renting out EVs is
considerably more profitable than using their batteries as a VPP. Refusing
customer rentals, in order to fulfill market commitments, induces opportunity
costs of lost rentals $\rho$ on the fleet. Imbalance costs $\beta$ occur, when
the fleet can not charge the committed amount of energy at all, even with refusing
rentals. In case (b), the fleet also faces opportunity costs of lost rentals
when individual EVs do not have enough SoC for planned trips of arriving
customers.

The controller faces additional risks by bidding one week ahead on the balancing
market, in contrast to bidding only 30 minutes ahead on the intraday market.
This is because predictions are more uncertain with a larger time horizon. To
account for all the mentioned risks, we introduce a /risk factor/ $\lambda \in
\Re_{0 \leq \lambda \leq 1}$, where $\lambda\!=\!0$ indicates no risk, and
$\lambda\!=\!1$ indicates a high risk. The controller determines the bidding
quantity $\Pb{h}$ by discounting the predicted available fleet charging power
$\fPhat{h}$ with the possible risk $\lambda_{h}$ of imbalance or opportunity
costs:
\begin{equation} \label{eq-model-pb}
  \Pb{h} \defeq
  \begin{cases}
    0, & \text{if}\ \Cb{h}(\fPhat{h}) \geq \Ci{h}(\fPhat{h})\\
    0, & \text{if}\ \Cb{h}(\fPhat{h}) \geq 10^3\Eb{h} \times p^{ind}\\
    \fPhat{h}\!\times\!(1\!-\!\lb{h}), & \text{otherwise}
  \end{cases}
\end{equation}
where $h$ is the market period of interest one week ahead. If the controller can
buy electricity at the intraday market at a lower price, it does not place a bid
at the balancing market (see eqref:eq-model-pb, first condition). If the
controller can charge cheaper at the regular industry tariff $p^{ind}$ (see
eqref:eq-model-pb, second condition), it does not place a bid either. In all
other cases, the controller submits $\Pb{h}$ to the market. The bidding quantity
for the intraday market $\Pi{h}$ depends on the previously committed charging
power $\Pb{h}$ and the newly predicted charging power $\fPhat{h}$:
\begin{equation} \label{eq-model-pi}
  \Pi{h} \defeq
  \begin{cases}
    0, & \text{if}\ \Ci{h}(\fPhat{h}\!-\!\Pb{h}) \geq 10^3\Ei{h}\!\times\!p^{ind}\\
    (\fPhat{h}\!-\!\Pb{h})\!\times\!(1\!-\!\li{h}), & \text{otherwise}
  \end{cases}
\end{equation}
where $h$ is the market period of interest 30 minutes ahead. Here the
undiscounted bidding quantity equals $\fPhat{h}\!-\!\Pb{h}$, since the amount of
electricity that the controller procured from the balancing market does not need
to be bought from intraday market again. When the controller submits bids to the
intraday market in the second decision phase, it is able to correct bidding
errors it made in the first decision phase, and optimize the bidding strategy of
the EV fleet.

*** Dispatching Electronic Vehicle Charging
In the last step, at electricity delivery time, the EVs have to be assigned to
the VPP and be /dispatched/ to charge (see\circled{D} in Figure
ref:fig-control-mechanism). Therefore the controller needs to detect how many
EVs are eligible to be used as VPP in the control period $t$. An EV $i$ is
eligible if (a) it is connected to a charging station ($\c$ = 1), and (b) it has
enough free battery storage available
($\omega_{i}\leq\Omega\!-\!\gamma\Delta{t}$) to charge the next control
period. Hence, the VPP is defined as:
\begin{equation}
    V\!P\!P \defeq \big\{i\in\F \;|\; \c = 1 \vee \omega_{i}\leq\Omega\!-\!\gamma\Delta{t}\big\} \text{ ,}
\end{equation}
where $\gamma\Delta{t}$ (kWh) denotes the amount of energy that can be charged
with the charging speed of $\gamma$ (kW) in control period $t$. $\gamma$ is
limited by either the EVs build-in charger, or the charging power of the
connected charging station. In this model we assume $\gamma$ is equal for all
considered EVs and charging stations. /Example:/ Assuming a charging power of
$\gamma\!=\!3.6\;\kw$, an EV battery capacity of $\Omega\!=\!17.6\;\kwh$, and
control periods of 5 minutes, the amount of energy charged in one control period
is $3.6\;\kw\!\times\frac{5}{60}\text{h}\!=\!0.3\;\kwh$. Hence, the maximally
charged electricity $\omega_{i}$ of an EV to be eligible for VPP use is
$17.6-\!0.3\!=\!17.3\;\kwh$.

#+BEGIN_SRC python :exports none
return(3.6 * (5/60))
#+END_SRC

#+RESULTS:
: 0.3

#+BEGIN_SRC python :exports none
return(17.6 - 0.275)
#+END_SRC

#+RESULTS:
: 17.325000000000003

Remember that the fleet has to provide the total committed charging power
$\Pb{h}+\Pi{h}$ across all control periods $t$ of the market period $h$,
independent of which individual EVs are actually charging the electricity. This
fact allows the controller to dynamically dispatch EVs every control period and
react to unforeseen rental demand. If a customer wants to rent out an EV that is
assigned to the VPP, the controller only has to refuse the rental, if no other
EV is available to charge instead. When no replacement EV is available, the
controller has to account for the lost rental profits $\oc$. If the VPP's total
amount of available charging power $\vpp{t}\!\times\!\gamma$ is not sufficient
to provide the total market commitments $\Pb{h}\!+\!\Pi{h}$, the fleet gets
charged imbalance costs $\beta_{h}$. Otherwise the full amount of committed
energy can be charged by the EVs of the VPP.

*** Evaluating the Bidding Risk
The controllers main goal is to choose the risk factors $\lb{h}$, $\li{h}$
for every market period $h$ that minimize the cost of charging, while avoiding
the risks of lost rental profits $\oc$ or imbalance costs $\beta_h$. The total
fleet costs are defined as follows:
\begin{equation} \label{eq-model-fleetcosts}
    \Cf{}(\theta_{\lambda}) \defeq \sum^{N_h}_h
    \bigg[ \Cb{h}(\Pb{h}) + \Ci{h}(\Pi{h}) + \beta_{h}
    + \sum_t^{T} \sum_i^{|\F|} \oc \bigg] \text{ ,}
\end{equation}
where $\theta_{\lambda}$ is the set of the risk factors $\lb{h}$,
$\li{h}\!\in\!\Re_{0 \leq \lambda \leq 1}$ for all considered market periods
$N_h$. $\F$ denotes the set of all EVs $i$ in the fleet and $|\F|$ the fleet
size. The costs for charging $\Cb{h}(\Pb{h})$, $\Ci{h}(\Pi{h})$ are dependent on
the chosen risk factors $\lb{h}$, $\li{h}$ (see eqref:eq-model-pb and
eqref:eq-model-pi), which are omitted here for simplicity. In summary, the
problem can be formulated as minimizing the total costs of the fleet, by
choosing the optimal set of risk factors $\theta_{\lambda}$:
\begin{equation} \label{eq-model-opti}
\begin{aligned}
    & \underset{\theta_{\lambda}}{\text{minimize}}
    && \Cf{}(\theta_{\lambda}) \\
    & \text{subject to}
    && 0 \leq \lb{h} \leq 1, \; \forall \lb{h} \in \theta_{\lambda}\\
    &&& 0 \leq \li{h} \leq 1, \; \forall \li{h} \in \theta_{\lambda}\\
\end{aligned}
\end{equation}

A goal of this thesis is to develop a model that can be applied to previously
unknown settings and learn from uncertain environments in the smart grid, such
as new mobility contexts and smart electricity markets. As discussed in Chapter
ref:sec-back-rl, RL is a suitable approach to achieve this goal and solve the
proposed controlled charging problem eqref:eq-model-opti. Before we introduce
the developed RL approach in Chapter ref:sec-model-rl, we illustrate the three
decision phases of the previously described control mechanism with a
comprehensive example in the next section.

*** Example: Decision Phases
On August 9, 2017 at 3pm, the controller enters the first bidding phase for the
market period $h$ = /16.08.2017 15:00-15:15/. It predicts that in that interval
250 EVs are connected to a charging station, resulting in 900 kW available fleet
charging power ($\fPhat{h}\!=\!900\;\kw$), given the charging power of 3.6 kW
per EV. Assuming the critical prices are $\ccp{h}\!=\!5\emw$,
$\cep{h}\!=\!-10\emwh$, and $\cup{h}\!=\!10\emwh$ in that market period, the
controller now evaluates the cheapest charging option. The flat industry
electricity tariff is assumed to be $p^{ind}\!=\!0.15\ekwh$. The costs for
charging with the predicted amount of available power from the balancing market
($\Cb{h}(900\;\kw)\!=\!-6.25\;\eur$) are less than charging from the intraday
market ($\Ci{h}(900\;\kw)\!=\!2.25\;\eur$) or charging at the industry tariff
($900\;\kw\!\times\!0.25\;\text{h}\!\times\!0.15\ekwh\!=\!33.75\;\eur$). In this
example, the fleet operator will even get paid 6.25 $\eur$ for charging the
fleet by choosing the balancing market.

In the next step, the controller has to submit bids to the balancing market. The
RL agent determined that the risk of bidding on the balancing market is
$\lb{h}\!=\!0.3$. Consequently, the controller sets the bidding quantity to
$\Pb{h}\!=\!\fPhat{h}\!\times\!(1\!-\!\lb{h})\!=\!900\;\kw\!\times0.7\!=\!630\;\kw$,
submits a bid to the market, and accounts for charging costs of
$\Cb{h}(630\;\kw)\!=\!-4.725\;\eur$.

One week later, 30 minutes before electricity delivery time, the controller
enters the second bidding phase. Due to the short time horizon, it predicts with
high accuracy that only $\fPhat{h}\!=\!810\;\kw$ (instead of 900 kW) is
available for the market period /16.08.2017 15:00-15:15/. By trading at the
intraday market, the controller can now charge the remaining available EVs with
a low risk of procuring more energy than it can maximally charge. At this point,
the RL agent determines a intraday bidding risk of $\li{h}\!=\!0.05$, and sets
the bidding quantity to
$\Pi{h}\!=\!(810\;\kw\!-\!630\;\kw)\!\times\!(1\!-\!0.05)\!=\!171\;\kw$. The
controller procures 171 kW from the intraday market and accounts for the
charging costs of $\Ci{h}(171\;\kw)\!=\!0.4275\;\eur$.

At electricity delivery time, August 16, 2017 at 3pm, the controller detects
255 EVs that are eligible for VPP use; EVs that are connected to a charging
station and have enough battery capacity left to charge during the next control
period. It assigns 223 EVs to provide the total committed 801 kW charging power
for the market period time $\Delta h$ of 15 minutes. During that time, three
customers want to rent out EVs that are allocated to the VPP. The first two
rentals are accepted because two other EVs of the VPP are available to charge
instead. The third rental has be to refused, since no EV can substitute the
charging power. Hence, the controller has to account for the opportunity costs
of that lost rental $\oc$.

#+BEGIN_SRC python :exports none :var h=15
def bal_cost(p_c, p_e, P):
    return(-(p_c * P * 0.001) + (p_e * P * (h/60) * 0.001))

return(bal_cost(5, -10, 630))
#+END_SRC

#+RESULTS:
: -4.725


#+BEGIN_SRC python :exports none :var h=15
def intraday_cost(p_u, P):
    return((p_u * P * (h/60) * 0.001))

return(intraday_cost(10, 171))
#+END_SRC

#+RESULTS:
: 0.4275

#+BEGIN_SRC python :exports none :var h=15
def industry_costs(p_i, P):
    return((p_i * P * (h/60)) / 100)

return(industry_costs(15, 900))
#+END_SRC

#+RESULTS:
: 33.75

#+BEGIN_SRC python :exports none
def pi(l):
    kw = (810 - 630) * (1 - l)
    return kw

return(pi(0.05))
#+END_SRC

#+RESULTS:
: 171.0

#+BEGIN_SRC python :exports none :var h=15
kw = 630  + 171
return(kw)
#+END_SRC

#+RESULTS:
: 801

** Reinforcement Learning Approach label:sec-model-rl
In the following chapter the developed RL approach is outlined. First, we define
the charging problem as an MDP, and second, the learning algorithm is explained.
Remember that the goal of the controlled charging problem is to choose a set of
risk factors $\theta_{\lambda}$ that minimize the fleets total costs across all
market periods. The controller is able to influence the costs, by setting the
risk factors $\lb{}$, $\li{}$ each market period $h$. The risk factors determine
the bidding quantities $\Pb{h}$, $\Pi{h}$ that the controller submits to the
balancing and intraday market, which in the end determine the fleet costs. The
RL agent decides on the risk factors (i.e., takes an action) based on the
observed state $S_{h}$ every market period $h$ (usually denoted as time step $t$
in the RL literature). The optimal set of risk factors is learned by the RL
agent through estimating a policy $\pi(a|s)$ that maps every state $s\in\S$ to
an action $a\in\A$.
*** Markov Decision Process Definition

MDPs are defined by the state space $\S$, the action space $\A$, a set of reward
signals $\R$ and the state-transition probabilities $p(s'|a,s)$. When
$p(s'|a,s)$ is unknown, as it is in our case, it is possible to use a model-free
approach (see Chapter ref:sec-td-learning). The state space compromises the
observed information the agent uses to decide on the action it is going to take.
We observed the following factors that influence the risk of bidding on the
markets:
1) The bidding period's time of the day

   In times of volatile customer rental demand, for example during rush hour,
   the uncertainty on the guaranteed amount of available EVs increases. Bidding
   for these periods involves a higher risk of not being able to fulfill market
   commitments.
2) The current and estimated future size of the VPP

   Large VPPs benefit from the /risk-pooling/ effect cite:kahlen17_fleet.
   Intuitively that means, larger VPPs are exposed to smaller risks: They have
   an increased probability that "lost" charging power, due to unforeseen EV
   rentals, can be substituted by other EVs of the VPP.
Since forecasts of available charging power are available by I-1, we define the
predicted VPP size $\vpphat{h}$ as the as the necessary amount of EVs to provide
the predicted charging power $\fPhat{}$ in time period $h$:
\begin{equation}
    \vpphat{h} \defeq \left\lceil\frac{\fPhat{h}}{\gamma}\right\rceil \text{ ,}
\end{equation}
where $\gamma$ is the charging power per EV. The brackets $\lceil x \rceil$,
some readers might not be familiar with, mean the smallest integer equal or
greater than $x$, which can also be written as ceil($x$). /Example:/ When the
controller predicted 910 kW available charging power, the required future size
of the VPP to charge with the predicted power is $\text{ceil}(910\;\kw/3.6\;\kw)
= 253$.


#+BEGIN_SRC python :exports none
import math
def vpp(p, gamma):
    return(math.ceil(p /  gamma))

return(vpp(910, 3.6))
#+END_SRC

#+RESULTS:
: 253

Based on the listed factors, we define the state space as the set of all
valid values of the following tuple:
\begin{equation}
    \S \defeq \left\langle t(h), \vpp{h}, \vpphat{h+2}, \vpphat{h+(7\!\times\!H)}\right\rangle \text{ ,}
\end{equation}
where:
- $t(h)$ is the bidding period's daytime in hours, with discrete values in the range
  $\big[0,\;23\big]\in\Ne$.
- $|VPP|_t$ is the current VPP size, with discrete values in the range
  $\big[0,\;|\F|\big] \in \Ne$.
- $\vpphat{h+2}$ is the predicted VPP size 30 minutes ahead, with discrete values in the range
  $\big[0,\;|\F|\big] \in \Ne$.
- $\vpphat{h+(7\!\times\!H)}$ is the predicted VPP size 7 days ahead, with discrete
  values in the range $\big[0,\;|\F|\big] \in \Ne$.
Considering all possible combinations of the values, the state space encompasses
$24\!\times\!|\F|^3$ states. When assuming a fleet size $|\F|$ of 500 EVs, that
are $3\!\times\!10^9$ different states.

#+BEGIN_SRC python :exports none
import math
return(24 * math.pow(500,3))
#+END_SRC

#+RESULTS:
: 3000000000.0

The agent takes actions by determining the risk that is associated with bidding
on the electricity markets at each market period $h$. Hence, the action space is
constituted by all combinations of valid values of the risk factors
$\lb{},\li{}$:
\begin{equation}
    \A \defeq \left\{\lb{},\li{} \in \Re_{0 \leq \lambda \leq 1} \right\} \text{ ,}
\end{equation}
where:
- $\lb{}$ is the risk factor for bidding on the balancing market 7 days ahead,
  with discrete values in the range $\big[0,1\big]$ in 0.05 increments.
- $\li{}$ is the risk factor for bidding on the intraday market 30 minutes
  ahead, with discrete values in the range $\big[0,1\big]$ in 0.05 increments.
Considering 20 discrete increments of the action values, the action space
encompasses $20^2 = 400$ actions. The state space and action space were
consciously discretized to achieve faster learning rates. Convergence in
continuous spaces is theoretically achievable, but computationally more complex
cite:sutton18_reinf. In order to facilitate faster learning in real-world
settings, where long training periods are not desirable, we chose to not pursue
this direction further.

# NOTE: Reward structure, possibilities
The reward signal is naturally defined as the fleet costs that occurred in the
last time step:
\begin{equation}
    R_{h+1} = \Cf{h} - \Cf{h-1} \text{ ,}
\end{equation}
where $\Cf{h}$ are the total accumulated fleet costs until the market period
$h$. When accumulating the occurred rewards for all time steps, we arrive at the
total fleet costs, which we aim to minimize. See eqref:eq-model-fleetcosts for a
complete formulation of the cost function. The agent's actions directly
determine the occurred costs or profits, and are presented to the agent in form
of a positive or negative reward signal. The particular challenge in the
proposed RL problem is the significantly /delayed reward/. Choosing a risk
factor in time step $h$ determines the reward up to 672 time steps later (7
days, with 15-minute time steps), when the electricity from the balancing market
has to be charged.

#+BEGIN_SRC python :exports none
return(7 * 24 * 4)
#+END_SRC

#+RESULTS:
: 672
*** Learning Algorithm label:sec-model-algo
This research proposes to solve the presented RL problem, with the double deep
Q-Network algorithm (DDQN), developed by
textcite:hasselt16_deep_reinf_learn_doubl_q_learn. DDQN is a state-of-the-art,
model-free RL approach that uses a deep neural network as a function
approximator to estimate optimal Q-values (see Chapter ref:sec-rl-fa for a
explanation of function approximation methods). It combines the revolutionary
deep Q-Network (DQN), originally proposed by
textcite:mnih15_human_level_contr_throug_deep_reinf_learn with double Q-Learning
cite:hasselt10_doubl_q. In double Q-Learning, experiences are randomly selected
to update two different value functions to select and evaluate actions (in
contrast to just one function for both tasks). DDQN has shown to reduce
overoptimistic action-value estimates of the DQN algorithm, resulting in more
stable and reliable learning results
cite:hasselt16_deep_reinf_learn_doubl_q_learn. Combined with the /dueling
network/ architecture, proposed by
textcite:wang15_duelin_networ_archit_deep_reinf_learn, this approach outperforms
existing deep RL methods. Dueling networks lead to faster convergence rates in
control problems with large action spaces than traditional single stream
approaches. This property is especially beneficial for our proposed RL problem,
as the defined action space (400 possible actions) is still comparably large in
comparison to classical control problems
cite:sutton96_gener,barto83_neuron_adapt_elemen_that_can. In Figure
ref:fig-model-dueling, the conventional single stream approach (top) versus the
dueling architecture (bottom) is depicted. The dueling architecture consists of
a neural network of any shape with two streams that separately estimate the
state-value and the action advantages. These estimates are later combined into
Q-values (see Figure ref:fig-model-dueling, green layer):
\begin{equation} \label{eq-duel-q}
    Q(s,a) = V(s) + \left(A(s,a) - \frac{1}{|\A|} \sum_{a'} A(s,a')\right) \text{ ,}
\end{equation}
where $V$ and $A$ are estimates of the value function and action advantages
respectively, represented by the two different streams in the network. By
subtracting the mean action advantages (see eqref:eq-duel-q, last term),
identifiability ($V$ and $A$ can be recovered, given $Q$) and stability of the
optimization is ensured. The separated streams allow to learn which states are
valuable without having to learn each state-action interaction individually.
Like this, a general state-value is learned that can be shared across many
different actions, leading to a faster convergence of dueling architectures
cite:wang15_duelin_networ_archit_deep_reinf_learn.

#+CAPTION[Dueling Network Architecture]: The dueling network architecture cite:wang15_duelin_networ_archit_deep_reinf_learn. The two separate streams achieve faster convergence rates by learning individual state values and action advantages first, and only later combining them into Q-values. label:fig-model-dueling
#+ATTR_LATEX: :width 0.95\linewidth :placement [h]
[[../fig/ddqn.pdf]]

Our agent uses the dueling DDQN algorithm with a standard ANN architecture. The
ANN consist of four input nodes, three fully-connected hidden layers with ReLU
cite:nair10_rectif_linear_units_improv_restr_boltz_machin activation functions,
and a linear output layer with two nodes. Further, an \epsilon-greedy policy
with a linear decreasing exploration rate was used. The exact network definition
can be found in Appendix ref:app-rl-network-def. We implemented the RL agent
with the neural networks API Keras[fn:1][fn:2], which is a high-level
abstraction layer of TensorFlow. TensorFlow is the de-facto standard for robust
and scalable machine learning in industry and research cite:abadi16_tensor.
Further, we used the shared research environment Google Colaboratory to train
and evaluate the agent. It offers free access to computing resources that are
optimized for training machine learning models.[fn:3]

#+LATEX: \clearpage

* Footnotes

[fn:1] https://www.keras.io

[fn:2] https://github.com/keras-rl/keras-rl

[fn:3] Google Colaboratory (https://colab.research.google.com) provides a NVIDIA
Tesla K80 GPU, with 2880 $\times$ 2 CUDA cores and 12GB GDDR5 VRAM, and can be
used up to 12 hours of consecutive training time.

# Additionally, the environment is equipped with a Intel(R) Xeon(R) CPU @ 2.30GHz
# (1 core, 2 threads), and 12GB available memory.
