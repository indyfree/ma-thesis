* Conclusion
# NOTE 5%
# NOTE Sec: Setting
Integrating volatile renewable energy sources into the electricity system
imposes challenges on the electricity grid. In order to ensure grid stability
and avoid blackouts, balancing power is needed to match electricity supply and
demand. Balancing power can be provided by VPPs that generate or consume energy
within a short period of time and offer these services on the electricity
markets cite:pudjianto07_virtual_power_plant_system_integ. EV fleet operators
can utilize idle vehicles to form VPPs and offer available EV battery capacity
as balancing power to the markets. The fleet can offer balancing services
directly via tender auctions on the balancing market or via continuous trades on
the intraday market, where participants procure or sell energy to self-balance
their portfolios cite:pape16_are_fundam_enoug. Both markets have complementary
properties in terms of price levels and lead times to delivery, which motivates the
creation of a VPP portfolio to profitably participate in both markets and extend
the business model of the fleet.

# NOTE Sec: Problem
# NOTE Mention risks? Mention portfolio (optimization)?
However, there are certain risks associated with this business model extension.
EVs can only be allocated to a VPP portfolio if they are connected to a charging
station and have sufficient free battery capacity available, information which
is unknown to the fleet at the time of market commitment. This uncertainty makes
it difficult for fleet operators to estimate the size of the VPP and calculate
the optimal bidding quantities. Moreover, if fleet operators offer more
balancing power than they can provide, they face high imbalance penalties from
the markets. For a sustainable business model fleet operators also need to
balance VPP activities with their primary offering, customer mobility. Denying
customer rentals to ensure that the fleet can fulfill the market commitments,
results in opportunity costs of lost rentals that compromise the profitability
of the fleet.

In the following chapter, we will summarize the conducted research of this
thesis to address the aforementioned challenges. Furthermore, the contribution
of this work is outlined and the main results are presented and discussed.
Finally, we list specific limitations of this study and give insights on further
areas of research.
** Contribution
# NOTE Sec: What we have done
#     1. Model (Control mechanism)
#     2. Simulation Platform
#     3. Integrated bidding strategy
#     4. RL Agent that optimizes strategy by determining risk
The core contributions of this thesis are the following: First, we
conceptualized a DSS for controlled EV charging under uncertainty. The DSS
constitutes the core of a business model extension for fleet operators to manage
a VPP portfolio of EV batteries. A controlled charging problem has been
mathematically formulated and a control mechanism introduced that aims solve the
proposed problem. Second, we developed an event-based simulation platform that
facilitates fleet management research with real-world data. We evaluated various
baseline bidding strategies within the simulation platform and tested out the
behavior of intelligent agents in the context of controlled EV charging. Third,
we proposed a novel integrated bidding strategy that offers balancing services
of a VPP portfolio to multiple electricity markets simultaneously. Instead of
submitting only conservative amounts of battery capacity to a single market,
like previous studies did
cite:kahlen17_fleet,kahlen18_elect_vehic_virtual_power_plant_dilem, our proposed
strategy aims to maximize profits by procuring electricity from the multiple
markets to the greatest extent possible without causing imbalances. Forth, we
proposed the usage of a RL agent that learns to optimize the composition of the
VPP portfolio and the associated risks of bidding on the markets. The agent
dynamically determines risk factors, dependent on available and predicted fleet
information and market conditions. Therefore, we formulated a MDP that is
designed for agents to work in previously unknown environments and uncertain
conditions. The RL approach was developed with the focus on real-world
applicability, fast convergence rates and generalizability. We expect the
proposed approach to work in a variety of different settings, with different
kinds of EVs (e.g., electronic bikes), independent of the geographical location
of the fleet.

# - RL can learn to dynamically adjust bidding quantities by learning risk
#   associated with bidding on each market. (What are the risks?)
# - obtained better results than similar studies in the field. (how much and which?)

# NOTE Sec: What we have found (include key numbers)
# TODO: Double check numbers!
The proposed method was evaluated in a series of expriments with real-world
carsharing data from Car2go in Stuttgart and German electricity market data from
June 2017 until January 2018. The results show that the developed method
improves existing approaches and would increase the gross profits of the fleet
by roughly $74 000\; \eur$ over a 1.5 year period. We found that the integrated
bidding strategy generates 49%-54% more profits, compared to the naive benchmark
strategies. Fleet controllers following the integrated bidding strategy can
mitigate risks and increase profits by exploiting market properties of both
markets. The fleet would also contribute to the integration of renewable energy
resources by providing around 1400 MWh of balancing power to stabilize the
electrical power grid. Since the inherently uncertain demand patterns of free
float carsharing are imposing additional challenges on providing balancing
services, better results are expected with other kinds of EV fleets.

Further, we showed that the proposed RL agent can optimize the integrated
bidding strategy under uncertainty by roughly 12%, missing the optimal result of
a bidding strategy with full information available by only 3%. The agent was
able to successfully estimate the bidding risk, avoid imbalances and keeping
lost rentals to a minimum. Additionally, we tested and compared the performance
of various modern RL algorithms and found that recent advances in deep RL do
improve the robustness and convergence rates in real-world applications. Despite
these improvements, all RL approaches needed more than 1.5 years of simulation
time to learn to avoid imbalance penalties, which makes a RL approach unsuitable
to deploy in unseen fleet environments without prior training data. A
sensitivity analysis showed that the prediction accuracy of available fleet
charging power has large impact on the fleet profitability. If the controllers
predictions are very uncertain, the RL agent estimates high bidding risks and
can only offer conservative amounts of balancing power on the markets.
Surprisingly, the effect of prediction accuracy on the total gross profit
increase is larger than the choice of bidding strategy, which emphasizes the
need for highly accurate mobility demand forecasting algorithms.

Our results compare well to other studies, while
textcite:kahlen17_fleet,kahlen18_elect_vehic_virtual_power_plant_dilem reported
an annual profit increase of up to $121 \eur$ (163$) per EV, the experiment
conducted in this thesis yielded an increase of $111 \eur$ from charging cheaper
than the regular industry price. In their model however, the authors did not
take imbalance cost into account, which we found to be a determining factor in
the bidding process. When examining a RL approach of EV fleet charging
schedules, textcite:vandael15_reinf_learn_heuris_ev_fleet reported that after 20
to 30 days of training, the algorithm converged to a near-optimal solution. In
our experiments, convergence was reached only after training more than 1.5 years
of simulation time. In contrast to their study, which focused the day-ahead
market, our research focused the balancing market, where participants can not
self-balance their portfolio on other markets, but are charged high imbalance
penalties. Therefore, it was a hard requirement for our proposed RL agent to
learn to never cause imbalances. Imbalances can occur up to one week after the
agent decided on a risk factor for a single bid, which makes it challenging to
connect the long-delayed reward signal to the taken action.
textcite:chis16_reinf_learn_based_plug_in investigated a RL approach to reduce
charging costs of an individual EV and reported cost savings of 10% to 50%.
While the authors consider a fixed driving schedule of a single EV, our research
considered charging an EV fleet with previously unknown mobility patterns.
Despite the additional uncertainty, we could achieve a cost reduction of
charging the fleet by 25%.

# NOTE: Mention Exploitation-Exploration dilemma?

# NOTE: Practical Insights/Summary:
# - RL works good
# - Modern Deep RL architectures matter
# - Prediction is more (?) important
# - At the same time the VPP activities have an environmental impact by reducing
#     CO_2 emissions through more efficient use of renewables. By adding more
#     balancing power renewable energy generation does not to be curtailed.
#     Grid needs to get rid of surplus electricity fast

# NOTE Sec: Discuss !!
# - Advantage over forecast like Kahlen: General approach, unseen environments

** Limitations

# We don't take market dynamics into account
# # NOTE: Citatation from kahlen
# The fleet controller offers bids and asks for every time interval. These offers
# contain both a quantity and a reservation price, which depends on the state of
# charge of the EV storage, as well as on the battery costs. However, the market
# may or may not accept these offers depending on the composition of the offer
# prices from the fleet owner and other market participants. The market auction
# mechanism ultimately decides when EVs will charge and discharge.

- Model:
  - Bidding Mechanism: one week ahead, always accepted
  - Policy & Regulation: EVs not allowed to provide balancing power, minimum
    bidding quantities 1MW.
  - Markets: Fleet is a price-taker, what about larger fleets? Simulate market influence
  - Deny rentals only in the same market period (More deny, less imbalance)
- RL: See cite:vazquez-canteli19_reinf_learn_deman_respon conclusion for
  limitations.
  - Training time in real time. Generalization to other cities?
** Future Research

- This result shows that leaving promising room for future research of highly
accurate mobility demand algorithms.
- Model:
    - Investigate modern/current market design, that changed their bidding
      mechanisms to to better integrate renewable energy generators.
      - Daily/Day-ahead tenders with 4 hour market periods.

      Mischpreisverfahren

       i.e. daily w/ 4h slots. German "Mischpreisverfahren"
- RL: Long-delayed rewards, different reward structure, memory based
- Prediction Algorithms improvement, reference to sensitivity analysis

#+LATEX: \clearpage

